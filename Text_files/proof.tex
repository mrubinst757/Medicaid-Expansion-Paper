\section{Proofs}\label{ssec:proof}

We divide our proofs into three sections: first, where we consider the performance of SBW under classical measurement error. Our key results are that the bias of the SBW estimator is equivalent to the bias of the OLS estimator, and that regression-calibration techniques can be used in this setting to correct for this bias. Second, we consider the properties of the H-SBW objective when the true covariates $X$ are observed. We show that if our assumed correlation structure for the outcome errors is correct, H-SBW produces the minimum conditional-on-X variance estimator within the constraint set. We also show how H-SBW weights relate to the implied regression weights from Generalized Least Squares. In the third, we consider using H-SBW in the context of classical measurement error and our proposed regression-calibration covariate adjustment. We show that when using the standard regression-calibration adjustment, SBW will balance the true covariates to appropriate targets in expectation, while H-SBW may be not and we show the required conditions. We conclude by proposing a modification of the regression-calibration procedure to correct for this.

\subsection{SBW and classical measurement error}\label{app:AsecI}

We show three results regarding the bias of the SBW estimator under the classical errors-in-variables model. First, we show that the bias of the SBW estimator that sets $\delta = 0$ (i.e. reweights the treated units to exactly balance the control units) is equal to the bias of the OLS estimator. Second, we show that if the observed covariate values for the treated data can be replaced by their conditional expectations $\eta_1$ given the noisy observations, then the SBW estimator will be unbiased. Finally, we consider the case where we can estimate $\eta_1$ using auxillary data to estimate the covariance matrix of the error terms and show that the SBW estimator is consistent if we replace $\eta_1$ by an estimate $\hat{\eta_1}$ in the constraint set. We take the perspective throughout that $X$ is random (i.e. the structural measurement error model).

Consider a dataset that consists of $i = 1, ..., n$ randomly sampled units where we observe the outcomes $Y_i$ and a treatment assignment indicator $A_i \in \{0, 1\}$. Let $n_a$ be the number of units in each treatment group, i.e, $n_a = \sum_{i = 1}^n \mathbbm{1}(A_i = a)$. Let $X_i \in \mathbb{R}^q$ be a vector of covariate values where $X_i \mid A_i = a \stackrel{iid}\sim MVN(\upsilon_a, \Sigma_{XX})$. 

Our target parameter is $\psi^1_0 = \mathbb{E}\{Y_i^1 \mid A_i = 0\}$.\footnote{Here we switch to the super-population target because we view $X$ as random.} We assume unconfoundedness ($Y_i^a \perp A_i \mid X_i$), consistency ($Y_i^A = A_iY_i + (1-A_i)Y_i$), and that $\mathbb{E}\{Y_i \mid X_i, A_i = a\} = \alpha_a + X_i^T\beta_a$. 

We begin by establishing the following identity:

\begin{equation}
\psi^1_0 = \mu_y + (\upsilon_0 - \upsilon_1)^T\beta_1
\end{equation}
%
where $\mu_y = \mathbb{E}\{Y_i \mid A_i = 1\}$ and $\upsilon_a = \mathbb{E}\{X_i \mid A_i = a\}$.

\begin{proof}

Using our causal and modeling assumptions we have that:

\begin{align*}
\mathbb{E}\{Y_i^1 \mid X_i, A_i = 0\} &= \mathbb{E}\{Y_i^1 \mid X_i, A_i = 1\} \\
&= \mathbb{E}\{Y_i \mid X_i, A_i = 1\} \\
&= \alpha_1 + X_i^T\beta_1 \\
&= \mu_y + (X_i - \upsilon_1)^T\beta \\
&\implies \psi^1_0 = \mu_y + (\upsilon_0 - \upsilon_1)^T\beta_1
\end{align*}
%
where the first equality follows from unconfoundedness, the second equality from consistency, the third from our parametric modeling assumptions, and the fourth by definition of $\alpha$. The final implication comes from plugging $\upsilon_0$ in place of $X_i$.
\end{proof}

We now outline the classical errors-in-variables model. Define 

\begin{equation}\label{eqn:cevoutcomemodel}
Y_i^a = \alpha_a + X_i^T\beta_a + \epsilon_i
\end{equation}
%
where $\epsilon_i \stackrel{iid}\sim N(0, \sigma^2)$ and $\mathbb{E}\{\epsilon_i \mid X_i, A_i\} = 0$. We consider the case where we observe $W_i$, a vector of mean-unbiased proxies for the true (unobserved) covariate vector $X_i$; i.e., $W_i = X_i + \nu_i$, where $\nu_i \stackrel{iid}\sim MVN(0, \Sigma_{\nu\nu})$. Consider the model:

\begin{equation}\label{eqn:errorcors}
(\epsilon_i, \nu_i) \sim MVN((0, 0), \begin{pmatrix} 
\sigma^2 & 0 \\ 
0 & \Sigma_{\nu\nu}  
\end{pmatrix}
\end{equation}
%
In other words the error in the model for the potential outcomes is uncorrelated with the error in the covariates. Let $u_i = (\epsilon_i, \nu_i)$. We further assume that the covariates are normally distributed and uncorrelated with any of the error terms:

\begin{equation}\label{eqn:xerrorcors}
(X_i, u_i) \mid A_i = a \sim MVN((\upsilon_a, 0), \begin{pmatrix} 
\Sigma_{XX} & 0 \\ 
0 & \Sigma_{uu}  
\end{pmatrix}
\end{equation}
%
Then we see that $(X_i, W_i) \mid A_i = a \stackrel{iid}{\sim} MVN((\upsilon_a, \upsilon_a), \Sigma)$\footnote{In contrast to our application, we instead assume here that $\Sigma_{WW \mid A = 1} = \Sigma_{WW \mid A = 0} = \Sigma_{WW}$ and $\Sigma_{XX \mid A = 1} = \Sigma_{XX \mid A = 0} = \Sigma_{XX}$. This is not a necessary assumption, but helps simplify notation.} where 

\begin{equation}\label{eqn:xwcors}
\Sigma = \begin{pmatrix} 
\Sigma_{XX} & \Sigma_{XX} \\ 
\Sigma_{XX} & \Sigma_{WW}  
\end{pmatrix}
\end{equation}
%
and $\Sigma_{WW} = \Sigma_{XX} + \Sigma_{\nu\nu}$. Let $\mathbf{\kappa} = \Sigma_{WW}^{-1}\Sigma_{XX}$. By the normality of the joint distribution of $X_i$ and $W_i$, we also know that

\begin{equation}
\mathbb{E}\{X_i \mid W_i, A_i = a\} = \upsilon_a + \mathbf{\kappa}^T(W_i - \upsilon_a)
\end{equation}
%
We consider estimating $\psi^1_0$ in this setting. Let 

\begin{align*}
\hat{\psi}^{1, reg}_0 = \bar{Y}_1 + (\bar{W}_0 - \bar{W}_1)^T\hat{\beta}_1
\end{align*}
%
where $\hat{\beta}_1$ is the OLS estimator of $\beta_1$, $\bar{W}_a = n_a^{-1}\sum_{i:A_i = a} W_i$ and $\bar{Y}_a$ is defined analogously. Let 

\begin{align*}
\hat{\psi}^{1, sbw}_0 = \sum_{i: A_i = 1} \hat{\gamma}_i Y_i
\end{align*}
%
be the SBW estimator with the vector of weights $\gamma$ solving Equation~\ref{eqn:sbwobjective} over the constraint set $\Gamma(W_1, \bar{W}_0, 0)$.\footnote{For simplicity we also assume throughout that there exists $\gamma \in \Gamma(\mathbf{Z}, \zeta, 0)$ for any $\zeta$ and $\mathbf{Z}$ defined in context.}

\begin{proposition}\label{cl1}
The bias of $\hat{\psi}^{1, reg}_0$ is equal to the bias of $\hat{\psi}^{1, sbw}_0$; specifically, 

\begin{align*}
\mathbb{E}\{\psi^{1, reg}_0 - \psi^1_0\} = \mathbb{E}\{\psi^{1, sbw}_0 - \psi^1_0\} = (\upsilon_0 - \upsilon_1)^T(\mathbf{\kappa} - I_q)\beta
\end{align*}
\end{proposition}

\begin{proof}
Recall that $\mathbb{E}(\hat{\beta}_1) = \kappa\beta_1$ (see, e.g., \cite{gleser1992importance}). Consider the error of $\hat{\psi}^{1, reg}_0$: 

\begin{align*}
    \hat{\psi}^{1, reg}_0 - \psi^1_0 &= \bar{Y}_1 + (\bar{W}_0 - \bar{W}_1)^T\hat{\beta}_1 - (\mu_y + (\upsilon_0 - \upsilon_1)^T\beta_1) \\
    &= \underbrace{(\bar{Y}_1 - \mu_y)}_{T_1} + \underbrace{(\bar{W}_0 - \upsilon_0)^T\hat{\beta}_1}_{T_2} - \underbrace{(\bar{W}_1 - \upsilon_1)^T\hat{\beta}_1}_{T_3} + \underbrace{(\upsilon_0 - \upsilon_1)^T(\hat{\beta}_1 - \beta_1)}_{T_4} \\
    \implies \mathbb{E}\{\hat{\psi}^{1, reg}_0 - \psi^1_0\} &= (\upsilon_0 - \upsilon_1)^T(\mathbf{\kappa} - I_d)\beta_1
\end{align*}
%
The first equality holds by definition and the second by rearranging terms. The second equality consists of four terms. $T_1$ is simply the estimation error from a sample average, which has expectation zero. $T_2$ is the product of the estimation error from a sample average and $\hat{\beta}$; this also has expectation zero because $\bar{X}_0$ is estimated on a different part of the sample than $\hat{\beta}_1$, so these errors are independent of $\hat{\beta}_1$. $T_3$ also has expectation zero because $\bar{W}_1^T\hat{\beta}_1 = \bar{Y}_1$, which has expectation $\mu_y$, and $\upsilon_1^T\hat{\beta}_1$ also has expectation $\mu_y$. We are then left with $T_4$; we substitute $\mathbb{E}\{\hat{\beta}\} = \mathbf{\kappa}\beta$ to get the final result. 

We now derive the bias of $\hat{\psi}^{1, sbw}$:

\begin{align*}
    \hat{\psi}^{1, sbw}_0 - \psi^1_0 &= \sum_{i: A_i = 1}\gamma_iY_i - (\alpha_1 + \upsilon_0^T\beta_1) \\
    &= \sum_{i: A_i = 1} \gamma_i(\alpha_1 + X_i^T\beta_1 + \epsilon_i) - (\alpha_1 + \bar{W}_0^T\beta_1 + (\upsilon_0 - \bar{W}_0)^T\beta_1) \\
    &= \sum_{i: A_i = 1} (\gamma_i(W_i - v_i)^T\beta_1 + \gamma_i\epsilon_i) - \bar{W}_0^T\beta_1 + (\upsilon_0 - \bar{W}_0)^T\beta_1 \\
    &= \underbrace{-\sum_{i: A_i = 1}\gamma_iv_i^T\beta_1}_{T_1} + \underbrace{\sum_{i: A_i = 1}\gamma_i\epsilon_i}_{T_2}  + \underbrace{(\upsilon_0 - \bar{W}_0)^T\beta_1}_{T_3}
\end{align*}
%
Conditioning on $W_i$, we can take expectations over $X_i$, and see that $T_2$ has expectation zero (noting that the weights, conditional on $W_i$, are independent of these errors). $T_3$ is simply the scaled sum of mean zero estimation error and therefore has expectation zero. We conclude by considering $T_1$ and again take expectations over $X_i$ conditional on $W_i$: 

\begin{align*}
    \sum_{i: A_i = 1} \gamma_i\mathbb{E}\{X_i - W_i \mid W_i\}^T\beta_1 &= \sum_{i: A_i = 1} \gamma_i (\upsilon_1 + \mathbf{\kappa}^T(W_i - \upsilon_1))^T\beta_1 - \sum_{i: A_i = 1}\gamma_i W_i^T\beta_1 \\
    &= (\upsilon_1 + \mathbf{\kappa}^T(\bar{W}_0 - \upsilon_1))^T\beta_1 - \bar{W}_0^T\beta_1 \\
    &= (\kappa^T(\bar{W}_0 - \upsilon_1))^T\beta_1 - (\bar{W}_0 - \upsilon_1)^T\beta_1  \\
    &= (\bar{W}_0 - \upsilon_1)^T(\mathbf{\kappa} - I_d)\beta_1 \\
    &= (\upsilon_0 - \upsilon_1)^T(\mathbf{\kappa} - I_d)\beta_1 + (\bar{W}_0 - \upsilon_0)^T(\mathbf{\kappa} - \mathbf{I}_q)\beta_1 \\
    \implies \mathbb{E}\{\hat{\psi}^{1, sbw}_0 - \psi^1_0\} &= (\upsilon_0 - \upsilon_1)^T(\mathbf{\kappa} - I_d)\beta_1
\end{align*}
%
The final line holds because in the second to last line, we can take expectation over $W_i$ and see that the second term is a scaled sum of mean zero estimation error and has expectation zero. 
\end{proof}

Let $\hat{\psi}^{1, sbw}_0(\eta_1(W_1), \upsilon_1, 0))$ be the SBW estimator defined over the constraint set $\Gamma(\eta_1(W_1), \upsilon_1, 0)$.

\begin{proposition}
The estimator $\hat{\psi}^{1, sbw}_0(\eta_1)$ is unbiased: that is,
$\mathbb{E}\{\hat{\psi}^{1, sbw}_0(\eta_1)\} = \psi_0^1$
\end{proposition}

\begin{proof}

Assuming no model error, by linearity we know that

\begin{align*}
Y_i^1 = \alpha_1 + \eta_a(W_i)^T\beta_1 + (X_i - \eta_a(W_i))^T\beta_1
\end{align*}

We then have that:

\begin{align*}
    \hat{\psi}^{1, sbw}_0(\eta_1) - \psi^1 &= \sum_{i: A_i = 1}\gamma_i^\star Y_i - (\alpha_1 + \upsilon_0^T\beta_1) \\
    &= \sum_{i: A_i = 1}\gamma_i^\star\alpha_1 + \sum_{i: A_i = 1}\gamma_i^\star\eta_1(W_i)^T\beta_1 + \sum_{i: A_i = 1}\gamma_i^\star(X_i - \eta_1(W_i))^T\beta_1 - (\alpha_1 + \upsilon_0^T\beta_1) \\
    &= \sum_{i: A_i = 1}\gamma_i^\star(X_i - \eta_1(W_i))^T\beta_1
\end{align*}

Conditional on $W$, the weights are fixed and $X_i - \eta_1(W_i)$ has expectation zero; therefore, the estimator is unbiased.
\end{proof}
\begin{remark}

While we have assumed no model error, this estimator still has variance, conditional on $W_i$, equal to

\begin{align*}
\sum_{i: A_i = 1} \gamma_i^{\star^2}\beta_1^T Cov(X_i \mid W_i)\beta_1
\end{align*}

where, assuming that $(X_i, W_i)$ are jointly normally distributed, $Cov(X_i \mid W_i) = \Sigma_{XX} - \Sigma_{XX}\Sigma_{WW}^{-1}\Sigma_{XX}$. Therefore, the variance of this estimator is higher than if we knew the true $X_i$ unless $\Sigma_{WW} = \Sigma_{XX}$ (i.e. we observe $X_i$). 
\end{remark}

\begin{remark}
Assuming there is a model error simply leads to the additional term $\sum_{i: A_i = 1}\gamma_i\epsilon_i$. This again has expectation zero, because the weights remain independent of the error in the outcome model, and adds a term to the total variance (conditional on $W_i$) equal to

\begin{align*}
\sigma^2\sum_{i: A_i = 1}\gamma_i^{\star^2}
\end{align*}
\end{remark}

\begin{remark}
    This proof (and general estimation technique) relies on viewing $X$ as random, while in the paper we define our target estimand conditional on $X$. If we consider $X$ fixed we can define $\eta_1(W, n) = \bar{X}_1 + \Sigma_{WW}(n)^{-1}\Sigma_{XX}(n)(W_i - \bar{X}_1)$, where $\bar{X}_1$ is the sample average among the treated units and $\Sigma_{WW}(n)$ and $\Sigma_{XX}(n)$ are the sample covariance matrices. Assume that the limit of these quantities as $n \to \infty$ exists and are $\eta_1$, $\upsilon_1$, $\Sigma_{WW}$, and $\Sigma_{XX}$, respectively. We can then interpret these terms in the final expression as the limit of these sample statistics as $n \to \infty$, and again we see that this term approaches zero.
\end{remark}

Proposition 2 assumes that we know $\eta_1$ and $\upsilon_a$; however, in practice we estimate it from the data. Moreover, the estimation of $\hat{\eta}_1$ typically involves both the observed dataset and auxillary data, which we have not yet specified here. We given an example of this below.

Recall that $\eta_1(W_i) = \upsilon_1 + \kappa^T(W_i - \upsilon_1)$, where $\kappa = \Sigma_{WW}^{-1}\Sigma_{XX}$. We can easily estimate $\upsilon_a$ consistently using $\bar{W}_a$; the challenge is estimating $\kappa$. 

Following \cite{gleser1992importance}, consider the setting where we observe $T$ independent vectors of measurements $W_i^\star$ from known values $X_i^\star$, so that $v_i^\star = W_i^\star - X_i^\star$. Assume that $\Sigma_{v^\star v^\star} = \Sigma_{vv}$. We can then estimate $\hat{\Sigma}_{vv} = \frac{1}{T}\sum_{i=1}^T(W_i^\star - X_i^\star)'(W_i^\star - X_i^\star)$. We can then estimate

\begin{align*}
\hat{\kappa} = (n\hat{\Sigma}_{WW})^{-1}(n(\hat{\Sigma}_{WW} - \hat{\Sigma}_{vv}))
\end{align*}
%
assuming $n(\hat{\Sigma}_{WW} - \hat{\Sigma}_{vv})$ is positive semi-definite, and where $\hat{\Sigma}_{WW} = \frac{1}{n}\sum_{i=1}^n (W_i - \bar{W})(W_i - \bar{W})'$. 

\begin{proposition}
Let $\tilde{\gamma} = \tilde{\gamma}(\hat{\eta}_1, \bar{W}_0, 0)$ be the weights that solve the SBW objective over the constraint set $\Gamma(\hat{\eta}_1, \bar{W}_0, 0)$. The estimator $\sum_{i: A_i = 1}\tilde{\gamma}_iY_i \to \psi^1_0$ as $n \to \infty$, $T \to \infty$.
\end{proposition}

\begin{proof}
Rewriting the error expression from Proposition 2 leads to the additional terms $\sum_{i: A_i = 1}\tilde{\gamma}_i(\eta_1(W_i) - \hat{\eta_1}(W_i))^T\beta_1$ and $(\upsilon_0 - \bar{W}_0)^T\beta_1$. The second term is scaled estimation error and has expectation zero. It therefore suffices to show that 

\begin{align*}
\sum_{i: A_i = 1}\tilde{\gamma}_i(\eta_1(W_i) - \hat{\eta_1}(W_i))^T\beta_1 \to 0
\end{align*}

By the weak law of large numbers, $\hat{\Sigma}_{WW} - \hat{\Sigma}_{\nu\nu} \to \Sigma_{XX}$ as $n \to \infty$, $T \to \infty$; similarly $\hat{\Sigma}_{WW} \to \Sigma_{WW}$ as $n \to \infty$. By the continuous mapping theorem $\hat{\kappa} \to \kappa$. Since $\bar{W}_0 \to \upsilon_0$, we have that $\hat{\eta_1} \to \eta_1$. 

Let $\gamma^\star$ be the weights defined in Proposition 2. We can then rewrite the error term above as

\begin{align*}
\sum_{i: A_i = 1}(\dot{\gamma}_i - \gamma_i^\star)(\hat{\eta}_1(W_i) - \eta(W_i))^T\beta_1 + \sum_{i: A_i = 1}\gamma_i^\star(\hat{\eta}_1(W_i) - \eta_1(W_i))^T\beta_1
\end{align*}

As $n \to \infty$, $T \to \infty$, $\hat{\eta}_1 \to \eta_1$, and therefore by definition of the SBW objective, $\dot{\gamma}_i \to \gamma_i^\star$. These two terms therefore both converge in probability to zero. 
\end{proof}

\begin{remark}
While here we have specified a particular form of auxillary data used to estimate $\Sigma_{\nu\nu}$, we can be agnostic about the specifics of the auxillary data so long as $\hat{\Sigma}_{\nu\nu}$ is a consistent estimate of $\Sigma_{\nu\nu}$. For example, in our application we use the replicate survey weights from the ACS microdata to estimate this quantity and we do not require access to a known $X_i^\star$ for any observation for our estimation procedure.
\end{remark}

\subsection{Properties of the H-SBW objective}\label{app:AsecII}

We now consider properties of the H-SBW estimator. In Proposition 4 we prove the properties of the H-SBW estimator assuming the true covariates $X$ are observed. In Proposition 4 we show that under a linear model with equi-correlated within-state errors the H-SBW estimator produces the minimum conditional-on-X variance estimator within the constraint set. In Proposition 5 we show that when relaxing the positivity constraint on $\Gamma$ the H-SBW weights are equivalent to the GLS weights. In Lemma 1 we establish that the solution to the constrained H-SBW weights is equivalent to the GLS solution on some subset of the data. These results nest SBW and OLS as special cases.

Assume that we observe $M$ states indexed by $s$ and $p_s$ CPUMAs per state, indexed by $c$. Let $X = (X_0, X_1)^T$ be the $n$ by $q$ matrix of covariates comprised of sub-matrices $X_1$ and $X_0$ with $n_1$ and $n_0$ units, respectively.

\begin{proposition}
    Consider the outcome model in Equation~\ref{eqn:linmod} and let $\rho'$ be the within-state correlation of the error terms. Let $\hat{\gamma} = \hat{\gamma}(\rho', X, \bar{X}_0, \delta')$ be the weights that solve Equation~\ref{eqn:objective} for $\rho = \rho'$ across the constraint set $\Gamma(X_1, \bar{X}_0, \delta')$. $\bar{Y}_0^1 = \sum_{sc}\hat{\gamma}_{sc}Y_{sc}$ is the minimum conditional-on-X variance estimator within the constraint set $\Gamma(X_1, \bar{X}_0, \delta')$.
\end{proposition}

\begin{proof}
\begin{align*}
    Var(n_t^{-1}\sum_{sc: A_s = 1}\gamma_{sc}Y_{sc} \mid X_{sc}, A_{sc}) &= n_t^{-2}\sum_{s: A_s = 1}^{m_1}(\sum_{c = 1}^{p_s}\gamma_{sc}^2(\sigma^2 + \eta^2) + \sum_{c \ne d}\gamma_{sc}\gamma_{sd}\eta^2) \\
    &\propto \sum_{s: A_s = 1}(\sum_{c = 1}^{p_s}(\gamma_{sc}^2 + \sum_{c \ne d}\rho \gamma_{sc}\gamma_{sd})
\end{align*}
%
where the second line follows by dividing by $\eta^2 + \sigma^2$. By definition of the H-SBW objective, which minimizes this function for known $\rho'$, the H-SBW estimator must produce the minimum conditional-on-X variance estimator within the constraint set $\Gamma(X, \bar{X}_0, \delta')$.
\end{proof}

\begin{proposition}
    Consider the constraint set $\Gamma_U(Z, \zeta, 0) = \{\gamma: \sum_{sc}\gamma_{sc}Z_{sc} = \zeta, \sum_{A_{sc} = 1}\gamma_{sc} = 1\}$. Define $V_1 = (1, X_1)$. The H-SBW weights optimized over $\Gamma_U(V_1, \bar{V}_0, 0)$ are equivalent to the Oaxaca-Blinder GLS weights $(\bar{V}_0^T(V_1^T\Omega^{-1}V_1)^{-1}V_1\Omega^{-1})^T$. 
\end{proposition}

\begin{proof}
    We show this by using the duality of the optimization problem. We first consider the primal problem:
    
    \begin{align*}
        \min_{\gamma} \frac{1}{2}\gamma^T\Omega\gamma \text{ st } V_1^T\gamma = \bar{V}_0
    \end{align*}
    
    The Lagrangian is
    
    \begin{align*}
        \mathcal{L}(\gamma, \beta) = \frac{1}{2}\gamma^T\Omega\gamma - (V_1^T\gamma - \bar{V}_0)^T\beta
    \end{align*}
    
    Taking the first order conditions with respect to $\gamma$ implies that the minimizer $\hat{\gamma}$ satisfies:
    
    \begin{align*}
        \hat{\gamma} = \Omega^{-1}V_1\beta
    \end{align*}

    We can then define the dual problem $q(\beta)$:
    
    \begin{align*}
        q(\beta) = -\frac{1}{2}(V_1\beta)^T\Omega^{-1}(V_1\beta) + \bar{V}_0^T\beta
    \end{align*}
    
    We can then solve for $\min_{\beta} -q(\beta)$ by taking the first order conditions with respect to $\beta$ and solving for the minimizer $\hat{\beta}$. This yields:
    
    \begin{align*}
        & V_1^T\Omega^{-1}V_1\beta = \bar{V}_0 \\
        &\implies \hat{\beta} = (V_1^T\Omega^{-1}V_1)^{-1}\bar{V}_0
    \end{align*}
    
    By strong duality the dual problem which minimizes $-q(\beta)$ is equivalent to the primal problem. Given $\hat{\beta}$, we then see that $\hat{\gamma}$ satisfies:
    
    \begin{align*}
        \hat{\gamma} &= \Omega^{-1}V_1(V_1^T\Omega^{-1}V_1)^{-1}\bar{V}_0 \\
        &= (\bar{V}_0^T(V_1^T\Omega^{-1}V_1)^{-1}V_1^T\Omega^{-1})^T
    \end{align*}
    
    These are simply the GLS weights, thus concluding the proof.
\end{proof}

\begin{remark}
    Because the first element of the vector $\bar{V}_0$ and the first column of the matrix $V_1$ are 1, this implies that $\sum_{A_{sc} = 1}\gamma_{sc}V_{1, sc} = \sum_{A_{sc} = 1}\gamma_{sc}1 = 1$; therefore, the summing to one constraint was implied by the inclusion of an intercept.
\end{remark}

\begin{remark}
    If $\Omega = \sigma^2I_n$ this Proposition implies that OLS weights are equivalent to the SBW weights for constraint set $\Gamma_U(X_1, \bar{X}_0, 0)$. Other generalizations of this connection with respect to ridge-regression are found in \cite{ben2018augmented}; see also \cite{chattopadhyay2021implied}.
\end{remark}

\begin{proposition}
Consider covariates $Z_{11}, ..., Z_{mp_m}$ where the first element is an intercept, and where the rows are indexed by state $s$ and region $c$ with $p_s$ regions per state -- $11, ..., 1p_1, ..., mp_m$. Define $\hat{\gamma}^{GLS}(Z, \zeta)$ to be an n-dimensional vector of weights defined by:

\begin{align*}
    \hat{\gamma}_{sc}^{GLS} = \zeta^T(Z^T\Omega^{-1}Z)^{-1}(Z\Omega^{-1})_{sc}
\end{align*}

Let $\hat{\gamma}^{H-SBW}(Z, \zeta, 0)$ be the minimizer across $\Gamma(Z, \zeta, 0)$, and assume this is feasible. If so, for any dataset $X$, there exists a subset of data points $S = \{X_{sc}: sc \in \mathcal{I}\}$ such that $\hat{\gamma}^{GLS}(S, \zeta', 0) = \hat{\gamma}^{H-SBW}(X, \zeta', 0)$.
\end{proposition}

\begin{proof}

We begin by proving Lemma~\ref{lemma:a1}.

\begin{lemma}\label{lemma:a1}
Define $\mathcal{S}_1 := \{\sum_{i=1}^n a_iX_i = \zeta', a_i \ge 0\}$ and $\mathcal{S}_2 := \{\sum_{i=1}^n a_iX_i = \zeta'\}$. Define $f(\gamma) = \gamma'\Omega\gamma$ for $\Omega \succ 0$. Then let $a_1$ be a solution (assumed feasible) to $\min_a f(a) \text{ st } a \in \mathcal{S}_1$ and $a_2$ be a solution to $\min_a f(a) \text{ st } a \in \mathcal{S}_2$. If $a^1_i > 0$ for all elements, then $a_1 = a_2$.
\end{lemma}

\begin{proof}
We show the contrapositive: if $a_1 \ne a_2$ then there must be an element $i$ such that $a_{1, i} = 0$. Because $\mathcal{S}_1 \subseteq \mathcal{S}_2$, if $a_2 \in \mathcal{S}_1$, then it must also minimize $f(\gamma) \text{ st } \gamma \in \mathcal{S}_1$. This implies that if $a_2 \ne a_1$, then $a_2 \not\in \mathcal{S}_1$.

Consider the case where $a_2 \ne a_1$. We know that $f(a_2) \le f(a_1)$. Define $\mathcal{S}_{a_1} := \{a: f(a) \le f(a_1)\}$ and note that $a_2$ must be in this set. Because $f$ is convex, this set is convex and there exists a line within the set connecting $a_1$ and $a_2$. Since $a_1 \in \mathcal{S}_1$ and $a_2 \not\in\mathcal{S}_1$ then this line segment must intersect $\mathcal{S}_1$ at the boundary at a point $a^\star \in \mathcal{S}_{a_1}$. But then we know that $f(a^\star) \le f(a_1)$; moreover, because $a^\star \in \mathcal{S}_1$ this implies by definition that $a^\star = a_1$. 

Since $a_1$ is on the boundary of $\mathcal{S}_1$, we know that some element in $a_1$ must be equal to zero. 
\end{proof}

Now define $v_{sc} = \mathds{1}(\hat{\gamma}_{sc}^{H-SBW} > 0)$ and define $S = \{X_{sc}: v_{sc} = 1\}$. Define $\hat{\gamma}_1(S, \zeta', 0)$ to be the minimizer across $\Gamma(S, \zeta, 0)$ and $\hat{\gamma}_2(X, \zeta', 0)$ analagously. 

We assert that $\hat{\gamma}_1 = \hat{\gamma}_2$. Across $n$ rows of $X$, define $\hat{\gamma}_1^\star = v_{sc}\hat{\gamma}_{1, sc}$. Notice that $\sum_{sc}\gamma_{1, sc}^\star X_{sc} = \sum_{sc}\gamma_{2, sc} X_{sc} = \zeta'$. If these weights are not equivalent then $\sum_{sc}\hat{\gamma}_{1, sc}^{\star, 2}v_{sc}\le \sum_{sc}\hat{\gamma}_{2, sc}^2$. This then implies that $\hat{\gamma}_2$ is not the minimizer across $\Gamma(X, \zeta', 0)$, which is a contradiction. Therefore the weights must be equal.

Finally, define the GLS weights:

\begin{align*}
    \gamma_{sc}^\star = \zeta^T(S^T\Omega^{-1}S)^{-1}(S\Omega^{-1})_{sc}
\end{align*}

Since we know that the H-SBW solution $\gamma_1 > 0$ is feasible, by Lemma~\ref{lemma:a1} we conclude $\gamma^\star = \gamma_1$.
\end{proof}

\begin{remark}
Taking $\rho = 0$ shows that the SBW weights are equivalent to OLS weights on some subset of the data.
\end{remark}

This highlights the connection between regression weights and H-SBW weights; this also shows us that we can express H-SBW weights that achieve exact balance as regression weights estimated on a  subset of the data. We will use this fact in the following section.

\subsection{H-SBW with measurement error}\label{app:AsecIII}

We conclude by analyzing the performance of H-SBW in the context of measurement error. Throughout our analysis, we assume a positive equi-correlation among our covariates within each state. While our results are specific to this assumed correlation structure, this particular example illustrates the potential problems that come from using H-SBW with regression-calibration. In Propositions 7 we define the conditions where the H-SBW objective will yield a solution across $\Gamma(\eta_1(W), \upsilon_0, 0)$ that balances the true covariates $X$ in expectation. In Proposition 8 we show that if our covaiate adjustment procedure correctly models the correlation structure of the covariates then H-SBW will balance the true covariates in expectation and therefore yield an unbiased estimate of the parameter $\bar{Y}_0^1$.

We first specify the correlation structure of the data. Assume $X_{sc} \sim N(\mu_j, \Sigma_{XX})$, $\mu_j \sim N(\upsilon_1, \Sigma_{SS})$. Therefore $Cov(X_{sc}, X_{s'd}) = \Sigma_{SS}\mathds{1}(s = s', c \ne d)$. We also assume that the measurement error model defined in Equations~\ref{eqn:errorcors}, ~\ref{eqn:xerrorcors}, and ~\ref{eqn:xwcors} holds. These models imply that $Cov(W_{sc}, W_{s'd}) = \Sigma_{SS}\mathds{1}(s = s', c \ne d)$ and that $Cov(W_{sc}, X_{s'd}) = \Sigma_{SS}\mathds{1}(s = s', c \ne d)$. We assume for simplicity throughout that each state has $p_s$ regions, and that $p_s$ is constant across all states.

We then have the entire data $(X_{11}, ..., X_{1p_1}, ..., X_{sp_s}, W_{11}, ..., W_{1p_1}, ..., W_{sp_s} \sim N(\mathbf{\upsilon}_1, \Sigma)$, where 

\begin{align*}
\Sigma &= \begin{pmatrix}
\Sigma_{X} & \Sigma_{X} \\
\Sigma_{X} & \Sigma_{W}
\end{pmatrix} 
\end{align*}
%
where $\Sigma_X$ and $\Sigma_W$ are block-diagonal matrices with $p_s$ by $p_s$ blocks indexed by $b_s$ $\Sigma_{X, b_s}$ and $\Sigma_{W, b_s}$, with each element containing a $d$ by $d$ matrix: 

\begin{align*}
\Sigma_{X, b_s} &= \begin{pmatrix}
\Sigma_{XX}, \Sigma_{SS}, ..., \Sigma_{SS} \\
\Sigma_{SS}, \Sigma_{XX}, ..., \Sigma_{SS} \\
... \\
\Sigma_{SS}, \Sigma_{SS}, ..., \Sigma_{XX}
\end{pmatrix};
\Sigma_{W, b_s} = \begin{pmatrix}
\Sigma_{WW}, \Sigma_{SS}, ..., \Sigma_{SS} \\
\Sigma_{SS}, \Sigma_{WW}, ..., \Sigma_{SS} \\
... \\
\Sigma_{SS}, \Sigma_{SS}, ..., \Sigma_{WW} \\
\end{pmatrix}
\end{align*}
%
Then by the conditional normal distribution, we define:

\begin{align*}
    \mathbb{E}\{X_{sc} \mid W\} &= \eta_{sc}^\star(W) = \upsilon_1 + ((\Sigma_W^{-1}\Sigma_X)^T(W - \upsilon_1))_{sc} \\
    &= \upsilon_1 + A(X_{sc} - \upsilon_1) + \sum_{s, c\ne d}B(X_{sd} - \upsilon_1)
\end{align*}
%
for some $d$ by $d$ matrices $A$ and $B$. Notice that if $\Sigma_{SS} = 0$ (ie all the covariates are uncorrelated), then $B = 0$ and $A = (\Sigma_{WW}^{-1}\Sigma_{XX})^T$ and so $\eta^\star_{sc}(W) = \eta(W_{sc})$.

Assume that this data represents ``treated'' units. Our goal is to reweight this data to some target $\upsilon_0$ (for example, the mean of the control units). Of course, we can think more generally about the problem of reweighting some data with common mean $\upsilon$ and covariance structure $\Sigma$ to any target $\zeta$.

\begin{proposition}
    The GLS weights that fall in the constraint set $\Gamma_U(\eta_1(W), \zeta, 0)$ do not necessarily balance the true covariates asymptotically (i.e. the estimator is inconsistent) unless one of the following conditions holds:
    \begin{itemize}
    \item $\Sigma_{SS} = 0$; that is, $X$ is uncorrelated across regions
    \item $p_s = 1$; that is, there is only one region per state (note that this forces the first condition to be true)
    \item $\kappa = I_d$; that is, there is no measurement error
    \item $\Omega = I_n$; that is, the GLS weights are OLS weights
    \end{itemize}
\end{proposition}

\begin{proof}
For ease of exposition and conceptual clarity, we simplify the setting further and consider $X_{sc} \in \mathbb{R}$ and consider the regression weights without an intercept. This relaxes the summing to 1 constraint on the weights, which would desirable if our assumed model of the outcome does not contain an intercept. All of the intuition for this result comes from this simple case; although, we prove the more general case below.

Consider the treated unit covariate vector $X_1$ and without loss of generality, assume $\upsilon_1 = 0$. Consider the target $\upsilon_0$ and $\hat{X}_{sc} = \eta_{1, sc} = \frac{\sigma^2_x}{\sigma^2_w}W_{sc} = \kappa W_{sc}$. 

We can write the GLS weights with respect to the correlation matrix $\Sigma$ (defined above):

\begin{align*}
\hat{\gamma}_{sc} = (\sum_{s}\sum_{c=1}^{p_s} a_{1} \hat{X}_{sc}^2 + a_{2} \sum_{c\ne d} \hat{X}_{sc}\hat{X}_{sd})^{-1}(a_{1}\hat{X}_{sc} + a_{2}\sum_{c\ne d} \hat{X}_{sd})\zeta
\end{align*}
%
where $a_1$ is any of the diagonal elements of $\Omega^{-1}$, and $a_2$ represents the constant in any within-block off-diagonal element (note that these are constant across states because we are assuming an equal number of regions per state; otherwise, these would differ by state).

Next, we define 
    \begin{align*}
        \sum_{sc} \hat{\gamma}_{sc} X_{sc} &= (\sum_{s}\sum_{c=1}^{p_s} a_{1} \hat{X}_{sc}^2 + a_{2} \sum_{c\ne d} \hat{X}_{sc}\hat{X}_{sd})^{-1}(\sum_{s}\sum_{c=1}^{p_s}a_{1}X_{sc}\hat{X}_{sc} + X_{sc}\sum_{c\ne d}a_{2}\hat{X}_{sd})\zeta \\
        &= T_1^{-1}T_2\zeta
    \end{align*}

For $\gamma$ to asymptotically reweight $X$ to $\zeta$, we require that $T_1^{-1}T_2 \to 1$ as $n \to \infty$. However,

    \begin{align*}
        T_1 &\to \sum_s \kappa^2 (a_{1s} p_s\sigma^2_w + a_{2s} p_s(p_s - 1)\sigma^2_s)\\
        &= \sum_s \kappa a_{1s} p_s\sigma^2_x + \kappa^2a_{2s}p_s(p_s - 1)\sigma^2_s \\
        T_2 &\to \sum_s \kappa (a_{1} p_s\sigma^2_x + a_{2}p_s(p_s - 1)\sigma^2_s)
    \end{align*}
    
 The proposition follows by observing that $T_1^{-1}T_2 \not\to 1$ unless one of the aforementioned conditions is met.
\end{proof}    
    
\begin{remark}
    The expected imbalance comes from the estimate of the covariance $\sigma^2_s$ in the denominator, based on $\sum \hat{X}_{sc}\hat{X}_{sd}$. The GLS weights (as defined above) prevent our weights from balancing $X_{sc}$ due to the dependence on this misestimate. OLS weights do not have this dependence and are therefore unbiased.
\end{remark}

\begin{remark}
    Notice that $a_1 \ge 0$ and $a_2 \le 0$ for any $\Omega$ defined above where $p_s > 1$. The numerator is therefore less than or equal to than the denominator, meaning that the achieved balance $\zeta'$ ($\sum_{sc}\gamma_{sc}X_{sc}$) will be less than or equal to $\zeta$.
\end{remark}

For completeness, we consider the more general case below with a full covariate vector (that may include an intercept term) $X_{sc}$. 

\begin{proof}
    Let $\hat{X} = W\kappa$. Consider $\hat{\gamma}_{sc} = \zeta'^T(\hat{X}^T\Omega^{-1}\hat{X})^{-1}(\hat{X}\Omega^{-1})_{sc}$. We want to derive an expression for 
    
    \begin{align*}
        \sum_{sc}X_{sc}\hat{\gamma}_{sc}X_{sc} &= \sum_{sc}X_{sc}(\zeta'^T(\hat{X}^T\Omega^{-1}\hat{X})^{-1}(\hat{X}\Omega^{-1})_{sc}) \\ 
        &= \zeta'^T(\hat{X}^T\Omega^{-1}\hat{X})^{-1}\sum_{sc}X_{sc}(\hat{X}\Omega^{-1})_{sc}^T
    \end{align*}
    
    Then we have that
    
    \begin{align*}
        \hat{X}^T\Omega^{-1}\hat{X} &\to
        \sum_s \kappa^T(a_1p_s\Sigma_{WW} + p_s(p_s - 1)a_2\Sigma_{SS})\kappa\\
        &= \sum_s a_1p_s\Sigma_{XX}\kappa + \kappa^T p_s(p_s - 1)a_2\Sigma_{SS} \kappa  \\
        \sum_{sc}X_{sc}(\hat{X}\Omega^{-1})_{sc}^T &= \sum_{sc}X_{sc}(\sum_{s}a_{1}\hat{X}_{sc}^T + a_{2}\sum_{c\ne d}\hat{X}_{sd}^T) \\
        &= \sum_s(p_s a_1 \Sigma_{XX} + p_s(p_s - 1) a_2 \Sigma_{SS})\kappa + o_p(1)
    \end{align*}
    
    For $\hat{\gamma}$ to balance $X_{sc}$ to $\zeta'$ asymptotically, we require that $T_1^{-1}T_2 \to I_q$. We again see that this won't hold unless one of the conditions in Proposition 8 holds.
\end{proof}

\begin{remark}
Similar to the univariate case above, we see that balance is not achieved unless $\kappa = I_q$, $\Sigma_{SS} = 0$, $p_s = 1$, or $A_2 = 0$. In other words, we cannot achieve balance asymptotically in expectation unless (1) there is no measurement error; (2) $X_{sc}$ and $X_{sd}$ are uncorrelated; (3) there is only one county per state; (4) $\Omega$ is a diagonal matrix (which implies that $A_2 = 0$).
\end{remark}

\begin{remark}
While we study the OLS and GLS weights, as we noted above in Proposition 6, the H-SBW weights are equivalent to the GLS weights on some subset of the data. Therefore, these results also generally apply to the H-SBW weights optimized over the constraint set $\Gamma(\eta_1(W), \upsilon_0, 0)$. However, the asymptotic bias for the H-SBW weights may differ because the subset of the data where the GLS and H-SBW weights are equivalent will in general include fewer observations per block. 
\end{remark}

We conclude by noting that there is a possible correction for this bias: balance on $\eta^\star(W)$ rather than $\eta(W)$. 

\begin{proposition}
    Assuming the models defined above, for any $\rho$, the H-SBW estimator that reweights $\eta^\star$ to $\upsilon_0$ is unbiased.
\end{proposition}

\begin{proof}
    For simplicity, assume the following model holds without any error:
    
    \begin{align*}
        Y_{sc} = X_{sc}^T\beta_1
    \end{align*}
    
    Consider $\eta_{1, sc}(W)$ as defined above. By linearity, we can rewrite this model as:
    
    \begin{align*}
        Y_{sc} = (X_{sc} - \eta_{a, sc}(W))^T\beta_1 + \eta_{1, sc}(W)
    \end{align*}
    
    This implies that the error can be expressed as:
    
    \begin{align*}
        \hat{\psi}^1_0 - \psi^1_0 = \sum_{sc}\gamma_{sc}(X_{sc} - \eta^\star_{1, sc}(W))^T\beta
    \end{align*}
    
    Conditional on $W$, $\gamma_{sc}$ is fixed and $\mathbb{E}\{X_{sc} \mid W\} = \eta^\star$ so this expression is equal to 0.
\end{proof}

\begin{remark}
    This proposition shows us that if we correctly model the correlation structure of the data in our regression calibration step, we can use GLS or H-SBW without inducing bias (assuming all of our models are correct). This is the approach followed in \cite{huque2014impact}, who consider runing GLS with a spatial correlation structure in hte context of a one-dimensional covariate measured with error. 
\end{remark}

\begin{remark}
    These results are specific to the correlation structure $\Omega$, defined above. For other assumed correlation structures -- either for the outcomes or covariates -- we expect that biases would also arise from using the naive regression calibration adjustment, though we do not derive more general results here.
\end{remark}

\begin{remark}
    Our previous remarks in Section~\ref{app:AsecI} with respect to the using estimates of $\hat{\eta}$ and $\hat{\upsilon}$ all apply here: the consistency of such an estimator follows as $n \to \infty$, $T \to \infty$ following the estimation procedure proposed in that same section.
\end{remark}
