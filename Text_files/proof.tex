\section{Proofs}

We show three results on the bias of the SBW estimator under the classical errors-in-variables model. First, we show that the bias of the SBW estimator that sets $\delta = 0$ is equal to the bias of the OLS estimator. Second, we show that if the observed covariate values can be replaced by their conditional expectations $\eta$ given the noisy observations, then the SBW estimator will have zero bias. Finally, we consider the case where we estimate $\eta$ using auxillary data to estimate the covariance matrix of the error terms and show that the SBW estimator is consistent if we replace $\eta$ by an estimate $\hat{\eta}$ in the constraint set.

Consider a dataset of consists of $i = 1, ..., n$ units where we observe the outcomes $Y_i$ and a treatment assignment indicator $A_i \in \{0, 1\}$. Let $n_a$ be the number of units in each treatment group, i.e, $n_a = \sum_{i = 1}^n \mathbbm{1}(A_i = a)$. Let $X_i \in \mathbb{R}^d$ be a vector of covariates. 

Our target parameter is $\psi = \mathbb{E}\{Y_i^1 \mid A_i = 0\}$. We assume unconfoundedness ($Y_i^a \perp A_i \mid X_i$), consistency ($Y_i^a = Y_i \mid A_i = a$), and that $\mathbb{E}\{Y_i \mid X_i, A_i = a\} = \alpha_a + X_i^T\beta_a$. We begin by establishing the following identity:

\begin{equation}
\psi = \mu_y + (\mu_0 - \mu_1)^T\beta_1
\end{equation}

where $\mu_y = \mathbb{E}\{Y_i \mid A_i = 1\}$ and $\mu_a = \mathbb{E}\{X_i \mid A_i = a\}$.

\begin{proof}
By the law of iterated expectations, $\psi = \int \mathbb{E}\{Y_i^1 \mid X_i, A_i = 0\}p(x \mid a = 0)dx$. Using our causal and modeling assumptions we have that:

\begin{align*}
\mathbb{E}\{Y_i^1 \mid X_i, A_i = 0\} &= \mathbb{E}\{Y_i^1 \mid X_i, A_i = 1\} \\ 
&= \mathbb{E}\{Y_i \mid X_i, A_i = 1\} \\
&= \alpha + X_i^T\beta \\
&= \mu_y + (X_i - \mu_1)^T\beta \\
&\implies \psi = \int (\mu_y + (X_i - \mu_1)^T\beta) p(x \mid a = 0)dx \\
&= \mu_y + (\mu_0 - \mu_1)^T\beta
\end{align*}

where the first equality follows from unconfoundedness, the second equality from consistency, the third from our parametric modeling assumptions, and the fourth by definition of $\alpha$. The final implication and result follow from plugging these expressions back into the integral and solving.

\end{proof}

We now outline the classical errors-in-variables model. Let $Y_i(X_i, a) = \alpha_a + X_i^T\beta_a + \epsilon_i$ where $\epsilon_i \stackrel{iid}\sim N(0, \sigma^2)$. We consider the case where we observe $W_i$, a vector of mean-unbiased proxies for the true (unobserved) covariate vector $X_i$; i.e., $W_i = X_i + v_i$, where $v_i \stackrel{iid}\sim MVN(0, \Sigma_{vv})$. Consider the model:

\begin{equation}
(\epsilon_i, v_i) \sim MVN((0, 0), \begin{pmatrix} 
\sigma^2 & 0 \\ 
0 & \Sigma_{vv}  
\end{pmatrix}
\end{equation}

In other words the error in the outcome model is uncorrelated with the error in the covariates. Let $u_i = (\epsilon_i, v_i)$. We further assume that the covariates are uncorrelated with any of the error terms:

\begin{equation}
(X_i, u_i) \sim MVN((\mu_a, 0), \begin{pmatrix} 
\Sigma_{XX} & 0 \\ 
0 & \Sigma_{uu}  
\end{pmatrix}
\end{equation}

Then we see that $(X_i, W_i) \mid A_i = a \stackrel{iid}{\sim} MVN((\mu_a, \mu_a)^T, \Sigma)$\footnote{For simplicity we have also assumed that $\Sigma_{WW \mid A = 1} = \Sigma_{WW \mid A = 0} = \Sigma_{WW}$ and $\Sigma_{XX \mid A = 1} = \Sigma_{XX \mid A = 0} = \Sigma_{XX}$. This is not a necessary assumption, but helps simplify notation.} where 

$$
\Sigma = \begin{pmatrix} 
\Sigma_{XX} & \Sigma_{XX} \\ 
\Sigma_{XX} & \Sigma_{WW}  
\end{pmatrix}
$$ 

and $\Sigma_{WW} = \Sigma_{XX} + \Sigma_{vv}$. Let $\kappa = \Sigma_{WW}^{-1}\Sigma_{XX}$. By the normality of the joint distribution of $X_i$ and $W_i$, we also know that

\begin{equation}
\mathbb{E}\{X_i \mid W_i, A_i = a\} = \mu_a + \kappa^T(W_i - \mu_a)
\end{equation}

We consider estimating $\psi$ in this setting. Let 

$$
\hat{\psi}^{reg} := \bar{Y}_1 + (\bar{W}_0 - \bar{W}_1)^T\hat{\beta}_1
$$ 

where $\hat{\beta}_1$ is the OLS estimator of $\beta_1$, where $\bar{W}_a = n_a^{-1}\sum_{i:A_i = a} W_i$ and $\bar{Y}_a$ is defined analogously. Let 

$$
\hat{\psi}^{sbw} := \sum_{i: A_i = a} \gamma_i Y_i
$$ 

be the SBW estimator, where $\gamma$ solves the following objective:

$$
\gamma = \arg\min_{\theta \in \Gamma} \sum_{i: A_i = 1}\theta_i^2
$$

$$
\Gamma = \{\theta: \mathbf{W_1}^T\theta = \bar{W}_0, \theta_i > 0, \theta^T1 = 1\}
$$

where $\mathbf{W_1}$ is the $n_1$ by $d$ matrix of observed covariates for the treated group.

\begin{proposition}\label{cl1}
The bias of $\hat{\psi}^{reg}$ is equal to the bias of $\hat{\psi}^{sbw}$; specifically, 

$$
\mathbb{E}\{\psi^{reg} - \psi\} = \mathbb{E}\{\psi^{sbw} - \psi\} = (\mu_0 - \mu_1)^T(\kappa - I_d)\beta
$$
\end{proposition}

\begin{proof}
Recall that $\mathbb{E}(\hat{\beta}_1) = \kappa\beta_1$ (c.l. \cite{gleser1992importance}). Consider the error of $\hat{\psi}^{reg}$: 

\begin{align*}
    \hat{\psi}^{reg} - \psi &= \bar{Y}_1 + (\bar{W}_0 - \bar{W}_1)^T\hat{\beta}_1 - (\mu_y + (\mu_0 - \mu_1)^T\beta_1) \\
    &= \underbrace{(\bar{Y}_1 - \mu_y)}_{T_1} + \underbrace{(\bar{W}_0 - \mu_0)^T\hat{\beta}_1}_{T_2} - \underbrace{(\bar{W}_1 - \mu_1)^T\hat{\beta}_1}_{T_3} + \underbrace{(\mu_0 - \mu_1)^T(\hat{\beta}_1 - \beta_1)}_{T_4} \\
    & \to (\mu_0 - \mu_1)^T(\kappa - I_d)\beta_1
\end{align*}

The first equality holds by definition and the second by rearranging terms. The second equality consists of four terms. $T_1$ is simply the estimation error from a sample average, which has expectation zero. $T_2$ is the product of the estimation error from a sample average and $\hat{\beta}$; this also has expectation zero because $\bar{X}_0$ is estimated on a different part of the sample than $\hat{\beta}$, so these errors are independent. $T_3$ also has expectation zero because $\bar{W}_1^T\hat{\beta}_1 = \bar{Y}_1$, which has expectation $\mu_1$, and $\mu_1^T\hat{\beta}_1$ also has expectation $\mu_1$. We are then left with $T_4$; we substitute $\mathbb{E}{\hat{\beta}} = \kappa\beta$ to get the final result. 

We now derive the bias of $\hat{\psi}^{sbw}$:

\begin{align*}
    \hat{\psi}^{sbw} - \psi &= \sum_{i: A_i = 1}\gamma_iY_i - (\alpha + \mu_0^T\beta) \\
    &= \sum_{i: A_i = 1} \gamma_i(\alpha + X_i^T\beta + \epsilon_i) - (\alpha + \bar{W}_0^T\beta + (\mu_0 - \bar{W}_0)^T\beta) \\
    &= \sum_{i: A_i = 1} (\gamma_i(W_i - v_i)^T\beta + \gamma_i\epsilon_i) - \bar{W}_0^T\beta + (\mu_0 - \bar{W}_0)^T\beta \\
    &= \underbrace{-\sum_{i: A_i = 1}\gamma_iv_i^T\beta}_{T_1} + \underbrace{\sum_{i: A_i = 1}\gamma_i\epsilon_i}_{T_2}  + \underbrace{(\mu_0 - \bar{W}_0)^T\beta}_{T_3}
\end{align*}

Conditioning on $W_i$, we can take expectations over $X_i$, and see that $T_2$ has expectation zero (noting that the weights, conditional on $W_i$, are independent of these errors). $T_3$ is simply the scaled sum of mean zero estimation error and therefore has expectation zero. We conclude by considering $T_1$ and again take expectations over $X_i$ conditional on $W_i$: 

\begin{align*}
    \sum_{i: A_i = 1} \gamma_i\mathbb{E}\{X_i - W_i \mid W_i\}^T\beta &= \sum_{i: A_i = 1} \gamma_i (\mu_1 + \kappa^T(W_i - \mu_1))^T\beta - \sum_{i: A_i = 1}\gamma_i W_i^T\beta \\
    &= (\mu_1 + \kappa^T(\bar{W}_0 - \mu_1))^T\beta - \bar{W}_0^T\beta \\
    &= (\kappa^T(\bar{W}_0 - \mu_1))^T\beta - (\bar{W}_0 - \mu_1)^T\beta  \\
    &= (\bar{W}_0 - \mu_1)^T(\kappa - I_d)\beta \\
    &= (\mu_0 - \mu_1)^T(\kappa - I_d)\beta + (\bar{W}_0 - \mu_0)^T(\kappa - I_d)\beta \\
    &\to (\mu_0 - \mu_1)^T(\kappa - I_d)\beta
\end{align*}

The final line holds because in the second to last line, we can take expectation over $W_i$ and see that the second term is a scaled sum of mean zero estimation error and has expectation zero. We therefore see that the  bias of the SBW estimator is equivalent to the OLS estimator.
\end{proof}

Let $\hat{\psi}^{sbw}(\eta)$ be the SBW estimator that reweights $\eta_1(W_i)$ rather than $W_i$; i.e. $\hat{\psi}^{sbw}(\eta) = \sum_{i: A_i = 1}\gamma_i^\star Y_i$ where $\gamma^\star$ is the solution to

$$
\arg\min_{\theta \in \Gamma} \sum_{i: A_i = 1}\theta_i^2
$$

$$
\Gamma := \{\theta: \eta_1(W_1)^T\theta = \mu_0, \theta_i > 0, \theta^T1 = 1\}
$$

\begin{proposition}
The estimator $\hat{\psi}^{sbw}(\eta)$ is unbiased, i.e.
$\mathbb{E}\{\hat{\psi}^{sbw}(\eta)\} = \psi$
\end{proposition}

\begin{proof}

By linearity we know that

$$
Y_i = \alpha + \eta_a(W_i)^T\beta_a + (X_i - \eta_a(W_i))^T\beta_a
$$

We then have that:

\begin{align*}
    \hat{\psi}^{sbw}(\eta) - \psi &= \sum_{i: A_i = 1}\gamma_i^\star Y_i - (\alpha + \mu_0^T\beta) \\
    &= \sum_{i: A_i = 1}\gamma_i^\star\alpha + \sum_{i: A_i = 1}\gamma_i^\star\eta_1(W_i)^T\beta_1 + \sum_{i: A_i = 1}\gamma_i^\star(X_i - \eta_1(W_i))^T\beta_1 - (\alpha + \mu_0^T\beta_1) \\
    &= \sum_{i: A_i = 1}\gamma_i^\star(X_i - \eta_1(W_i))^T\beta_1
\end{align*}

Conditional on $W$, the weights are fixed and $X_i - \eta_1(W_i)$ has expectation zero; therefore, the estimator is unbiased.

\end{proof}
\begin{remark}

While we have assumed no model error, this estimator still has variance, conditional on $W_i$, equal to

$$
\sum_{i: A_i = 1} \gamma_i^\star^2\beta^TCov(X_i \mid W_i)\beta
$$

where, assuming that $(X_i, W_i)$ are jointly normally distributed, $Cov(X_i \mid W_i) = \Sigma_{XX} - \Sigma_{XX}\Sigma_{WW}^{-1}\Sigma_{XX}$. Therefore, the variance of this estimator is higher than if we knew the true $X_i$ unless $\Sigma_{WW} = \Sigma_{XX}$ (i.e. we observe $X_i$). 

\end{remark}

\begin{remark}
Assuming there is a model error simply leads to the additional term $\sum_{i: A_i = 1}\gamma_i\epsilon_i$. This again has expectation zero, because the weights remain independent of the error in the outcome model, and adds a term to the total variance (conditional on $W_i$) equal to

$$
\sigma^2\sum_{i: A_i = 1}\gamma_i^2
$$
\end{remark}

In practice we don't know $\eta_a$ or $\mu_0$ but rather estimate it from the data. Moreover, the estimation of $\hat{\eta}_1$ typically involves both the observed dataset and auxillary data, which we have not yet specified here. We now consider a simple version of this case.

Recall that $\eta_a(W_i) = \mu_a + \kappa^T(W_i - \mu_a)$, where $\kappa = \Sigma_{WW}^{-1}\Sigma_{XX}$. We can easily estimate $\mu_a$ consistently using $\bar{W}_a$; the challenge is estimating $\kappa$. 

Following \cite{gleser1992importance}, consider the setting where we observe $T$ independent vectors of measurements $W_i^\star$ from known values $X_i^\star$, so that $v_i^\star = W_i^\star - X_i^\star$. Assume that $\Sigma_{v^\star v^\star} = \Sigma_{vv}$. We can then estimate $\hat{\Sigma}_{vv} = \frac{1}{T}\sum_{i=1}^T(W_i^\star - X_i^\star)'(W_i^\star - X_i^\star)$. We can then estimate

$$
\hat{\kappa} = \hat{\Sigma}_{WW}^{-1}(\hat{\Sigma}_{WW} - n\hat{\Sigma}_{vv})
$$

assuming $\hat{\Sigma}_{WW} - n\hat{\Sigma}_{vv}$ is positive semi-definite, and where $\hat{\Sigma}_{WW} = \sum_{i=1}^n (W_i - \bar{W})'(W_i - \bar{W})$. 

\begin{proposition}

Let $\gamma$ be the weights that solve the SBW objective with $\sum_{i: A_i = 1}\theta_i\hat{\eta}_1(W_i) = \bar{W}_0$. The estimator $\sum_{i: A_i = 0}\gamma_iY_i \to \psi$ as $n \to \infty$, $T \to \infty$.

\end{proposition}

\begin{proof}

Rewriting the error expression from Lemma A.2 leads to the additional terms $\sum_{i: A_i = 1}\gamma_i(\eta_1(W_i) - \hat{\eta_1}(W_i))^T\beta$ and $(\mu_0 - \bar{W}_0)^T\beta$. The second term is scaled estimation error and has expectation zero. It therefore suffices to show that 

$$
\sum_{i: A_i = 1}\gamma_i(\eta_1(W_i) - \hat{\eta_1}(W_i))^T\beta \to 0
$$

By the weak law of large numbers, $\frac{1}{n}\hat{\Sigma}_{WW} - \hat{\Sigma}_{vv} \to \Sigma_{XX}$ as $n \to \infty$, $T \to \infty$; similarly $\hat{\Sigma}_{WW} \to \Sigma_{WW}$ as $n \to \infty$. By the continuous mapping theorem $\hat{\kappa} \to \kappa$. Since $\bar{W}_0 \to \mu_0$, we have that $\hat{\eta} \to \eta$. 

Let $\gamma^\star$ be the weights defined in Lemma A.3. We can then rewrite the error term above as

$$
\sum_{i: A_i = 1}(\gamma_i - \gamma_i^\star)(\hat{\eta}_1(W_i) - \eta(W_i))^T\beta + \sum_{i: A_i = 1}\gamma_i^\star(\hat{\eta}_1(W_i) - \eta_1(W_i))^T\beta
$$

As $n \to \infty$, $T \to \infty$, $\hat{\eta}_1 \to \eta_1$, and, by definition of the SBW objective, $\gamma_i \to \gamma_i^\star$. These two terms therefore both converge in probability to zero. 

\end{proof}

Our estimation of $\eta_a$ follows a somewhat different procedure which we outline in Appendix B.

\section{Adjustment Details}

We now discuss how we estimate $\eta_a(W_{sc})$ in our data, recalling that $s$ indexes the state and $c$ indexes the CPUMA. We begin by estimating the covariance matrix $\Sigma_{UU, sc}$, the sampling variability for each CPUMA using the individual replicate survey weights to generate $b = 80$ additional CPUMA-level datasets. We then estimate the covariance matrix of the measurement errors using:

\begin{equation}
\hat{\Sigma}_{vv, sc} = \frac{4}{80}\sum_{b=1}^{80}(X_{sc}^B - \bar{X}_{sc})(X_{b, sc} - \bar{X}_{sc})^T
\end{equation}

where the $4$ in the numerator comes from the process used to generate the replicate survey weights. 

Unlike in our proofs above, we do not assume that $\Sigma_{XX \mid A = a}$ or $\Sigma_{vv \mid A = a}$ are equal for $A_i \in \{0, 1\}$.

Instead let $\hat{\Sigma}_{vv \mid A = a} = n_a^{-1}\sum_{s, c: A_s = a} (W_{sc} - \bar{W})(W_{sc} - \bar{W})^T$. This estimator is calculated on the original observed dataset. We then estimate $\Sigma_{XX \mid A = a}$ using:

\begin{equation}
\hat{\Sigma}_{XX \mid A = a} = \hat{\Sigma}_{WW \mid A = a} - n_a^{-1}\sum_{s, c: A_s = a} \hat{\Sigma}_{vv, sc}
\end{equation}

Define

\begin{equation}
\hat{\kappa}_a = (\hat{\Sigma}_{WW \mid A = a})^{-1}(\hat{\Sigma}_{XX \mid A = a})
\end{equation}

We can think of this as a matrix of estimated coefficients of a linear regressions of the (unobserved) matrix $X_{sc}$ on (observed) matrix $W_{sc}$. We can estimate $X_{sc}$ as

\begin{equation}
\hat{\eta}_a(W_{sc}) = \bar{W}_a + \hat{\kappa}_a^T(W_{sc} - \bar{W}_a)
\end{equation}

where $\bar{W}_a = n_a^{-1}\sum_{s, c: A_s = a} W_{sc}$. This approximately aligns with the adjustments suggested by \cite{carroll2006measurement} and \cite{gleser1992importance}. 

In our setting we have additional access to information about a substantial source of heterogeneity in the measurement error: in particular, regions with large populations are estimated quite precisely, while regions with small populations are estimated much less precisely. This is because the survey is a one-percent sample across all regions. Moreover, for a given CPUMA, some covariates are measured using three years of data, and others only one. However, using the conventional regression calibration approach will adjust precisely estimated covariates to imprecisely estimated covariates in a similar way. 

Our preferred estimators therefore use an alternative approach where we model an individual-level $\Sigma_{vv, sc}$ as a function of the sample sizes used to estimate each covariate. In particular, let $s_{sc}$ be the d-dimensional vector of the sample sizes used to estimate each covariate value for a given CPUMA. Let $S_{sc} = \sqrt{s_{sc}}\sqrt{s_{sc}}^T$. We assume that $\sqrt{s_{sc}}v_{sc} \sim N(0, \Sigma_{vv}^a)$. We then know that $\Sigma_{vv, sc}^m = \Sigma_{vv}^a \oslash S_{sc}$. We add the superscript $m$ to distinguish that is a modeled covariance matrix.

To estimate these matrices, we pool our initial estimates of the CPUMA-level covariance matrices ($\hat{\Sigma}_{vv, sc}$) to generate $\hat{\Sigma}_{vv}^a = n_a^{-1}\sum_{s, c: A_s = a} S_{sc} \circ \Sigma_{vv, sc}$. We then estimate $\hat{\Sigma}_{vv, sc}^m = \hat{\Sigma}_{vv}^a \oslash S_{sc}$. From this we estimate $\hat{\Sigma}_{XX \mid A = a} = \hat{\Sigma}_{WW \mid A = a} - n_a^{-1}\sum_{s, c: A_s = a}\hat{\Sigma}_{vv, sc}^a$. Finally, we calculate $\hat{\kappa}_{sc} = (\hat{\Sigma}_{XX \mid A = a} + \hat{\Sigma}_{vv, sc}^m)^{-1}\hat{\Sigma}_{XX \mid A = a}$, which we use to estimate $\hat{\eta}_a(W_{sc})$. 

The benefit of this estimator is that we account for unit-level heterogeneity in the measurement error. Specifically, this adjustment should primarily affect outlying values of imprecisely estimated covariates, while leaving precisely estimated covariates largely unchanged. Moreover, we are able to use the full efficiency of using all units in the modeling. The disadvantage is that, as shown in Appendix~\ref{sec:appendixsumstat}, this adjustment, compared to the conventional approach, is more likely to lead to extreme values that fall outside of the support of the original data, or of the possible values entirely. A second cost of this procedure is that this aggregation models all differences as a function of the sample sizes, and averages over any potential heterogeneity due to heteroskedasticity (i.e. the measurement error covariance matrix changes depending on the true value of $X_{sc}$). However, in practice we find that either adjustment yields quite similar results (see Appendix~\ref{ssec:allresults}).
