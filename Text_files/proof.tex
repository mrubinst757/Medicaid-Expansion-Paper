\section{Proofs}\label{ssec:proof}

We show three results regarding the bias of the SBW estimator under the classical errors-in-variables model. First, we show that the bias of the SBW estimator that sets $\delta = 0$ (i.e. reweights the treated units to exactly balance the control units) is equal to the bias of the OLS estimator. Second, we show that if the observed covariate values for the treated data can be replaced by their conditional expectations $\eta_1$ given the noisy observations, then the SBW estimator will be unbiased. Finally, we consider the case where we can estimate $\eta_1$ using auxillary data to estimate the covariance matrix of the error terms and show that the SBW estimator is consistent if we replace $\eta_1$ by an estimate $\hat{\eta_1}$ in the constraint set. In contrast to our application, we take the perspective throughout that $X$ is random.

Consider a dataset that consists of $i = 1, ..., n$ randomly sampled units where we observe the outcomes $Y_i$ and a treatment assignment indicator $A_i \in \{0, 1\}$. Let $n_a$ be the number of units in each treatment group, i.e, $n_a = \sum_{i = 1}^n \mathbbm{1}(A_i = a)$. Let $X_i \in \mathbb{R}^q$ be a vector of covariate values where $X_i \mid A_i = a \stackrel{iid}\sim MVN(\upsilon_a, \Sigma_{XX})$. 

Our target parameter is $\psi^1 = \mathbb{E}\{Y_i^1 \mid A_i = 0\}$.\footnote{Here we switch to the super-population target because we view $X$ as random.} We assume unconfoundedness ($Y_i^a \perp A_i \mid X_i$), consistency ($Y_i^A = AY_i + (1-A)Y_i$), and that $\mathbb{E}\{Y_i \mid X_i, A_i = a\} = \alpha_a + X_i^T\beta_a$. For simplicity we also assume throughout that there exist positive weights $\gamma$ that exactly balance the covariates, i.e. $\sum_{i: A_i = 1}\gamma_iX_{i, r} = \bar{X}_{0, r}$ for all $r = 1, ..., q$ and $\sum_{i: A_i = 1}\gamma_i = 1$, $\gamma_i > 0$. 

We begin by establishing the following identity:

\begin{equation}
\psi^1 = \mu_y + (\upsilon_0 - \upsilon_1)^T\beta_1
\end{equation}

where $\mu_y = \mathbb{E}\{Y_i \mid A_i = 1\}$ and $\upsilon_a = \mathbb{E}\{X_i \mid A_i = a\}$.

\begin{proof}

Using our causal and modeling assumptions we have that:

\begin{align*}
\mathbb{E}\{Y_i^1 \mid X_i, A_i = 0\} &= \mathbb{E}\{Y_i^1 \mid X_i, A_i = 1\} \\
&= \mathbb{E}\{Y_i \mid X_i, A_i = 1\} \\
&= \alpha_1 + X_i^T\beta_1 \\
&= \mu_y + (X_i - \upsilon_1)^T\beta \\
&\implies \psi^1 = \mu_y + (\upsilon_0 - \upsilon_1)^T\beta_1
\end{align*}

where the first equality follows from unconfoundedness, the second equality from consistency, the third from our parametric modeling assumptions, and the fourth by definition of $\alpha$. The final implication comes from plugging $\upsilon_0$ in place of $X_i$.

\end{proof}

We now outline the classical errors-in-variables model. Let $Y_i(X_i, a) = \alpha_a + X_i^T\beta_a + \epsilon_i$ where $\epsilon_i \stackrel{iid}\sim N(0, \sigma^2)$. We consider the case where we observe $W_i$, a vector of mean-unbiased proxies for the true (unobserved) covariate vector $X_i$; i.e., $W_i = X_i + v_i$, where $v_i \stackrel{iid}\sim MVN(0, \Sigma_{vv})$. Consider the model:

\begin{equation}
(\epsilon_i, v_i) \sim MVN((0, 0), \begin{pmatrix} 
\sigma^2 & 0 \\ 
0 & \Sigma_{vv}  
\end{pmatrix}
\end{equation}

In other words the error in the outcome model is uncorrelated with the error in the covariates. Let $u_i = (\epsilon_i, v_i)$. We further assume that the covariates are uncorrelated with any of the error terms:

\begin{equation}
(X_i, u_i) \mid A_i = a \sim MVN((\upsilon_a, 0), \begin{pmatrix} 
\Sigma_{XX} & 0 \\ 
0 & \Sigma_{uu}  
\end{pmatrix}
\end{equation}

Then we see that $(X_i, W_i) \mid A_i = a \stackrel{iid}{\sim} MVN((\upsilon_a, \upsilon_a), \Sigma)$\footnote{In contrast to our application, we instead assume here that $\Sigma_{WW \mid A = 1} = \Sigma_{WW \mid A = 0} = \Sigma_{WW}$ and $\Sigma_{XX \mid A = 1} = \Sigma_{XX \mid A = 0} = \Sigma_{XX}$. This is not a necessary assumption, but helps simplify notation.} where 

$$
\Sigma = \begin{pmatrix} 
\Sigma_{XX} & \Sigma_{XX} \\ 
\Sigma_{XX} & \Sigma_{WW}  
\end{pmatrix}
$$ 

and $\Sigma_{WW} = \Sigma_{XX} + \Sigma_{vv}$. Let $\kappa = \Sigma_{WW}^{-1}\Sigma_{XX}$. By the normality of the joint distribution of $X_i$ and $W_i$, we also know that

\begin{equation}
\mathbb{E}\{X_i \mid W_i, A_i = a\} = \upsilon_a + \kappa^T(W_i - \upsilon_a)
\end{equation}

We consider estimating $\psi^1$ in this setting. Let 

$$
\hat{\psi}^{1, reg} = \bar{Y}_1 + (\bar{W}_0 - \bar{W}_1)^T\hat{\beta}_1
$$ 

where $\hat{\beta}_1$ is the OLS estimator of $\beta_1$, $\bar{W}_a = n_a^{-1}\sum_{i:A_i = a} W_i$ and $\bar{Y}_a$ is defined analogously. Let 

$$
\hat{\psi}^{1, sbw} = \sum_{i: A_i = a} \gamma_i Y_i
$$ 

be the SBW estimator, where

\begin{align*}
\gamma &= \arg\min_{\theta} \sum_{i: A_i = 1}\theta_i^2 \text{ such that } W_{1, r}^T\theta = \bar{W}_{0, r} \ \ \ (r = 1, ..., q) \\
&\theta_i > 0, \sum_{i: A_i = 1}\theta_i = 1
\end{align*}

where $W_{a, r}$ is the $n_a$ dimensional vector associated with covariate $r$.

\begin{proposition}\label{cl1}
The bias of $\hat{\psi}^{1, reg}$ is equal to the bias of $\hat{\psi}^{1, sbw}$; specifically, 

$$
\mathbb{E}\{\psi^{1, reg} - \psi^1\} = \mathbb{E}\{\psi^{1, sbw} - \psi^1\} = (\upsilon_0 - \upsilon_1)^T(\kappa - I_d)\beta
$$
\end{proposition}

\begin{proof}
Recall that $\mathbb{E}(\hat{\beta}_1) = \kappa\beta_1$ (c.l. \cite{gleser1992importance}). Consider the error of $\hat{\psi}^{1, reg}$: 

\begin{align*}
    \hat{\psi}^{1, reg} - \psi^1 &= \bar{Y}_1 + (\bar{W}_0 - \bar{W}_1)^T\hat{\beta}_1 - (\mu_y + (\upsilon_0 - \upsilon_1)^T\beta_1) \\
    &= \underbrace{(\bar{Y}_1 - \mu_y)}_{T_1} + \underbrace{(\bar{W}_0 - \upsilon_0)^T\hat{\beta}_1}_{T_2} - \underbrace{(\bar{W}_1 - \upsilon_1)^T\hat{\beta}_1}_{T_3} + \underbrace{(\upsilon_0 - \upsilon_1)^T(\hat{\beta}_1 - \beta_1)}_{T_4} \\
    \implies \mathbb{E}\{\hat{\psi}^{1, reg} - \psi^1\} &= (\upsilon_0 - \upsilon_1)^T(\kappa - I_d)\beta_1
\end{align*}

The first equality holds by definition and the second by rearranging terms. The second equality consists of four terms. $T_1$ is simply the estimation error from a sample average, which has expectation zero. $T_2$ is the product of the estimation error from a sample average and $\hat{\beta}$; this also has expectation zero because $\bar{X}_0$ is estimated on a different part of the sample than $\hat{\beta}$, so these errors are independent. $T_3$ also has expectation zero because $\bar{W}_1^T\hat{\beta}_1 = \bar{Y}_1$, which has expectation $\mu_y$, and $\upsilon_1^T\hat{\beta}_1$ also has expectation $\mu_y$. We are then left with $T_4$; we substitute $\mathbb{E}{\hat{\beta}} = \kappa\beta$ to get the final result. 

We now derive the bias of $\hat{\psi}^{1, sbw}$:

\begin{align*}
    \hat{\psi}^{1, sbw} - \psi^1 &= \sum_{i: A_i = 1}\gamma_iY_i - (\alpha_1 + \upsilon_0^T\beta_1) \\
    &= \sum_{i: A_i = 1} \gamma_i(\alpha_1 + X_i^T\beta_1 + \epsilon_i) - (\alpha_1 + \bar{W}_0^T\beta_1 + (\upsilon_0 - \bar{W}_0)^T\beta_1) \\
    &= \sum_{i: A_i = 1} (\gamma_i(W_i - v_i)^T\beta_1 + \gamma_i\epsilon_i) - \bar{W}_0^T\beta_1 + (\upsilon_0 - \bar{W}_0)^T\beta_1 \\
    &= \underbrace{-\sum_{i: A_i = 1}\gamma_iv_i^T\beta_1}_{T_1} + \underbrace{\sum_{i: A_i = 1}\gamma_i\epsilon_i}_{T_2}  + \underbrace{(\upsilon_0 - \bar{W}_0)^T\beta_1}_{T_3}
\end{align*}

Conditioning on $W_i$, we can take expectations over $X_i$, and see that $T_2$ has expectation zero (noting that the weights, conditional on $W_i$, are independent of these errors). $T_3$ is simply the scaled sum of mean zero estimation error and therefore has expectation zero. We conclude by considering $T_1$ and again take expectations over $X_i$ conditional on $W_i$: 

\begin{align*}
    \sum_{i: A_i = 1} \gamma_i\mathbb{E}\{X_i - W_i \mid W_i\}^T\beta_1 &= \sum_{i: A_i = 1} \gamma_i (\upsilon_1 + \kappa^T(W_i - \upsilon_1))^T\beta_1 - \sum_{i: A_i = 1}\gamma_i W_i^T\beta_1 \\
    &= (\upsilon_1 + \kappa^T(\bar{W}_0 - \upsilon_1))^T\beta_1 - \bar{W}_0^T\beta_1 \\
    &= (\kappa^T(\bar{W}_0 - \upsilon_1))^T\beta_1 - (\bar{W}_0 - \upsilon_1)^T\beta_1  \\
    &= (\bar{W}_0 - \upsilon_1)^T(\kappa - I_d)\beta_1 \\
    &= (\upsilon_0 - \upsilon_1)^T(\kappa - I_d)\beta_1 + (\bar{W}_0 - \upsilon_0)^T(\kappa - I_d)\beta_1 \\
    \implies \mathbb{E}\{\hat{\psi}^{1, sbw} - \psi^1\} &= (\upsilon_0 - \upsilon_1)^T(\kappa - I_d)\beta_1
\end{align*}

The final line holds because in the second to last line, we can take expectation over $W_i$ and see that the second term is a scaled sum of mean zero estimation error and has expectation zero. 
\end{proof}

Let $\hat{\psi}^{1, sbw}(\eta_1)$ be the SBW estimator that reweights $\eta_1(W_i)$ rather than $W_i$; i.e. $\hat{\psi}^{sbw, 1} \eta_1) = \sum_{i: A_i = 1}\gamma_i^\star Y_i$ where 

\begin{align*}
\gamma^\star = &\arg\min_{\theta} \sum_{i: A_i = 1}\theta_i^2 \text{ such that } \eta_1(W_{1, r})^T\theta = \upsilon_{0, r} \ \ \ (r = 1, ..., q) \\
&\theta_i > 0, \sum_{i: A_i = 1} \theta_i = 1
\end{align*}

\begin{proposition}
The estimator $\hat{\psi}^{1, sbw}(\eta_1)$ is unbiased, i.e.
$\mathbb{E}\{\hat{\psi}^{1, sbw}(\eta_1)\} = \psi^1$
\end{proposition}

\begin{proof}

Assuming no model error, by linearity we know that

\begin{align*}
Y_i^1 = \alpha_1 + \eta_a(W_i)^T\beta_1 + (X_i - \eta_a(W_i))^T\beta_1
\end{align*}

We then have that:

\begin{align*}
    \hat{\psi}^{1, sbw}(\eta_1) - \psi^1 &= \sum_{i: A_i = 1}\gamma_i^\star Y_i - (\alpha_1 + \upsilon_0^T\beta_1) \\
    &= \sum_{i: A_i = 1}\gamma_i^\star\alpha_1 + \sum_{i: A_i = 1}\gamma_i^\star\eta_1(W_i)^T\beta_1 + \sum_{i: A_i = 1}\gamma_i^\star(X_i - \eta_1(W_i))^T\beta_1 - (\alpha + \upsilon_0^T\beta_1) \\
    &= \sum_{i: A_i = 1}\gamma_i^\star(X_i - \eta_1(W_i))^T\beta_1
\end{align*}

Conditional on $W$, the weights are fixed and $X_i - \eta_1(W_i)$ has expectation zero; therefore, the estimator is unbiased.

\end{proof}
\begin{remark}

While we have assumed no model error, this estimator still has variance, conditional on $W_i$, equal to

$$
\sum_{i: A_i = 1} \gamma_i^{\star^2}\beta_1^T Cov(X_i \mid W_i)\beta_1
$$

where, assuming that $(X_i, W_i)$ are jointly normally distributed, $Cov(X_i \mid W_i) = \Sigma_{XX} - \Sigma_{XX}\Sigma_{WW}^{-1}\Sigma_{XX}$. Therefore, the variance of this estimator is higher than if we knew the true $X_i$ unless $\Sigma_{WW} = \Sigma_{XX}$ (i.e. we observe $X_i$). 

\end{remark}

\begin{remark}
Assuming there is a model error simply leads to the additional term $\sum_{i: A_i = 1}\gamma_i\epsilon_i$. This again has expectation zero, because the weights remain independent of the error in the outcome model, and adds a term to the total variance (conditional on $W_i$) equal to

$$
\sigma^2\sum_{i: A_i = 1}\gamma_i^{\star^2}
$$
\end{remark}

\begin{remark}
This proof (and general estimation technique) relies on viewing $X$ as random, while in the paper we define our target estimand conditional on $X$. If we consider $X$ fixed we can define $\eta_1(W, n) = \bar{X}_1 + \Sigma_{WW}(n)^{-1}\Sigma_{XX}(n)(W_i - \bar{X}_1)$, where $\bar{X}_1$ is the sample average among the treated units and $\Sigma_{WW}(n)$ and $\Sigma_{XX}(n)$ are the sample covariance matrices. Assume that the limit of these quantities as $n \to \infty$ are $\eta_1$, $\upsilon_1$, $\Sigma_{WW}$, and $\Sigma_{XX}$, respectively. We can then interpret these terms in the final expression as the limit of these sample statistics as $n \to \infty$, and again we see that this term asymptotically approaches zero.
\end{remark}

Proposition 2 assumes that we know $\eta_1$ and $\upsilon_a$; however, in practice we estimate it from the data. Moreover, the estimation of $\hat{\eta}_1$ typically involves both the observed dataset and auxillary data, which we have not yet specified here. We now consider a simple version of this case.

Recall that $\eta_1(W_i) = \upsilon_1 + \kappa^T(W_i - \upsilon_1)$, where $\kappa = \Sigma_{WW}^{-1}\Sigma_{XX}$. We can easily estimate $\upsilon_a$ consistently using $\bar{W}_a$; the challenge is estimating $\kappa$. 

Following \cite{gleser1992importance}, consider the setting where we observe $T$ independent vectors of measurements $W_i^\star$ from known values $X_i^\star$, so that $v_i^\star = W_i^\star - X_i^\star$. Assume that $\Sigma_{v^\star v^\star} = \Sigma_{vv}$. We can then estimate $\hat{\Sigma}_{vv} = \frac{1}{T}\sum_{i=1}^T(W_i^\star - X_i^\star)'(W_i^\star - X_i^\star)$. We can then estimate

$$
\hat{\kappa} = (n\hat{\Sigma}_{WW})^{-1}(n(\hat{\Sigma}_{WW} - \hat{\Sigma}_{vv}))
$$

assuming $n(\hat{\Sigma}_{WW} - \hat{\Sigma}_{vv})$ is positive semi-definite, and where $\hat{\Sigma}_{WW} = \frac{1}{n}\sum_{i=1}^n (W_i - \bar{W})(W_i - \bar{W})'$. 

\begin{proposition}
Let $\tilde{\gamma}$ be the weights that solve the SBW objective with $\sum_{i: A_i = 1}\tilde{\gamma}_i\hat{\eta}_1(W_{i, r}) = \bar{W}_{0, r}$ for all $r = 1, ..., q$. The estimator $\sum_{i: A_i = 1}\tilde{\gamma}_iY_i \to \psi^1$ as $n \to \infty$, $T \to \infty$.
\end{proposition}

\begin{proof}

Rewriting the error expression from Lemma A.2 leads to the additional terms $\sum_{i: A_i = 1}\tilde{\gamma}_i(\eta_1(W_i) - \hat{\eta_1}(W_i))^T\beta_1$ and $(\upsilon_0 - \bar{W}_0)^T\beta_1$. The second term is scaled estimation error and has expectation zero. It therefore suffices to show that 

$$
\sum_{i: A_i = 1}\tilde{\gamma}_i(\eta_1(W_i) - \hat{\eta_1}(W_i))^T\beta_1 \to 0
$$

By the weak law of large numbers, $\hat{\Sigma}_{WW} - \hat{\Sigma}_{vv} \to \Sigma_{XX}$ as $n \to \infty$, $T \to \infty$; similarly $\hat{\Sigma}_{WW} \to \Sigma_{WW}$ as $n \to \infty$. By the continuous mapping theorem $\hat{\kappa} \to \kappa$. Since $\bar{W}_0 \to \upsilon_0$, we have that $\hat{\eta_1} \to \eta_1$. 

Let $\gamma^\star$ be the weights defined in Lemma A.3. We can then rewrite the error term above as

\begin{align*}
\sum_{i: A_i = 1}(\tilde{\gamma}_i - \gamma_i^\star)(\hat{\eta}_1(W_i) - \eta(W_i))^T\beta_1 + \sum_{i: A_i = 1}\gamma_i^\star(\hat{\eta}_1(W_i) - \eta_1(W_i))^T\beta_1
\end{align*}

As $n \to \infty$, $T \to \infty$, $\hat{\eta}_1 \to \eta_1$, and therefore by definition of the SBW objective, $\tilde{\gamma}_i \to \gamma_i^\star$. These two terms therefore both converge in probability to zero. 

\end{proof}

Our estimation of $\eta_1$ follows a somewhat different procedure which we outline in Appendix B.

\subsection{H-SBW Objective}

We show that the H-SBW estimator minimizes the variance of any estimator within the constraint set given a constant within-group correlation structure. Here we assume that the true covariates are measured. Consider the outcome model:

\begin{equation}
    Y_{sc}(X_{sc}, a) = \alpha_a + X_{sc}^T\beta_a + c_s + \epsilon_{sc}
\end{equation}

where $\mathbb{E}\{c_s \mid X_{sc}, A_{sc}\} = \mathbb{E}\{\epsilon_{sc} \mid X_{sc}, A_{sc}\} = \mathbb{E}\{c_s\epsilon_{sc} \mid X_{sc}, A_{sc}\} = 0$, and $Var(\epsilon_{sc}) = \sigma^2$ and $Var(c_s) = \eta^2$. Let $\rho = \frac{\eta^2}{\sigma^2 + \eta^2}$.

The conditional variance of any weighting estimator $\sum_{sc: A_{sc} = 1}\gamma_{sc}Y_{sc}$ using weights $\gamma$ where $\gamma^T1 = n_t$ is then equal to:

\begin{align}
    Var(n_t^{-1}\sum_{sc: A_s = 1}\gamma_{sc}Y_{sc} \mid X_{sc}, A_{sc}) &= n_t^{-2}\sum_{s: A_s = 1}^{m_1}(\sum_{c = 1}^{p_s}\gamma_{sc}^2(\sigma^2 + \eta^2) + \sum_{c \ne d}\gamma_{sc}\gamma_{sd}\eta^2) \\
    &\propto \sum_{s: A_s = 1}(\sum_{c = 1}^{p_s}(\gamma_{sc}^2 + \sum_{c \ne d}\rho \gamma_{sc}\gamma_{sd})
\end{align}

where the second line follows by dividing by $\eta^2 + \sigma^2$. This suggests the H-SBW objective, which minimizes the (conditional) variance of this estimator for known $\rho$.

\subsection{H-SBW with measurement error}

In this section we analyze the asymptotic bias of balancing weights when the the covariates are correlated across units. Throughout our analysis, we assume a positive equi-correlation within groups (the same assumed correlation structure for $Y$). Under this model we show that SBW that reweights $\eta_{sc} = \mathbb{E}\{X_{sc} \mid W_{sc}\}$ is unbiased while H-SBW is biased. 

Define $\eta^\star_{sc} = \mathbb{E}\{X_{sc} \mid W\}$. Assume that $X_{sc} \sim N(\mu_j, \Sigma_{XX})$, $\mu_j \sim N(\mu, \Sigma_{SS})$, $Cov(X_{sc}, X_{s'd}) = \Sigma_{SS}\mathds{1}(s = s', c \ne d)$. Therefore we have that the entire data $(X_{11}, ..., X_{1p_1}, ..., X_{sp_s}, W_{11}, ..., W_{1p_1}, ..., W_{sp_s} \sim N(\mathbf{\upsilon}, \Sigma)$, where 

\begin{align*}
\Sigma = \begin{pmatrix}
\Sigma_{X} & 0 \\
0 & \Sigma_{W}
\end{pmatrix} = \begin{pmatrix}
\Sigma_{11} & 0 &, ..., &0 \\
0 & \Sigma_{22} &, ..., &0 \\
... \\
0 & 0 & ,... , & \Sigma_{2n2n}
\end{pmatrix} 
\end{align*}

\begin{align*}
    & \Sigma_{11, kk}, ..., \Sigma_{nn, kk} = \Sigma_{XX} \\
    & \Sigma_{n+1n+1, kk}, ..., \Sigma_{2n2n, kk} = \Sigma_{WW} \\
    & \Sigma_{jj, kl} = \Sigma_{SS} \\
    & \Sigma_{XX} = \mathbb{E}\{X_{sc}X_{sc}^T\} \\
    & \Sigma_{WW} = \mathbb{E}\{W_{sc}W_{sc}^T\} \\
    & \Sigma_{SS} = \mathbb{E}\{\mu_s\mu_s^T\}
\end{align*}

Notice that if $(X, W)$ were independent, then $\Sigma_{jj, kl} = 0$. Then by the form of the conditional normal distribution, we have

\begin{align*}
    \eta_{sc}^\star(W) = \mathbb{E}\{X_{sc} \mid W\} = \upsilon + (\Sigma_X\Sigma_W^{-1})_{sc, }(W - \upsilon)
\end{align*}

Again note that if $\Sigma_{SS} = 0$, then this expression equals

\begin{align*}
    \eta_{sc}(W) = \upsilon + \Sigma_{XX}\Sigma_{WW}^{-1}(W_{sc} - \upsilon)
\end{align*}

\begin{proposition}
    \begin{align*}
    \mathbb{E}\{\eta_{sc}^\star(W) - \eta_{sc}(W)\} = 0
    \end{align*}
\end{proposition}

\begin{proof}

\begin{align*}
    \eta_{sc}^\star(W) - \eta_{sc}(W_{sc}) &= (\Sigma_{W}^{-1}\Sigma_{X})_{sc}(W - \mu) - \Sigma_{WW}\Sigma_{XX}^{-1}(W_{sc} - \mu) \\
    &= (B_1 - B_2)(W_{sc} - \upsilon) + C\sum_{s, d \ne c}(W_{sd} - \upsilon)
\end{align*}

The proposition follows from noting that $\mathbb{E}\{W_{sc}\} = \upsilon$.
\end{proof}

Therefore we can rewrite

\begin{align*}
Y_{sc} = (X_{sc} - \eta^\star_{sc} + \eta^\star_{sc} - \eta_{sc})^T\beta + \eta^T_{sc}\beta
\end{align*}

\begin{proposition}
    Any weighting estimator that reweights $\eta^\star$ to $\upsilon_1$ is unbiased.
\end{proposition}

\begin{proof}
    Notice that 
    \begin{align*}
        Y_{sc} = (X_{sc} - \eta^\star_{sc})^T\beta + \eta^\star_{sc}^T\beta
    \end{align*}
    Therefore
    
    \begin{align*}
        \hat{\psi} - \psi = \sum_{sc}\gamma_{sc}(X_{sc} - \eta^\star_{sc})^T\beta
    \end{align*}
    
    Conditional on $W$, $\gamma_{sc}$ is fixed and $\mathbb{E}\{X_{sc} \mid W\} = \eta^\star$ so this expression is equal to 0.
\end{proof}

In this paper we instead propose and consider a weighting estimators $\hat{\psi}$ that reweight $\eta_{sc}$ to the desired target $\upsilon_1$.

\begin{proposition}

A weighting estimator estimator $\hat{\psi}$ that reweights $\eta_{sc}$ to $\upsilon_1$ is unbiased for $\psi$ under the following conditions:

\begin{align}
    \beta &= 0 \\
    \Sigma_{SS} &= 0 \\
    \mathbb{E}\{\sum_{sc}\gamma_{sc}\eta^\star_{sc}\} &= \upsilon_1
\end{align}
\end{proposition}

The first two are trivial conditions. The third condition is of interest, and depends on how the weights are generated. We study this by re-expressing our weights as regression-weights.

\begin{proof}
    \begin{align*}
    \hat{\psi} - \psi = \underbrace{\sum_{sc}\gamma_{sc}(X_{sc} - \eta^\star{sc})^T\beta}_{T_1} + \underbrace{\sum_{sc}\gamma_{sc}(\eta^\star_{sc} - \eta_{sc})^T\beta}
    _{T_2}
    \end{align*}
    
    $T_1$ is in expectation equal to zero, conditional on $W$, as also noted above. By definition $\sum_{sc}\gamma_{sc}\eta_{sc} = \upsilon_1$; therefore, the error is equal to $\sum_{sc}(\gamma_{sc}\eta^\star_{sc} - \upsilon_1)$. Taking the expectation of this quantity gives the form of the bias, verifying condition (3). Clearly, if $\beta = 0$ then the estimator remains unbiased regardless of this error, verifying (1). (2) follows because if $\Sigma_{SS} = 0 \implies \eta^\star = \eta$.
\end{proof}

\begin{proposition}
(1) the SBW estimator that exactly balances on $\eta_{sc}$ also balances $\eta^\star_{sc}$, and is therefore unbiased 

(2) the H-SBW estimator that exactly balances $\eta_{sc}$ does not necessarily balance $\eta^\star_{sc}$, and therefore may be biased
\end{proposition}

\begin{remark}
This result is of general interest to users of the regression calibration in regression contexts as well, as we show why we using OLS with regression calibration will lead to unbiased results while using GLS may lead to biased results. 
\end{remark}

\begin{proof}
We show this by using the connection between balancing weights and regression weights. In particular, \cite{chattopadhyay2021implied} shows that without the positivity constraint, the SBW objective that enforces exact balance returns the same weights as the regression estimator with an intercept. That is

$$
\gamma_{sc} = b^T(\sum_{sc} X_{sc} X_{sc}^T)^{-1}X_{sc}
$$

For ease of exposition and conceptual clarity, we simplify the setting further and consider $X_{sc} \in \mathbb{R}$ and consider the regression weights without an intercept. This effectively relaxes the summing to 1 constraint on the weights, which would desirable if our assumed model of the outcome does not contain an intercept. However, we extend our results to the more general setting below.

First, note that in this case the regression weights using known $X_{sc}$ are equal to:

\begin{align*}
    \gamma_{sc}^{OLS, X} = (\sum_{sc}X_{sc}^2)^{-1}X_{sc}
\end{align*}


For any target $b$, this weight will balance the covariate $X$ to be: that is, $\sum_{sc}\gamma_{sc}X_{sc} = b$.

    \begin{align*}
        \sum_{sc} \gamma_{sc}^{OLS, X} X_{sc} &= \sum_{sc} X_{sc} (\sum_{sc} X_{sc}^2)^{-1}X_{sc}b \\
        &= (\sum_{sc} X_{sc}^2)^{-1}\sum_{sc} X_{sc}^2b \\
        &= b
    \end{align*}

Without loss of generality, assume $\upsilon_1 = 0$. We now consider balancing on $\eta_{1, sc} = \mathbb{E}\{X_{sc} \mid W_{sc}, A_{sc} = 1\} = \frac{\sigma^2_x}{\sigma^2_w}W_{sc} = \kappa W_{sc}$. 

    \begin{align*}
        \sum_{sc} \gamma_{sc} X_{sc}^{OLS, \eta} &= \sum_{sc} X_{sc} (\sum_{sc} \hat{X}_{sc}^2)^{-1}\hat{X}_{sc}b \\
        &= (\kappa^2 \sum_{sc} W_{sc}^2)^{-1}\kappa \sum_{sc}W_{sc}X_{sc}b \\
        &= (\kappa \sigma^2_w)^{-1}\sigma^2_xb + o_p(1) \\
        &\to b
    \end{align*}
\end{proof}

We next write an expression of the GLS weights:

\begin{align*}
\gamma_{sc}^{GLS, \eta} = (\sum_{s}\sum_{c=1}^{p_s} a_1 \hat{X}_{sc}^2 + a_2 \sum_{c\ne d} \hat{X}_{sc}\hat{X}_{sd})^{-1}(a_1\hat{X}_{sc} + a_2\sum_{c\ne d} \hat{X}_{sd})b
\end{align*}

where $a_1$ is any of the diagonal elements of $\Omega^{-1}$, and $a_2$ represents the constant in any within-block off-diagonal element. 

Next, we define 
    \begin{align*}
        \sum_{sc} \gamma_{sc} X_{sc} &= (\sum_{s}\sum_{c=1}^{p_s} a_1 \hat{X}_{sc}^2 + a_2 \sum_{c\ne d} \hat{X}_{sc}\hat{X}_{sd})^{-1}(a_1\sum_{s}\sum_{c=1}^{p_s}X_{sc}\hat{X}_{sc} + a_2X_{sc}\sum_{c\ne d} \hat{X}_{sd})b \\
        &= T_1^{-1}T_2b
    \end{align*}
    We first consider $T_2$. Notice that
    \begin{align*}
        n^{-1}T_1 &\to n^{-1}\sum_s \kappa a_1 p_s\sigma^2_x + \kappa a_2 p_s(p_s - 1)\sigma^2_s\\
        n^{-1}T_2 &\to n^{-1}\sum_s \kappa a_1 p_s\sigma^2_x + a_2p_s(p_s - 1)\sigma^2_s
    \end{align*}
    
    Notice that $T_1^{-1}T_2 \not\to 1$; therefore, these weights do not achieve the desired balance.


\begin{remark}
    For simplicity consider the case where $p_s = 2$ for all $s = 1, ..., m$. Then
    \begin{align*}
        T_1 &\to \kappa a_1\sigma^2_x + \kappa^2 a_2\sigma^2_s \\
        T_2 &\to \kappa(a_1 \sigma^2_x + a_2 \sigma^2_s) \\
        &\implies T_1^{-1}T_2 \to b\frac{a_1\sigma^2_x + a_2\sigma^2_s}{a_1\sigma^2_x + a_2\kappa\sigma^2_s}
    \end{align*}
    
    We see that this term is not equal to $b$ unless one of three cases hold: (1) $\sigma^2_s = 0$ -- that is, the covariates are independent of one another; (2) $\kappa = 1$ -- that is, there is no measurement error; (3) $a_2 = 0$. This third case would hold if we assumed a diagonal matrix $\Omega$; ie, we estimated OLS or WLS. However, given our assumed $\Omega$, this term is not in general zero. If none of these conditions are true, then the GLS weights will lead to bias unless $\beta = 0$. 
    
    Finally, notice that $a_1 > 0$ and $a_2 < 0$ for any $c\Omega$ defined above. We therefore see that the numerator is smaller than the denominator, meaning that the achieved balance $b'$ ($\sum_{sc}\gamma_{sc}X_{sc}$) will be less than $b$.
\end{remark}

\begin{remark}
The bias essentially comes from the estimate of the covariance $\sigma^2_s$ in the denominator, based on $\sum \hat{X}_{sc}\hat{X}_{sd}$. The GLS weights (as defined above) prevent our weights from balancing $\eta^\star$ due to the dependence on this misestimate. OLS weights do not have this dependence and are therefore unbiased.
\end{remark}

For completeness, we consider the more general case below with a full covariate vector (that may include an intercept term) $X_{sc}$. However, the intuition from the simplest case is exactly the same.

\begin{proposition}
    The GLS-weights that balance the covariate vector $\hat{X}_{sc}$ are fail to balance the covariates $\eta^\star_{sc}$, with a difference equal to:
    
    \begin{align*}
        \mathbb{E}\{\sum_{sc}\gamma_{sc}(\eta^\star - \upsilon_1)\} &= \upsilon_1^T(T_1^{-1}T_2 - I_d) \\
        T_1 &= \kappa(A_1\Sigma_{WW} + A_2\Sigma_{SS})\kappa \\
        T_2 &= \kappa A_1\Sigma_{XX} + A_2\Sigma_{SS} \\
    \end{align*}
\end{proposition}

\begin{proof}

    Consider $\gamma_{sc} = \hat{X}_{sc}(\hat{X}^T\Omega^{-1}\hat{X})^{-1}(\hat{X}\Omega^{-1})_{sc}$. Then we have that
    
    \begin{align*}
        \mathbb{E}\{\hat{X}^T\Omega^{-1}\hat{X}\} &= 
        \kappa(A_1\Sigma_{WW} + A_2\Sigma_{SS})\kappa \\
        &= \kappa A_1\Sigma_{XX} + \kappa A_2\Sigma_{SS} \kappa \\
        \mathbb{E}\{(\hat{X}\Omega^{-1})_{sc}X_{sc}^T\} &= \kappa(A_1\Sigma_{XX} + A_2\Sigma_{SS}) \\
        &\not\implies T_1^{-1}T_2 \to I_d
    \end{align*}
\end{proof}

\begin{remark}
Similar to the univariate case above, we see that balance is not achieved unless $\kappa = I_d$, $\Sigma_{SS} = 0$, or $A_2 = 0$. In other words, we cannot achieve balance asymptotically in expectation unless (1) there is no measurement error; (2) $X_{sc}$ and $X_{sd}$ are independent; (3) $\Omega$ is a diagonal matrix (ie $A_2 = 0$).
\end{remark}

Our analysis makes use of the connection between regression weights and balancing weights to analyze the bias when using regression calibration techniques. This is equivalent to relaxing the positivity constraint on SBW (\cite{chattopadhyay2021implied}) and imposing exactly balance. Analyzing the bias given the positivity constraints is more challenging; in general we conjecture that the positivity constraint will improve balance (and therefore bias) relative to the regression weights as this constraint in general limits fitting to noisy imputations. Even so, the results will remain biased, which we confirm in our simulations. We leave further analysis of this problem to future work.

