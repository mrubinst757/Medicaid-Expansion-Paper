% Template for the submission to:
%   The Annals of Applied Statistics    [AOAS]
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% In this template, the places where you   %%
%% need to fill in your information are     %%
%% indicated by '???'.                      %%
%%                                          %%
%% Please do not use \input{...} to include %%
%% other tex files. Submit your LaTeX       %%
%% manuscript as one .tex document.         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[aoas]{imsart}

%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb,centernot,float,import,makeidx,subfiles}
\RequirePackage{natbib}
%\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{graphicx}% uncomment this for including figures
\usepackage{hyperref}
\usepackage[nokeyprefix]{refstyle}
\usepackage{varioref}
\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothezis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}
\newcommand{\matr}[1]{\mathbf{#1}} % undergraduate algebra version
\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{remark}{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\theoremstyle{plain}
%\newtheorem{???}{???}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{???}[???]{???}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\theoremstyle{remark}
%\newtheorem{???}{???}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{???}[???]{???}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlocaldefs

% reference external document
\makeatletter
\newcommand*{\addFileDependency}[1]{
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{
    \externaldocument{#1}
    \addFileDependency{#1.tex}
    \addFileDependency{#1.aux}
}
%%% END HELPER CODE

% put all the external documents here!

\begin{document}

\begin{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{The Effect of Medicaid Expansion on Non-Elderly Adult Uninsurance Rates Among States that did not Expand Medicaid}
%\title{A sample article title with some additional note\thanksref{T1}}
\runtitle{Medicaid Expansion}
%\thankstext{T1}{A sample of additional note to the title.}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%Only one address is permitted per author. %%
%%Only division, organization and e-mail is %%
%%included in the address.                  %%
%%Additional information can be included in %%
%%the Acknowledgments section if necessary. %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{Max} \snm{Rubinstein}\ead[label=e1]{mrubinst@andrew.cmu.edu; amelia@andrew.cmu.edu}} and
\author[A]{\fnms{Amelia} \snm{Haviland}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{Carnegie Mellon University, Heinz College and Department of Statistics and Data Science \printead{e1}}

\end{aug}

\begin{flushleft}
We consider balancing weights in the context of hierarchical data measured with mean-zero random noise. The balancing weights literature typically assumes that the covariates are measured without error, and the popular SBW objective (\cite{zubizarreta2015stable}) is known to produce the minimum variance estimator (conditional on the covariates) when the errors in the outcome model are independent and identically distributed. We instead consider the case when our covariates are measured with mean-zero random noise and our outcome model has errors that are correlated within states. Our proposed approach follows the regression-calibration literature (see, e.g., \cite{gleser1992importance}) and requires access to auxillary data to obtain an estimate of the variability of the error. Assuming a linear model and a classical measurement error model, we show that the bias of the SBW when balancing on the noisy covariate measurements is equivalent to the bias of the OLS estimator, and that balancing on an adjusted dataset (using regression calibration) removes this bias. On the other hand, we also find that accounting for state-level dependencies can induce bias when using regression-calibration, a result that may be of more general interest to people using regression-calibration with GLS instead of OLS. We then apply these techniques to estimate the effect of Medicaid expansion on the adult uninsurance rate in states that did not expand Medicaid as if they had expanded Medicaid in 2014 using region-level data estimated using the American Communities Survey. Using our proposed method, we estimate that Medicaid expansion would caused a -2.33 (-3.49, -1.16) percentage point change in the adult uninsurance rate among non-expansion states in 2014. 
\end{flushleft}


\begin{keyword}
\kwd{Synthetic controls}
\kwd{balancing weights}
\kwd{medicaid expansion}
\kwd{measurement error}
\kwd{hierarchical data}
\kwd{regression to the mean}
\end{keyword}

\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please use \tableofcontents for articles %%
%% with 50 pages and more                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Main text entry area:

\section{Introduction}

The 2010 Affordable Care Act (ACA) required states to expand their Medicaid eligibility requirements by 2014 to offer coverage to all adults with incomes at or below 138 percent of the federal poverty level (FPL). The United States Supreme Court ruled this requirement unconstitutional in 2012, allowing states to decide whether to expand Medicaid coverage. In 2014, twenty-six states and the District of Columbia expanded their Medicaid programs. From 2015 through 2020 an additional twelve states elected to expand their Medicaid programs. More recently, Oklahoma and Missouri voted to expand their programs in July 2021 \footnote{https://www.kansascity.com/news/politics-government/article250170945.html}. Following the passage of the American Rescue Plan in March 2021, Republican state legislatures in other traditionally conservative states, including Alabama, North Carolina, and Wyoming are also reportedly considering expanding their programs.\footnote{https://www.nbcnews.com/politics/politics-news/changed-hearts-minds-biden-s-funding-offer-shifts-medicaid-expansion-n1262229} The effects of Medicaid expansion on various outcomes, including uninsurance rates, mortality rates, and emergency department use, have been widely studied, primarily by using the initial expansions in 2014 and 2015 to divide expansion states into ``treated'' states and non-expansion states as ``control'' states. 

Medicaid enrollment is not automatic, and Medicaid take-up rates have historically varied across states. This variation is partly a function of state discretion in administering programs: for example, program outreach, citizenship verification policies, and application processes differ across states (\cite{courtemanche2017early}). When states expanded their eligibility requirements, the number that actually enrolled in Medicaid afterwards is random among the eligible individuals. Understanding how Medicaid eligibility expansion actually affected the number of uninsured individuals is an important effect. Existing studies have estimated that Medicaid expansion reduced the uninsurance rate between three and six percentage points among states that expanded Medicaid. These estimates differed depending on the data used, specific target population, study design, and level of analysis (see, e.g., \cite{kaestner2017effects}, \cite{courtemanche2017early}, \cite{frean2017premium}). However, none of these studies have directly estimated what the treatment's effect would have been on the controls (ETC). 

We study the effect of 2014 Medicaid expansion on adult uninsurance rates among states that did not expand Medicaid, using approximate balancing weights to estimate this effect (\cite{wang2017minimal}). Approximate balancing weights are a popular estimation method in causal inference that grew out of the propensity score weighting literature. Rather than iteratively modeling the propensity score until the inverse probability weights achieve a desired level of balance (the so-called ``propensity score tautology'' \cite{imai2014covariate}), recent papers propose using optimization methods to generate weights that enforce covariate balance between the treated and control units (see, e.g., \cite{hainmueller2012entropy}, \cite{imai2014covariate}, \cite{zubizarreta2015stable}). From an applied perspective, there are at least four benefits of this approach: first, it does not require iterating propensity score models to generate satisfactory weights. Second, these methods do not use outcomes in the modeling stage, mitigating the risk of cherry-picking model specifications. Third, these methods can constrain the weights to prevent extrapolation from the data, reducing model dependence \cite{zubizarreta2015stable}. Finally, the estimates are more interpretable: by making the comparison group explicit, it is easy to communicate exactly which units contributed to the counterfactual estimate.

To date most proposed methods in the balancing weights literature requires the following two assumptions: (1) the covariates are measured without error, and (2) the observations are independent. Our approach relaxes these assumptions and provides a general method that can be used for other applications given sufficient auxillary data. In our application we estimate our covariates using annual American Community Survey (ACS) microdata aggregated to the consistent public use microdata area (CPUMA) level. The sampling variability in the covariate estimates is a form of measurement error that may bias our effect estimates. We use regression calibration techniques to reduce the bias from the estimation error of our covariates \cite{gleser1992importance}. Moreover, CPUMAs are regions that nest within states. A common assumption in the applied literature is that regions within states contain dependencies that can worsen the efficiency of standard estimation procedures (see, e.g., \cite{cameron2015practitioner}). Using the assumed correlation structure outlined in \cite{kloek1981ols}, we modify the stable balancing weights objective (\cite{zubizarreta2015stable}) to account for state-level dependencies in the outcomes.\footnote{This approach can accommodate other assumed correlation structures as well.}

Our approach is also related to the ``synthetic controls'' literature. Synthetic controls are a popular balancing weights approach frequently used in the applied economics literature when studying the effects of policy changes using region-level time series cross-sectional data. The typical estimand in the synthetic controls literature is the treatment effect on the treated (ETT) (\cite{abadie2010synthetic}). By contrast, we consider the problem of estimating the ETC, which is also an estimand of substantive interest in the context of Medicaid expansion. For example, \cite{miller2019medicaid} used their estimates of the ETT to predict that had states that did not expand Medicaid done so, they would have seen 15,000 fewer deaths during their study period. However, these projections are based on estimates of the ETT, and therefore may be invalid in the presence of effect heterogeneity. We complement this literature by making our causal estimand and assumptions explicit and outlining our assumptions to directly target this effect using similar data.

Section 2 provides an overview of the data and defines the study period, covariates, outcome, and treatment. Section 3 discusses our methods, beginning by defining our target estimand, and then outlining our identification, estimation, and inferential procedures. Section 4 presents our results. Section 5 contains a discussion of the policy relevance of our findings, and Section 6 contains a brief summary. The Appendices contain additional materials, including proofs, summary statistics, and additional results.

\section{Policy Problem and Data}

\subsection{Policy Problem Statement}

We seek to predict how uninsurance rates would have changed in states that did not expand Medicaid in 2014 had they expanded Medicaid. In contrast to many existing studies, we target the ETC rather than the ETT, which we believe may differ. One natural reason we might think this is that every state had different coverage policies prior to 2014, and non-expansion states tended to have less generous policies than expansion states; ``Medicaid expansion'' therefore represents a heterogeneous set of treatments that are distributed unevenly across expansion and non-expansion states. Averaged over the non-expansion states, which had higher uninsurance rates prior to Medicaid expansion, we might expect the ``average effect'' to be larger in absolute magnitude than among the expansion states, where ``Medicaid expansion'' on average reflected less drastic policy changes.

Moreover, even if we thought of ``Medicaid expansion'' as a single set of policy changes (as our analysis assumes below; see Section~\ref{sssec:txassign}), we still might expect these effects to differ. For example, all states that were entirely controlled by the Democratic Party at the executive and legislative levels expanded their Medicaid programs, and only states where the Republican Party controlled at least part of the state government failed to expand their programs. Prior to the 2014 Medicaid expansion, \cite{sommers2012understanding} found that conservative governance was associated with lower Medicaid take-up rates. This could reflect differences in program implementation. Similar differences in implementation may cause heterogeneous effects of Medicaid expansion, if, for example, conservative states on average make it more difficult to enroll in Medicaid.\footnote{We emphasize that we view expansion implementation as existing downstream from ``Medicaid expansion'', which we view as the policy change itself.} Interestingly, \cite{sommers2012understanding} also find that the association between conservative governance and lower take-up rates prior to 2014 existed even after controlling for a variety of factors pertaining to state-level policy administration decisions. They posit that this may reflect cultural conservatism: people in conservative states are more likely to view enrollment in social welfare programs negatively, and therefore be less likely to enroll. Regardless of the cause, these factors could work attenuate the effects of Medicaid expansion among non-expansion states relative to expansion states. 

Regardless of whether the ETC differs from the ETT, targeting the ETC is interesting in it's own right: to the extent the goal of studying Medicaid expansion is to understand the foregone benefits (or potential harms) of Medicaid, estimating the ETC is an important quantity of interest. Authors have previously made claims about the ETC without targeting the estimand directly: as noted above, \cite{miller2019medicaid} use their estimates of the ETT to predict that had non-expansion states expanded Medicaid, they would have seen 15,000 fewer deaths during their study period. Moreover, because many downstream effects of Medicaid expansion are plausibly monotonic in the number of newly insured (including mortality), we study the effects on the non-elderly adult uninsurance rate. We therefore contribute to the literature by targeting this causal estimand directly.

\subsection{Data Source and Study Period}

Our primary data source is the annual household and person public use microdata files from the American Community Survey (ACS) from 2011 through 2014. The ACS is an annual cross-sectional survey of approximately three million individuals across the United States. The public use microdata files include information on individuals in geographic areas greater than 65,000 people. The smallest geographic unit contained in these data are public-use microdata areas (PUMAs), arbitrary boundaries that nest within states but not within counties or other more commonly used geographic units. One limitation of these data is a 2012 change in the PUMA boundaries, which do not overlap well with the previous boundaries. As a result, the smallest possible geographic areas that nest both PUMA coding systems are known as consistent PUMAs (CPUMAs). The United States contains 1,075 total CPUMAs, with states ranging from having one CPUMA (South Dakota, Montana, and Idaho) to 123 CPUMAs (New York). Our primary dataset (discussed further in Section~\ref{sssec:txassign}) contains 929 CPUMAs among 46 states. The average total number of sampled individuals per CPUMA across the four years is 1,001; the minimum number of people sampled was 334 and the maximum is 23,990.

We choose to begin our study period in 2011 following \cite{courtemanche2017early}, who note that several other aspects of the ACA were implemented in 2010 -- including the provision allowing for dependent coverage until age 26 and the elimination of co-payments for preventative care -- and likely induced differential shocks across states. We also restrict our post-treatment period to 2014 because several additional states expanded Medicaid in 2015, including Indiana, Michigan, and Pennsylvania. However, these states did not expand Medicaid contemporaneously with the 2014 ACA provisions. Without strong assumptions, these second-year expansion states cannot help us estimate the effect of the 2014 expansion. 

\subsection{Treatment assignment} \label{sssec:txassign}

While some states expanded Medicaid in 2014 and other states did not, assigning a binary treatment status simplifies a more complex reality. There are three reasons to be cautious about this simplification. First, as noted previously, states differed substantially in their Medicaid coverage policies prior to 2014. Given perfect data we might ideally consider Medicaid expansion as a continuous treatment with values proportional to the number of newly eligible individuals. The challenge, again, is correctly identifying newly eligible individuals in the data (see \cite{frean2017premium}, who attempt to address this). Second, \cite{frean2017premium} note that five states (California, Connecticut, Minnesota, New Jersey, and Washington) and the District of Columbia adopted partial limited Medicaid expansions prior to 2014. \footnote{\cite{kaestner2017effects} and \cite{courtemanche2017early} also consider Arizona, Colorado, Hawaii, Illinois, Iowa, Maryland, and Oregon to have had early expansions.} Lastly, timing is an issue: among the states that expanded Medicaid in 2014, Michigan's expansion did not go into effect until April 2014, while New Hampshire's expansion did not occur until September 2014.

Our primary analysis excludes New York, Vermont, Massachusetts, Delaware, and the District of Columbia from our pool of expansion states because these states had comparable Medicaid coverage policies prior to 2014 (\cite{kaestner2017effects}). We also exclude New Hampshire because it did not expand Medicaid until September 2014. While Michigan expanded Medicaid in April 2014, we leave this state in our pool of ``treated'' states. We consider the remaining expansion states as ``treated'' and the non-expansion states (including those that later expanded Medicaid) as ``control'' states. We later consider the sensitivity of our results to these classifications by removing the early expansion states noted by \cite{frean2017premium}. Our final dataset contains aggregated statistics for all of the above variables for 925 CPUMAs in our non-expansion and our pool of expansion states. There are 414 CPUMAs among 24 non-expansion states and 511 CPUMAs among 21 expansion states. When we exclude the early expansion states for sensitivity analyses, we are left with 296 CPUMAs across 17 expansion states. We provide a complete list of states by classification in Appendix AA.

\subsection{Outcome}

Our outcome of interest is the non-elderly adult uninsurance rate in 2014, which we denote using $Y$. While take-up among the Medicaid-eligible population is a more natural outcome, we choose the non-elderly adult uninsurance rate for two reasons, one theoretic and one practical. First, Medicaid eligibility in the post-period is likely endogenous: Medicaid expansion may affect an individual's income and poverty levels, which in general define Medicaid eligibility. Second, we can better compare our results with the existing literature, including \cite{courtemanche2017early}, who use the same outcome. One drawback of using this outcome is that the simultaneous adoption of other ACA provisions by all states in 2014 also affect this rate. However, we only attempt to estimate the effect of Medicaid expansion in 2014 in the context of this changing policy environment. We discuss this further in Section ~\ref{ssec:estimand} and Section~\ref{ssec:identification}. 

\subsection{Covariates}

We use the underlying individual-level ACS survey data and accompanying survey weights to aggregate the data at the CPUMA level. We choose our covariates to approximately align with those considered in \cite{courtemanche2017early} and that are likely to be potential confounders. Because we are ultimately interested in calculating rates, these variables include both the numerator and denominator counts. 

Using the data we estimate: the total non-elderly adult population for each year 2011-2014; the total labor force population (among non-elderly adults) for each year 2011-2013; and the total number of households averaged from 2011-2013. We also construct an average of the total non-elderly adult population from 2011-2013. These are our denominator variables. For our numerator counts, we estimate the total number of: females; whites; people of Hispanic ethnicity; people born outside of the United States; citizens; people with disabilities; married individuals; people with less than a high school education, high school degrees, some college, or college graduates or higher; people living under 138 percent of the FPL, between 139 and 299 percent, 300 and 499 percent, more than 500 percent, and who did not respond to the income survey question; people aged 19-29, 30-39, 40-49, 50-64; households with one, two, or three or more children, and households that did not respond about the number of children. We average these estimated counts from 2011-2013. For each individual year from 2011-2013, we then estimate the total number of people who were unemployed and uninsured at the time of the survey (calculated among all non-elderly adults and all non-elderly adults within the labor force, respectively). We divide the numerator counts by the corresponding denominator counts to estimate the percentage in each category. For the demographics, these include the average number of non-elderly adults from 2011-2013. For the time-varying variables, we use the corresponding year (where uninsurance rates are calculated as a fraction of the labor force rather than the non-elderly adult population). We also calculate the average non-elderly adult population growth and the average number of households to adults across 2011-2013. 

In addition to the ACS microdata, we use 2010 Census data to calculate the approximate percentage of people living within an ``urban'' area for each CPUMA. Finally, we include three state-level covariates reflecting the partisan composition of each state's government in 2013 using data from the National Conference of State Legislatures (NCLS). Specifically, we generate an indicator for states with a Republican governor, an indicator for states with Republican control over the lower legislative chamber, and an indicator for states with Republican control over both chambers of the legislature and the governorship.\footnote{Nebraska is the only state with a unicameral legislature; moreover, the legislature is technically non-partisan. We nevertheless classified them as having Republican control of the legislature.} 

\section{Methods}\label{sec:methods}

In this section we present our causal estimand, identifying assumptions, estimation strategy, and inferential procedure.

\subsection{Estimand} \label{ssec:estimand}

We let $c$ index CPUMAs, $s$ index states, and $t$ index time (by year) with $T = 2014$. Let $n_1$ be the number of treated CPUMAs, $n_0$ be the number of control CPUMAs, and $n$ be the total number of CPUMAs. Similarly, let and $m = m_1 + m_0$ states (with $m_1$ and $m_0$ defined analogously). Each state has $p_s$ CPUMAs. Letting $A_{st}$ indicate whether a state expanded Medicaid in year $t$ ($A_{st} = 1$) or not ($A_{st} = 0$), we use potential outcomes notation (see, e.g., \cite{rubin2005causal}) to denote a CPUMA's uninsurance rate under Medicaid expansion in year $t$ -- $Y_{cst}^{A_{st} = 1}$ -- and without Medicaid expansion -- $Y_{cst}^{A_{st} = 0}$. We define our targeted estimand:

\begin{equation}
\psi = \bar{Y}_{0, T}^1 - \bar{Y}_{0, T}^0 = n_0^{-1}\sum_{scT: A_{sT} = 0} \mathbb{E}\{Y_{scT}^{A_{sT} = 1} - Y_{scT}^{A_{sT} = 0} \mid X_{scT}\}
\end{equation}

This represents the expected treatment effect on non-expansion states conditioning on the observed covariate distribution of the non-expansion states (see, e.g., \cite{imbens2004nonparametric}).

The challenge is that we do not observe the counterfactual outcomes for non-expansion CPUMAs had they been in states that expanded their Medicaid programs. We therefore require causal assumptions to identify this counterfactual quantity using our observed data.\footnote{As noted previously, the 2014 Medicaid expansion occurred simultaneously with the implementation of several other major ACA provisions, including (but not limited to) the creation of the ACA-marketplace exchanges, the individual mandate, health insurance subsidies, and community-rating and guaranteed issue of insurance plans (\cite{courtemanche2017early}). Almost all states broadly implemented these reforms beginning January 2014. Conceptually we think of the other ACA components as a state-level treatment ($R$) separate from Medicaid expansion ($A$). Our total estimated effect may also include interactions between these policy changes; however, we do not attempt to separately identify these effects. Without further assumptions -- including that these effects are additive and that the year-one effects are constant across time -- we cannot generalize these results beyond 2014.} 

\subsection{Identification} \label{ssec:identification}

We appeal to the following causal assumptions to identify our target estimand from our observed data: the stable unit treatment value assumption (SUTVA), no unmeasured confounding given the true covariates and outcome values, and no anticipatory treatment effects. We additionally invoke parametric assumptions that model the measurement error and that express our estimand in terms of parameters from a linear model. Our estimand is not necessarily identifiable under these conditions.\footnote{For example, if all of the variables are jointly normal, then the model is not identifiable (see, e.g., \cite{cheng1999statistical}).} We therefore appeal to ideas from the ``regression-calibration'' literature (\cite{gleser1992importance}) to ensure that identification and consistent estimation of our target estimand is possible.

We first assume the SUTVA at the region level. Assuming the SUTVA has two implications for our analysis: first, that there is only one version of treatment; second, that each unit's potential outcome only depends on it's treatment assignment. We discussed potential violations of the first implication previously when considering how to reduce Medicaid Expansion to a binary treatment. The second implication could be violated if one CPUMA's expansion decision affected uninsurance rates in another CPUMA (see, e.g., \cite{frean2017premium}). On the other hand, our assumption does allows for interference among individuals living within CPUMAs, which is a weaker assumption than no interference among any individuals at all. Further addressing this issue is beyond the scope of this paper.

Second, we assume no anticipatory treatment effects. This implies that for any time period $t < T$, a CPUMA's observed outcome $Y_{sct}$ is equal to its potential outcome absent treatment $Y_{sct}^0$. This assumption is necessary because we will use pre-treatment outcomes as covariates. If these outcomes were affected by a state's decision to expand Medicaid prior to 2014, controlling for these covariates could bias our results. This assumption is violated in our study because several states allowed specific counties to expand Medicaid prior to 2014. We later test the sensitivity of our results to the exclusion of these states.

Third, we assume no unmeasured confounding. Specifically, we posit that in 2014 the potential outcomes for each CPUMA are marginally independent of the state-level treatment assignment conditional on CPUMA and state-level covariates $X_{scT}$, a $q$ dimensional vector of covariates. This vector includes both time-varying pre-treatment covariates, including pre-treatment outcomes, and covariates averaged across 2011-2013, such as demographic characteristics, that we discuss in Section~\ref{ssec:data}.\footnote{To be precise, letting $R$ be the other ACA provisions that were implemented across all states, we also assume that $Y^{A = a, R = 1}_{scT} \perp A_{scT} \mid X_{scT}$ (and that $Y_{scT}^{A, R = 1} = A_{scT}Y_{scT} + (1 - A_{scT})Y_{scT}$). This assumption implies that in the context of this changing policy environment (indicated by $R = 1$), Medicaid expansion has the same expected effect in expansion and non-expansion CPUMAs with covariate values $X = x_0$.}

\begin{align*}
Y_{scT}^a \perp A_{scT} \mid X_{scT}
\end{align*}

We believe this assumption is reasonable given our rich covariate set informed by prior literature. 

Finally, we assume that the potential outcomes are linear in the true covariates with mean zero equi-correlated errors within states:

\begin{equation}\label{eqn:linmod}
Y_{scT}^a = \alpha_{a, T} + X_{scT}^T\beta_{a, T} + \epsilon_{scT} + c_{sT}
\end{equation}

where the errors $\epsilon_{scT}$ and $c_{sT}$ are mean-zero and independent from each other. For example, $\epsilon_{scT}$ captures time-specific idiosyncracies at the local level, possibly due to the local policy environment. By contrast $c_{sT}$ captures time-specific idiosyncracies at the state-level that are shared across CPUMAs within a state due to the common policy environment. However, we emphasize that our assumption of no unmeasured confounding implies that these errors are uncorrelated with the true covariates and the treatment assignment. Because our covariates include pre-treatment outcomes, this assumption also implies that these errors are uncorrelated over time. For ease of notation, we hereafter omit the dependence of these quantities on $t$ for the remainder of this paper except when needed for clarification.

Under these assumption we can rewrite our causal estimand in terms of the model parameters:

\begin{equation}\label{eqn:outcome}
\bar{Y}_0^a = \alpha_a + \bar{X}_0^T\beta_a    
\end{equation}

If we observed $(Y, X)$, we could estimate the causal effect using our observed data using techniques such as OLS or SBW. However, instead of $(Y, X)$, we observe the noisy measurements $(J, W)$. Importantly, $Y_{sc}^a \perp A_{sc} \mid X_{sc} \centernot\implies J_{sc}^a \perp A_{sc} \mid W_{sc}$. Treating $(J, W)$ as equal to $(Y, X)$ may therefore result in biased estimates of $\bar{Y}_0^a$ (see Section~\ref{ssec:methodsmsrment} for more on this point). We rely on the following measurement error model to address this.

First, we assume that our observed covariates and outcomes are equal to the true values plus mean-zero independent (though not necessarily identically distributed) Gaussian noise. We believe this is reasonable because measurement error in our context is sampling variability. We further assume that the measurement errors in our outcomes is uncorrelated with the measurement error in our covariates. We also believe this is reasonable because our outcomes are measured on a different cross-sectional survey than our covariates. 

\begin{equation}\label{eqn:msrmenterror}
(J_{sc}, W_{sc}) \sim N((Y_{sc}, X_{sc}), \begin{pmatrix}
\xi^2_{sc} & 0 \\ 0 & \Sigma_{\nu\nu, sc} \end{pmatrix})
\end{equation}

Our primary data is then $O = (J, W, A)$; that is, a matrix of noisy measurements of the covariates and outcome, and a vector of the true treatment assignments.\footnote{Our covariates are almost all ratio estimates, which will in general be biased. This bias, however, decreases quickly with the sample size (is $O(n^{-1})$). Given that our CPUMA sample sizes are all over 300, we treat these estimates as unbiased in our analysis.} Under this model we can conclude that $\bar{J}_0$ serves as an unbiased estimate of $\bar{Y}_0^0$. The challenge remains identifying and estimating $\bar{Y}_0^1$.

The technical conditions for the identifiability of $\bar{Y}_0^1$ depends on the identifiability of $(\alpha_1, \beta_1)$, which in turn depend on underlying distributions of covariates, model errors, and measurement errors (see, e.g., \cite{cheng1999statistical}). We therefore instead leverage our access to auxillary data and use the ``regression calibration'' approach (see, e.g., \cite{carroll2006measurement}) to ensure that we can identify and consistently estimate $\bar{Y}_0^1$. At a high-level, regression calibration assumes access to auxillary data ihat can be used to estimate the measurement error covariance matrix $\Sigma_{\nu\nu}$ (assuming that $\Sigma_{\nu\nu, sc}$ is constant; alternatively, we can think of this as the limiting distribution of $n^{-1}\Sigma{\nu\nu, sc}$). Using this information and the observed data $W$, we generate a function $\eta(W)$ to impute $X$. We then run our estimation procedure on this imputed dataset. 

To be precise, we define our function $\eta_a$ as a linear model of the expected value of $X$ given $(W, A = a)$: $\eta_1(W_{sc}) = \mathbb{E}\{X_{sc} \mid W_{sc}, A_{s} = 1\} = \upsilon_1 + \kappa_1 (W_{sc} - \upsilon_1)$, where $\upsilon_1 = \mathbb{E}\{X \mid A = 1\}$, $\kappa_1 = \Sigma_{XX \mid A = 1}\Sigma_{WW \mid A = 1}^{-1}$, $\Sigma_{XX \mid A = 1} = \mathbb{E}\{X_{sc}X_{sc}^T \mid A = 1\}$ and $\Sigma_{vv} = \mathbb{E}\{v_{sc}v_{sc}^T\}$, $\Sigma_{WW \mid A = 1} = \Sigma_{XX \mid A = 1} + \Sigma_{vv} = \mathbb{E}\{W_{sc}W_{sc}^T\}$.\footnote{We caution that our estimand is conditional on $X$ while we motivate this procedure under the assumption that $X$ is random. We can redefine these terms as the limiting distribution as $n \to \infty$ of their empirical counterparts, under the assumption that these limits exist.}

By linearity, we can rewrite our outcome model in terms of $\eta_1$:

\begin{equation}
    J_{sc}^1 = \alpha_1 + \eta_{A_{sc}}(W_{sc})^T\beta_1 + (X_{sc} - \eta_{A_{sc}}(W_{sc}))^T\beta_1 + \xi_{sc} + \epsilon_{sc} + c_s 
\end{equation}

$\eta_1$ is not estimable given our observed data; for example, our observed data does not identify $\Sigma_{vv}$. We instead leverage our auxillary data to identify and estimate this quantity, and in turn to estimate $\bar{Y}_0^1$. We discuss the estimation procedure further below in Section ~\ref{ssec:methodsmsrment}.

\subsection{Estimation}

We propose to use balancing weights to estimate $\bar{Y}_0^1$. We first review balancing weights and the SBW objective proposed by \cite{zubizarreta2015stable}. This method requires that the covariates are measured without error. However, we show that under the classical-errors-in-variables model the SBW estimate of the super-population parameter $\mathbb{E}\{Y^1 \mid A = 0\}$ has the same bias as the OLS estimate (alternatively, we can think of this as the asymptotic bias of $\bar{Y}_0^1$). 

We therefore modify SBW for our setting. The broad steps are: first, we estimate a linear model of the true covariates given the observed covariates. We also leverage the the ACS microdata replicate survey weights for this procedure. We consider three covariate sets: (a) the original covariates with no adjustment; (b) a homogeneous adjustment that uses the same linear model for all CPUMAs; and (c) a heterogeneous adjustment that uses a different model for each CPUMA, accounting for the differential precision for each CPUMA's covariate measurements. Second, we estimate weights that balance the expansion states' adjusted covariates to the mean of the observed non-expansion states' covariates within a chosen vector of thresholds $\delta$. To generate our weights we use both the SBW objective, which minimizes the variance of the weights, and our proposed modification, which we call H-SBW. In contrast to SBW, H-SBW minimizes the variance of the estimator assuming positive equi-correlation between CPUMAs within states (we could theoretically use this approach with other assumed correlation structures as well). Third, due to remaining imbalances in the covariates, some of which are quite large, we follow the suggestion of \cite{ben2018augmented}, and the sensitivity of our results to ridge-regression augmented weights. This procedure allows us to achieve better covariate balance by extrapolating beyond the support of the data. We conclude by estimating the counterfactual mean $\bar{Y}^1_0$ with each set of weights.

\subsection{Balancing weights}

\cite{zubizarreta2015stable} propose to estimate the set of weights $\gamma$ that minimizes the following objective:

\begin{equation}\label{eqn:objective}
\gamma = \arg\min_{\tilde{\gamma} \in \Gamma} \sum_{sc: A_{sc} = 1} \gamma_{sc}^2
\end{equation}

\begin{equation}\label{eqn:constraint}
\Gamma = \{\tilde{\gamma}: \lvert \tilde{\gamma}^T b(X_{1, r}) - \bar{b(X_{0, r})} \lvert \le \delta_r \\ (r = 1, ..., q), \ \tilde{\gamma}_{sc} > 0, \sum_{sc: A_{sc} = 1}\tilde{\gamma}_{sc} = 1\}
\end{equation}

for $\delta_r = 0$ for all $r = 1, ..., q$. Once we find a set of weights satisfying this constraint, we can then estimate $\bar{Y}_0^1$ as

\begin{equation}\label{eqn:psi}
\hat{Y}_0^1 = \sum_{s: A_s = 1}^{m_1}\sum_{c = 1}^{p_s}\gamma_{sc}^\star Y_{sc} - n_0^{-1}\sum_{s: A_s = 0}^{m_0}\sum_{c = 1}^{p_s}Y_{sc}
\end{equation}

In the case where we observed $X_{sc}$ as assume $b$ is the identity, the bias of $\bar{Y}^1_0$ is less than or equal to $\lvert\beta_1\rvert^T\delta$, and is therefore 0 if we can set $\delta = 0$ (see, e.g., \cite{zubizarreta2015stable}). Moreover, $\hat{Y}_0^1$ produces the minimum variance estimator -- conditional on $X$ -- assuming that the errors are independent and identically distributed. This method also assumes that we observe the true covariates $X$.

\subsubsection{Measurement error}\label{ssec:methodsmsrment}

The first advancement in our estimation strategy comes in our balance constraints: rather than reweighting the observed covariates, we instead propose reweighting $\hat{\eta}_1(W_{sc})$. This helps to correct for the bias induced by the estimation error from the observed covariates. Theorem 1 below shows that if we do not do this we will not achieve the desired balance on $X$ and that this will bias our resulting estimator:

\begin{theorem}
Assume that Equation~\ref{eq:linmod} holds and consider the measurement error model in Equation~\ref{eqn:msrmenterror}. The SBW estimator that attempted to set $\sum_{sc: A_s = 1}W_{sc}\gamma_{sc} = \upsilon_0$  has the following expected imbalance on $X_{sc}$:

\begin{align*}
\mathbb{E}\{\sum_{sc}\gamma_{sc}W_{sc} - \upsilon_0\} \le \|(\upsilon_0 - \upsilon_1)^T(\kappa_1 - I_d)\| + \delta
\end{align*}

Under a linear outcome model, this leads to the following bound on the bias:

\begin{align*}
\mathbb{E}\{\hat{\psi}^{1}_0 - \psi^1_0\} = \lvert(\upsilon_0 - \upsilon_1)^T(\kappa_1 - I_d)\beta_1\lvert + \delta^T\lvert\beta\lvert
\end{align*}
\end{theorem}

Notice that when $\delta = 0$ this is equivalent to the bias of a linear combination of coefficient estimates from the OLS-based regression estimator.\footnote{While our estimand is defined as conditional on $X$, we can think of this as the asymptotic bias of this estimator as $n \to \infty$.} 

We prove this result in Appendix A. The intuition is as follows: exact balancing weights implicitly estimate $\beta_1$ on a subset of the data where we have sufficient covariate overlap. We can think of SBW as returning a solution to some weighted-least squares problem. Assuming that the outcome model holds across all of the data, WLS and OLS are estimating the same $\beta_1$; consequently, the bias that effects the least squares solution will have the same effect on the WLS, and therefore SBW, solution. We therefore instead balance on an estiamte of $\eta_1$, which Theorem 2 below shows is a consistent estimator of $\psi_0^1$:

\begin{theorem}

Let $\eta_1 = \mathbb{E}\{X_{sc} \mid W_{sc}, A_s = 1\}$. Consider the weights $\sum_{sc}\gamma_{sc}\eta_1(W_{sc}) = \upsilon_0$ where $\gamma_{sc}$ minimize the SBW objective. Then

\begin{align*}
    \mathbb{E}\{\sum_{sc}\gamma_{sc}\eta_{1, sc}Y_{sc}\} - \psi_0^1\} \le \lvert \delta \lvert ^T\beta
\end{align*}

Moreover, if we have a consistent estimate of $\eta_1$, $\hat{\eta}_1$, then

\begin{align*}
    \mathbb{E}\{\sum_{sc}\gamma_{sc}\hat{\eta}_{1, sc}Y_{sc}\} - \psi_0^1\} \to \lvert \delta \lvert ^T\beta
\end{align*}

\end{theorem}

We prove these results more formally in Appendix A. 

The challenge is estimating $\eta_1$. For our application we use the ACS microdata replicate survey weights to estimate CPUMA-level matrices $\Sigma_{vv, sc}$. Specifically, the ACS micro-data contains 80 sets of replicate survey weights we use to construct 80 additional CPUMA-level datasets. We then estimate $\Sigma_{vv, sc}$ as:

\begin{equation}
\hat{\Sigma}_{vv, sc} = \frac{4}{80}\sum_{b=1}^{80}(W_{sc}^B - \bar{W}_{sc})(W_{b, sc} - \bar{W}_{sc})^T
\end{equation}

where $\bar{W}_{sc}$ is the vector of covariate values estimated using the original ACS weights, and the $4$ in the numerator comes from the process used to generate the replicate survey weights. We average these together across the treated units to obtain $\hat{\Sigma}_{vv}$.

At this point estimating $\Sigma_{XX}$, and therefore $\eta_1$, is straightforward. Letting $\hat{\Sigma}_{WW \mid A = 1}$ be the empirical covariance matrix of $W$ on the original dataset, we estimate $\hat{\Sigma}_{XX \mid A = 1} = \hat{\Sigma}_{WW \mid A = 1} - \hat{\Sigma}_{vv \mid A = 1}$. Let $\hat{\kappa}_1 = \hat{\Sigma}_{WW \mid A = 1}^{-1}\hat{\Sigma}_{XX \mid A = 1}$. We generate $\hat{\eta}_1(W_1)$ using the empirical analogues of Equation~\ref{eqn:adjustment}:

\begin{equation}
\hat{\eta}_1(W_1) = \bar{W}_1 + \hat{\kappa}_1^T(W_1 - \bar{W}_1)
\end{equation}

This approximately aligns with the adjustments suggested by \cite{carroll2006measurement} and \cite{gleser1992importance}. However, this adjustment is optimal only when the measurement error variances are constant and we assume that the data are independent and identically distributed. \footnote{Additionally, we define our target estimand conditional on $X$, while regression-calibration is motivated by random-X theory. The adjustment procedure can be motivated in either case (see, e.g., \cite{gleser1992importance}). For example, we can define the covariance matrix $\Sigma_{XX}$ as the limit of the sample covariance matrix $\Sigma_{XX}(n)$ as $n\to\infty$.} 

Our application has two departures from the motivating theory: first, we have access to information about a substantial source of heterogeneity in the measurement error. Specifically, we know that regions with large populations are estimated quite precisely, while regions with small populations are estimated much less precisely. Additionally, for a given CPUMA, some covariates are measured using three years of data, and others only one. However, using the conventional regression calibration approach will adjust both precisely and imprecisely estimated CPUMA using a common coefficient matrix. While this does not affect the bias of our estimates, we can potentially improve the efficiency of our estimator by exploiting this information. We therefore consider an alternative estimation strategy that allows for a CPUMA-specific adjustment assuming the model $v_{sc} \sim N(0, \Sigma_{vv} \odot P_{sc})$, where $P_{sc} = p_{sc}p_{sc}^T$, where $p_{sc}$ is the vector of sample sizes used to calculate each covariate. We refer this procedure as the ``heterogeneous adjustment,'' and provide further details in Appendix B.

A second departure from the motivating theory is that our covariates are likely dependent. In Appendix A, Proposition X, we consider a positive equicorrelation dependence structure (similar to our outcome model) and show that this dependence does induce bias for the SBW estimator that balances the calibrated dataset. However, this dependence can bias estimators that have a more complicated dependence on the data, including the H-SBW objective we propose below. We show this in Appendix A, Proposition Y, and provide simulation evidence of this in Appendix X. Theoretically, we could adjust for this by using our observed data to estimate $\mathbb{E}\{X_{sc} \mid W\}$ under a known correlation structure; however, we do not pursue this approach for this application. 

This is the first application we are aware of to apply regression calibration in the context of balancing weights to address measurement error. Importantly, this technique requires access to auxillary information to estimate $\eta_1$, such as a replication study (see, e.g., \cite{gleser1992importance}), or survey microdata as in our application. Alternatively, region-level datasets often contain region-level variance estimates; if one is willing to assume $\Sigma_{vv, sc}$ is diagonal, one could use this approach as well. Regardless, we emphasize two critical assumptions for using this procedure in our context: (1) the outcome model is linear in the true covariates (i.e. $b$ as defined in Equation~\ref{eqn:constraint} is the identity); and (2) the measurement error in the outcome is uncorrelated with the measurement error in the covariates. The first assumption is strong, though commonly used.\footnote{Regression calibration techniques are used sometimes in the context of generalized linear models, when the bias induced by the error is thought to be small (see, e.g., CITE).} The second assumption is reasonable in our setting, because the outcomes are estimated from a different cross-section than the covariates. 

\subsubsection{H-SBW objective}

The next challenge is finding ``the best'' set of weights within our constraint set. Unlike the setting outlined in \cite{zubizarreta2015stable}, our application likely has state-level dependencies which may reduce the efficiency of the SBW estimator. We therefore add the tuning parameter $\rho \in [0, 1]$ to penalize the within-state cross product of the weights, as detailed in Equation~\ref{eqn:objective}, representing a constant within-state correlation of the errors.

\begin{equation}\label{eqn:objective}
\gamma = \arg\min_{\tilde{\gamma} \in \Gamma} \quad \sum_{s: A_s = 1}^{m_1}(\sum_{c = 1}^{p_s} \tilde{\gamma}_{sc}^2 + \sum_{c \ne d}\rho \tilde{\gamma}_{sc}\tilde{\gamma}_{sd})\\
\end{equation}

To build intuition about this objective, for $\delta \to \infty$, the following solution is attained:

\begin{equation}\label{eqn:sbwsol}
\gamma_{sc} \propto \frac{1}{(p_s - 1)\rho + 1}
\end{equation}

Setting $\rho = 0$ returns the SBW solution: $\gamma_{sc} \propto 1$. When setting $\rho = 1$, we get $\gamma_{sc} \propto \frac{1}{p_s}$. In other words, as we increase $\rho$, this objective downweights CPUMAs in states with large numbers of CPUMAs and upweights CPUMAs in states with small numbers of CPUMAs (assigning each CPUMA within a state equal weight). In short, as we increase $\rho$, the objective will attempt to more uniformly disperse weights across states. In Appendix A we show that the H-SBW objective produces the minimum variance estimator under the constraint set for this correlation structure. In theory we could incorporate any assumed correlation structure into this objective in a similar fashion; however, the number of tuning parameters might change. Broadly speaking, we can think of H-SBW being to SBW what generalized least squares (GLS) is to ordinary least squares (OLS): both SBW and OLS can produce unbiased estimates of model parameters, but H-SBW and GLS can improve the efficiency of an estimator under different assumptions about the correlation structure of the outcomes.

As noted previously, in the context of the regression-calibration approach proposed above, the penalty term $\rho$ induces bias when $\rho > 0$. Even so, if the H-SBW estimator still may have lower MSE than the SBW estimator depending on the distribution of the data. In our simulation study available in Appendix GG, we find that the bias is typically small when $\rho$ is small and/or the measurement errors are small (for a fixed $\beta$). The difference in performance of H-SBW relative to SBW is likely also related to the difference in the performance of OLS versus GLS in the context of measurement error (see, e.g., CITE). However, further investigation of these effects is beyond the scope of this study.

\subsubsection{Hyperparameter selection}

For our application we assume that the conditional expectation function for the potential outcomes under treatment are a linear function of the true covariates. We therefore seek to make $\delta$ as small as possible; practical guidance in the literature is to reduce the standardized mean differences to be less than 0.1 (CITATION). In our application, all of our covariates are measured on the same scale; and because some of these covariates have very small variances (for example, percent female), we instead target the percentage point differences. We can then estimate $\psi$ using Equation~\ref{eqn:psi}, substituting $J_{sc}$ for $Y_{sc}$ and plugging in the weights $\gamma$. 

We choose $\delta$ using domain knowledge about which covariates are most likely to be important predictors of treatment. Specifically, we know that pre-treatment outcomes are often the best predictors of post-treatment outcomes, so we constrain $\delta$ to be 0.05 percentage points (out of 100) for pre-treatment outcomes. Because health insurance is often tied to employment, we also prioritize balancing pre-treatment uninsurance rates, seeking to reduce imbalances below 0.15 percentage points. For the remaining covariates we let $\delta$ be 0.5 percentage points for average population growth and household to adult ratio, 1 percentage point for female, Hispanic ethnicity, white race, age category, disability, and number of children category; 2 percentage points for urban, citizenship, education category, income-to-poverty category, student, and foreign-born. Finally, we constrain the Republican governance indicators to fall within 25 percentage points. While we believe that Republican governance is an important covariate to balance, we are unable to reduce the constraints further given the support of the data.

Finally, we consider $\rho \in \{0, 1/6\}$. As noted above, the first choice is equivalent to the SBW objective, while the second assumes constant within-state equicorrelation of $1/6$. We choose this number to be small in order to limit the bias induced by H-SBW in the context of measurement error.

Data driven procedures for hyperparameter selection are also possible with access to pseudo-outcomes. For example, one could use feasible GLS using the pre-treatment outcomes and covariates to estimate $\rho$. This $\rho$ would be optimal under the assumption that the model holds in the pre-treatment period and that the correlation structure is constant over time and invariant to treatment assignment. Data-driven procedures for $\delta$ are also possible. For example, if a long pre-treatment period is available, \cite{abadie2015synthetic} propose tuning their weights with respect to covariate balance using a ``training'' and ``validation'' period. However, this procedure is heuristically better motivated when estimating the ETT than the ETC: we may believe that the $\delta$ that minimizes the bias of $\hat{Y}^0_{1, t}$ for $t < T$ minimizes the bias of $\hat{Y}^0_{1, T}$. However, the $\delta$ that minimizes the bias in $Y^1_{0, t}$ less plausibly minimizes the bias of $\hat{Y}^1_{0, T}$. This would be a particular worry if we believe there may be strong confounders of $Y^1$ that are only weakly confound $Y^0$. We discuss this further in Appendix XX.

\subsubsection{Bias-correction for imbalances}

In practice we are unable to reduce the balance constraints to our preferred level without generating very extreme weights. Following the recent literature on synthetic controls, we test the sensitivity of our results to the imbalances in the observed (or adjusted) covariates using ridge-regression augmented weights \cite{ben2018augmented}. Letting $\hat{X}_1$ be the matrix of adjusted covariates, and $\gamma^{hsbw}$ be our H-SBW weights, we consider the regression-augmented weights:

\begin{equation}
\gamma^{aug} = \gamma^{hsbw} + (\gamma^{hsbw}\hat{X}_1 - \bar{W}_0)^T(\hat{X}_1^T\Omega^{-1}\hat{X}_1 + \lambda I_q)^{-1}\hat{X}_1^T\Omega^{-1}
\end{equation}

where $\Omega$ is a block diagonal matrix with diagonal entries equal to one and the within-group off diagonals equal to $\rho$. We choose $\lambda$ so that all imbalances fall within 0.5 percentage points. The cost of this procedure is that we must extrapolate beyond the support of the data, increasing the model dependence of our estimates. We refer to \cite{ben2018augmented} for more details about this procedure. For our results we consider estimators using SBW ($\rho = 0$), H-SBW ($\rho = 1/6$), and ridge-augmented versions of SBW and H-SBW that we call BC-SBW and BC-HSBW. 

\subsection{Model validation}

For fixed covariates and targeted levels of imbalance $\delta$, we use the heuristic that a good model of $\bar{Y}^1_{0,T}$ should also be a good model of $\bar{Y}^0_{0,t}^0$ for $t < T$. We can also assume that when comparing two models, one with uniformly better covariate balance than the other, the model with uniformly better covariate balance should have lower bias both for $\bar{Y}_{0,T}^1$ and $\bar{Y}_{0,t}^0$ if our model assumptions are correct. We justify such comparisons by assuming the outcome model is linear in $X$ for all time periods $t$. Of course, this is a stronger assumption than necessary to estimate the ETC, which only requires that $\mu_{1,T}$ is linear in $X$. 

Specifically, we rerun our procedures on pre-treatment data to compare the performance of our models for a fixed $\delta$. In particular, we train our model on 2009-2011 data to predict 2012 outcomes, and 2010-2012 data to predict 2013 outcomes. We limit to one-year prediction error since our estimand is only one-year forward. We then examine the performance of H-SBW against SBW, which vary with respect to the tuning parameter $\rho$, the bias-corrected versions, and the covariate adjustment procedure used in the balancing constraints. 

We expect that the estimators trained on the adjusted data should perform better than the estimators trained on the unadjusted data. If our outcome and measurement error models are correct, these estimators should achieve better balance across the true covariates and therefore have lower bias than the estimators estimated on the unadjusted data. In theory, if our outcome model is correct, either of our adjusted covariate sets should have similar bias; however, if our model of the measurement error heterogeneity is good, we should also expect the heterogeneous adjustment to reduce the variance of the estimates. We use the performance on this data to choose which adjustment we prefer for our final results. Conditional on the adjustment, we assume that if the bias-corrected estimators all obtain uniformly better covariate balance than the uncorrected estimators, these estimators should also perform better than the uncorrected estimators. However, if the assumed outcome models are incorrect, these estimators may suffer from extrapolation bias and perform worse despite achieving better balance. Finally, we expect SBW to have similar performance to H-SBW, though as noted above H-SBW may have increased bias at the cost of reduced variability. 

\subsection{Inference}

We use the leave-one-state-out jackknife to estimate the variance of $\hat{Y}_0^1$ (see, e.g., \cite{cameron2015practitioner}). Specifically, we take the pool of expansion states and generate a list of datasets that exclude each state. For each dataset in this list we calculate the weights and the leave-one-state-out estimate $\bar{Y}^1_{0, -m}$. Throughout all iterations we hold our targeted mean fixed at $\bar{W}_0$.\footnote{That is, we treat $\bar{W}_0$ as identical to $\bar{X}_0$, ignoring the variability in the estimate. Note that the variability in this estimate is of smaller order than the variability in $\hat{Y}_0^1$, since the former does not depend on the number of states but instead the number of CPUMAs (and the sample size used to estimate each CPUMA-level covariate). Moreover, we treat $\bar{X}_0$ as fixed because our estimand is conditional on the observed covariate distribution of the non-expansion states.} When generating these estimates, if our preferred initial choice of $\delta$ does not converge, we gradually reduce the constraints until we can obtain a solution. We compute this variance estimate in two ways: first, we condition on our covariate adjustment $\hat{\eta}_1$. This is our preferred estimator; however, it does not account for the randomness in $\hat{\eta}_1$. We conduct a second procedure where for each dataset we also re-estimate $\hat{\eta}_1$ before estimating the weights. These results are available in Appendix E. We then calculate the variance:

\begin{equation}
    \hat{Var}(\hat{Y}_0^1) = \frac{M - 1}{M}\sum_{m = 1}^M(\hat{Y}^1_{0,-m} - \bar{Y}^1_{0, m})^2
\end{equation}

where $\bar{Y}_{0, m} = \frac{1}{M}\sum_{i=1}^M\hat{Y}^1_{0, -m}$. In other settings the jackknife has been shown to be a conservative approximation of the bootstrap (see, e.g., \cite{efron1981jackknife}). In a simulation study mirroring our setting (available in Appendix G), we also find that this procedure is conservative.

To obtain an estimate the variance of $\bar{Y}_0^0$ we run an auxillary regression model on the non-expansion states and use the CR-2 standard error adjustment to estimate the variance of the linear combination $\bar{W}_0^T\hat{\beta}_0$. Notice that we have no need to adjust the non-expansion state data to estimate this quantity: a linear regression line always contains the point $(\bar{W}_0, \bar{J}_0)$, which are unbiased estimates of $(\bar{X}_0, \bar{Y}_0)$. Therefore, $\mathbb{E}_W\{\bar{W}_0^T\hat{\beta}_0 \mid X\} = \bar{Y}_0^0$. Our final variance estimate $\hat{Var}(\hat{\psi})$ is the sum of $\hat{Var}(\hat{Y}_0^1)$ and $\hat{Var}(\hat{Y}_0^0)$ (though the latter is in general much smaller than the former). We use standard normal quantiles to generate confidence intervals. 

\section{Results}

We first present summary statistics regarding the variability of six time-varying covariates on our adjusted and unadjusted datasets. The second sub-section contains covariate balance diagnostics. The final sub-section contains our ETC estimates.

\subsection{Covariate adjustment}

Table~\ref{tab:adjust1} displays the effects of our covariate adjustment procedure on the variance of our pre-treatment outcomes and pre-treatment unemployment rates among the expansion states. We most heavily prioritize balancing these covariates, but they are also among the least precisely estimated (all of our other covariates average over multiple years of data). Table~\ref{tab:adjust1} displays the variance of each covariate on the unadjusted and adjusted datasets. We see that both the homogeneous and heterogeneous adjustment procedures reduce the variability in the data by comparable amounts (see Section~\ref{ssec:methodsmsrment} for definitions of these adjustments). Intuitively, these adjustment reduce the likelihood that our balancing weights will over-fit to noise in the data. These results are consistent across most of our other covariates.

\begin{table}[ht]
\caption{Sample variance on unadjusted and adjusted datasets, expansion states}
\label{tab:adjust1}
\begin{tabular}{lrrr}
  \hline
Variable & No adjustment & Heterogeneous & Homogeneous \\ 
  \hline
Uninsured Pct 2011 & 8.35 & 8.04 & 8.05 \\ 
  Uninsured Pct 2012 & 8.20 & 7.89 & 7.90 \\ 
  Uninsured Pct 2013 & 8.09 & 7.78 & 7.79 \\ 
  Unemployed Pct 2011 & 3.66 & 3.25 & 3.27 \\ 
  Unemployed Pct 2012 & 3.72 & 3.38 & 3.38 \\ 
  Unemployed Pct 2013 & 3.20 & 2.88 & 2.87 \\ 
   \hline
\end{tabular}
\end{table}

\subsection{Covariate balance}

Figure~\ref{fig:loveplotc1} displays the reduction of imbalances using our H-SBW weights. This plot only displays covariates with greater than one percentage point difference between the targeted mean in the expansion region and the mean values in the non-expansion region prior to weighting, and the reweighted treatment values using the homogeneous covariate adjustment $\hat{\eta}_1(W_{sc})$. Before applying our weights, we see that there are substantial imbalances in the Republican governance indicators, as well as pre-treatment uninsurance and unemployment rates. Our weights reduce these differences; however, some remain, particularly among the Republican governance indicators. A complete balance table is available in Appendix D, Table 4. 

\begin{figure}[H]
\begin{center}
    \caption{Balance plot, primary dataset}
    \label{fig:loveplotc1}
    \includegraphics[scale=0.5]{01_Plots/balance-plot-etuc1.png}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
    \caption{H-SBW versus BC-HSBW versus SBW, weights summed by state, primary dataset}
    \label{fig:sbwvhsbw1}
    \includegraphics[scale=0.6]{01_Plots/weights-by-state-sbw-hsbw-c1.png}
\end{center}
\end{figure}

We then use ridge-regression augmentation to extrapolate from the data in order to reduce all imbalances within 0.5 percentage points. Figure~\ref{fig:sbwvhsbw1} shows the total weights summed across states for three estimators: H-SBW, BC-HSBW, and SBW. For BC-HSBW we display the negative weights separately from the positive weights to show the extent of the extrapolation. This figure illustrates two key points: first, that H-SBW more evenly disperses the weights across states relative to SBW; second, that BC-HSBW extrapolates somewhat heavily in order to achieve the desired level of balance, particularly for CPUMAs in California, This is likely in part because California has the most CPUMAs of any state in the dataset.

Finally, we examine whether the H-SBW weights generated using the unadjusted data balance the adjusted covariates. While this metric does not reflect the ``true'' imbalances, the comparison gives some indication of whether the unadjusted weights are overfitting to noisy covariate measurements. Table~\ref{tab:balcomp} compares the imbalances among our pre-treatment outcomes using H-SBW weights generated on our unadjusted dataset applied to the adjusted (homogeneous) dataset. The ``Unweighted Difference'' column represents the raw difference in means, while the ``Weighted Diff'' column reflects the weighted difference that we calculate on the unadjusted dataset. The ``Homogeneous Diff'' column displays the weighted imbalance when applying the H-SBW weights to the dataset using the homogeneous adjustment, and likewise for ``Heterogeneous Diff.'' The weighted pre-treatment outcomes are approximately one percentage point lower than we desired in the two years prior to treatment using the heterogeneous adjustment, and -0.2 percentage points lower (on average) using the homogeneous adjustment. Compared with the weighted difference (no adjustment) this is suggestive that the unadjusted weights are overfitting to noisy covariates and may give an overly optimistic view of balance obtained. Given the high degree of expected correlation between pre-treatment and post-treatment outcomes, we may expect the estimator of $\bar{Y}^1_0$ trained on the unadjusted data to have a downward bias.

\begin{table}[ht]
\caption{Balance comparison: weights estimated on unadjusted data applied to adjusted data}
\label{tab:balcomp}
\begin{tabular}{lrrrr}
  \hline
Variables & Unweighted Diff & Weighted Diff (none) & Homogeneous Diff & Heterogeneous Diff\\ 
  \hline
Uninsured Pct 2011 & -3.09 & -0.05 & -0.11 & 0.92 \\ 
  Uninsured Pct 2012 & -2.99 & -0.05 & -0.21 & -1.06 \\ 
  Uninsured Pct 2013 & -3.00 & -0.05 & -0.38 & -0.93 \\
   \hline
\end{tabular}
\end{table}

\subsubsection{Model validation}

We compare the performance of our models by repeating the covariate adjustments and calculating our procedure on 2009-2011 ACS data to predict 2012 outcomes, and similarly for 2010-2012 data to predict 2013 outcomes for the treated states. Table~\ref{tab:pretxpred} displays these results, with the rows ordered by RMSE of the prediction errors. Table~\ref{tab:pretxpred} shows that the estimators trained on the covariate adjusted data have substantially better performance than the unadjusted data. Moreover, the estimators trained on the homogeneous adjustment seem to do slightly better than the ones that model the heterogeneity; we present our results using the homogeneous adjustment. In these earlier years we find that SBW tends to have slightly lower RMSE than H-SBW. However, the results are quite similar, as we expected. Finally, we see that the bias corrected estimators tend to perform worse in this application. This may indicate that the extrapolation bias outweighs the cost of reducing the covariate imbalances. While this does not imply that this model will perform badly when predicting $\bar{Y}^1_{0, T}$, it does suggest caution regarding these results. We see this as a function of our models being approximations: we expect in general that assuming the linear outcome models approximately holds on the support of the data where we have sufficient covariate overlap; however, these models may lead us astray when our weights extrapolate excessively from the data. The worst performing estimators are the bias-corrected estimators trained on the unadjusted data.

\begin{table}[ht]
\caption{Estimator pre-treatment outcome prediction error}
\label{tab:pretxpred}
\begin{tabular}{llrrr}
  \hline
Sigma estimate & Estimator & 2012 error & 2013 error & RMSE \\ 
  \hline
Homogeneous & SBW & -0.18 & -0.22 & 0.20 \\ 
  Homogeneous & H-SBW & -0.24 & -0.21 & 0.23 \\ 
  Heterogeneous & SBW & -0.25 & -0.30 & 0.27 \\ 
  Heterogeneous & H-SBW & -0.32 & -0.39 & 0.36 \\ 
  Homogeneous & BC-SBW & -0.42 & -0.35 & 0.39 \\ 
  Heterogeneous & BC-SBW & -0.45 & -0.39 & 0.42 \\ 
  None & SBW & -0.50 & -0.61 & 0.56 \\ 
  None & H-SBW & -0.52 & -0.61 & 0.57 \\ 
  Homogeneous & BC-HSBW & -0.53 & -0.62 & 0.58 \\ 
  Heterogeneous & BC-HSBW & -0.53 & -0.72 & 0.63 \\ 
  None & BC-SBW & -0.82 & -0.93 & 0.88 \\ 
  None & BC-HSBW & -0.93 & -0.99 & 0.96 \\ 
   \hline
\end{tabular}
\end{table}

We find a consistent negative bias across all of our estimators: all of our models tend to under-predict the true uninsurance rate among the non-expansion states the subsequent year by between a fifth to a whole percentage point. Assuming that the sign of this bias will also affect our estimates of $\bar{Y}^1_0$, we should expect our treatment effect estimates to have slight downward bias. That is, the true treatment effect may be smaller (closer to zero) in absolute magnitude than the estimated treatment effect. 

This bias is not unexpected: we can think of the uninsurance rate in expansion and non-expansion regions as being drawn from separate distributions with means $(\mu_{1t}, \mu_{0t})$ where $\mu_{1t} < \mu_{0t}$. Our measurement error model for the pre-treatment outcomes ($J_{sct} = Y_{sct} + \epsilon_{sct}$) suggests that the mean residuals of the reweighted expansion regions should be positive. The persistent negative bias is therefore a form of regression-to-the-mean that occurs during the post-treatment period, when the $\epsilon_{scT}$ gets closer to zero. This type of phenomenon has also been discussed assuming linear-factor models in the difference-in-differences and synthetic controls literatures (see, e.g., \cite{daw2018matching}). 

\subsection{Primary Results}

Using H-SBW we estimate an effect of -2.33 (-3.49, -1.16) percentage points. The SBW results are almost identical with -2.35 (-3.65, -1.06) percentage points. Compared to the unadjusted data we see very similar estimates at -2.34 (-2.85, -1.82) percentage points for H-SBW and -2.39 (-2.95, -1.83) percentage points for SBW. We see that H-SBW reduces the confidence intervals relative to SBW. We also observe that using the adjusted covariate set increases the width of the estimated confidence intervals. This increase in variability is expected because the adjustment procedure generally reduces the variability in the data, as we saw in Table~\ref{tab:adjust1}, thereby requiring that the balancing weights also increase in variability to achieve the desired level of balance. Importantly, this variance estimate conditions on the covariate adjustment, and does not take into account the randomness in this procedure, and may understate the true uncertainty. When we recalculate the entire adjustment procedure, we find that the confidence intervals are of a comparable magnitude. The results are available in Appendix E.

When we add the bias-correction, the absolute magnitude of the point estimate decreases: we estimate -2.05 (-3.32, -0.79) percentage points for BC-HSBW and -2.00 (-2.98, -1.01) percentage points for BC-SBW. Interestingly, in contrast to our validation tests, where the bias-corrected estimators tended to predict lower uninsurance rates than the other estimators, here the bias-correction predicts higher uninsurance rates for $\bar{Y}^1_0$. We also see that in contrast to the H-SBW and SBW estimators, the confidence interval for BC-SBW is narrower than for BC-HSBW. Table~\ref{tab:mainresults} presents all of our estimates, along with the results when excluding early expansion states. All adjusted estimates were closer to zero than the unadjusted estimates, though the point estimates from the SBW and H-SBW were estimators were virtually identical. We briefly note that the heterogeneous adjustments were all closer to zero than the unadjusted estimates. Complete results are available in Appendix E.

\begin{table}[ht]
\label{tab:mainresults}
\caption{Primary results}
\begin{tabular}{llll}
  \hline
Weight type & Adjustment & Estimate (95\% CI) & Early excluded estimate (95\% CI) \\ 
  \hline
H-SBW & Homogeneous & -2.33 (-3.49, -1.16) & -2.09 (-2.85, -1.33) \\ 
  H-SBW & None & -2.34 (-2.85, -1.82) & -2.28 (-2.82, -1.74) \\ 
  BC-HSBW & Homogeneous & -2.05 (-3.27, -0.82) & -1.94 (-2.96, -0.92) \\ 
  BC-HSBW & None & -2.22 (-2.87, -1.56) & -2.22 (-3.07, -1.38) \\ 
  SBW & Homogeneous & -2.35 (-3.65, -1.06) & -2.05 (-2.75, -1.35) \\ 
  SBW & None & -2.39 (-2.95, -1.83) & -2.21 (-2.71, -1.72) \\ 
  BC-SBW & Homogeneous & -2.07 (-3.17, -0.97) & -1.99 (-3.00, -0.99) \\ 
  BC-SBW & None & -2.19 (-2.90, -1.49) & -2.23 (-3.05, -1.40) \\ 
   \hline
\end{tabular}
\end{table}

We also consider the sensitivity of our analysis with respect to no anticipatory treatment effects. We exclude California, Connecticut, Minnesota, New Jersey, and Washington, which had partial expansions prior to 2014, and rerun our analyses. Figure 2 in Appendix D displays the H-SBW weights summed by state alongside BC-HSBW, which extrapolates to reduce the imbalances, and the results are available in Table~\ref{tab:mainresults}.

Our point estimates are similar to our primary analysis, though the numbers move slightly closer to zero. We also see that the differential between the estimates on the adjusted and unadjusted data is slightly larger: -2.28 (-2.82, -1.74) percentage points for H-SBW on the unadjusted dataset and -2.09 (-2.85, -1.33) on the adjusted data. We again find that when we add the bias-correction the point estimates again move closer to zero. Overall our primary results are relatively robust to the exclusion of these states. We conclude that potential violations of this causal assumption are not a large factor.

Lastly, we examine the robustness of our point estimates to the removal of individual states (these are the same point estimates used to calculate our confidence intervals). We find that removing Ohio tends to move the point estimates farther from zero, while removing North Dakota, Kentucky, or California tends to move the estimates closer to zero. Appendix E Figure 3 displays a heatmap showing how the estimates change for each estimator when removing each state.

\section{Discussion}

We estimate that had states that did not expand Medicaid in 2014 instead expanded their programs, they would have seen a -2.33 (-3.49, -1.16) percentage point change in the adult uninsurance rate. Existing estimates place the ETT between -3 and -6 percentage points. These estimates vary depending on the targeted sub-population of interest, the data used, the level of modeling (individuals or regions), and the modeling approach (see, e.g., \cite{courtemanche2017early}, \cite{kaestner2017effects}, \cite{frean2017premium}). Our estimate of the ETC are closer to zero than these ETT estimates. This difference may be a function of these different modeling strategies, or it may suggest that the ETC is smaller in absolute magnitude than the ETC. Regardless, due to the potential for effect heterogeneity, we emphasize the importance of directly estimating the targeted counterfactual of interest (e.g. the ETT or ETC), and being explicit about the assumptions used to estimate these quantities. We now consider our methodological contributions, study limitations, and we conclude by considering the policy implications of these findings.

\subsection{Methodological considerations}

We provide several methodological contributions to the literature on synthetic controls and balancing weights. First, we clarify some of the assumptions required to use synthetic controls to estimate the ETC. The challenge is that we need to predict treatment response rather than the outcome absent treatment. We emphasize that there may exist covariates that are weak confounders of the outcome absent treatment, but are strong confounders of the outcome under treatment.\footnote{In Appendix F we consider the role of Republican governance as an example, noting that \cite{kaestner2017effects} and \cite{courtemanche2017early} do not control for these factors when estimating their synthetic control weights. We show that these factors do substantially influence our estimates of $\bar{Y}_0^1$, though this does not rule out that these factors may also matter for predicting $\bar{Y}_1^0$.} While perhaps obvious, this point is not fully appreciated in the applied literature. For example, \cite{born2020lockdowns} recently used synthetic controls to estimate Sweden's COVID cases and deaths had they instituted a lockdown. The authors balance on pre-treatment infections, urbanization rate, and population size, and argue that the treatment is effectively random conditional on these covariates. While this may be true, the authors use the standard synthetic controls algorithm to generate their weights, using pre-treatment outcomes to determine the best model of the post-treatment counterfactual of $\bar{Y}^1_0$. For this application we forego this type of procedure and instead use our prior knowledge about the problem to prioritize covariate balance.

Second, our estimation procedure introduces and illustrates the H-SBW objective, which can improve upon the SBW objective when using hierarchical data. Assuming the errors in the outcome model follow the covariance structure posited by \cite{kloek1981ols}, H-SBW produces a lower variance estimator by more evenly dispersing weights across states. The assumption underlying the particular structure of our objective is that our model errors have constant variance and constant within-state correlation $\rho$. However, our procedure requires assuming the covariance structure and $\rho$ in advance. We choose $\rho = 1/6$ for this application. Identifying a data-driven approach to choose this tuning parameter (or perhaps for the covariance structure in general) could be a useful future contribution. 

Third, our estimation procedure accounts for measurement error in our covariates. We modify the constraint set to balance on a linear approximation to the true covariate values by adapting regression-calibration techniques (\cite{gleser1992importance}) to the balancing weights context. In Table~\ref{tab:balcomp} we show that the weights calculated on the unadjusted dataset fail to achieve the desired level of covariate balance on the adjusted dataset. We find that the weighted imbalances in our pre-treatment outcomes may be larger than we wanted, which we speculate for this application might bias our treatment effect downward. When we compared our estimates using the adjusted covariates to the unadjusted covariates, we find that our point estimates decrease (although often only slightly) in absolute magnitude. Essentially, when we generate weights on the unadjusted data to estimate the 2014 counterfactual outcome, they are likely fitting to noise, causing the observed level of balance to appear better than it truly is. Meanwhile, the re-weighted region may suffer from regression to the mean in the post-treatment period, making our treatment effect estimates appear larger in absolute magnitude than the truth. Once we adjust for the measurement error, our point estimates decrease in absolute magnitude (see also \cite{daw2018matching}, who discuss this phenomenon in more detail in the context of difference-in-differences designs). Overall, our study provides a roadmap for future studies that may wish to correct for potential measurement error while using balancing weights. 

One direction for further work is to calibrate this procedure to determine an optimal bias-variance tradeoff with respect to the measurement error. The procedure we implemented here was likely sub-optimal with respect to the mean-square error of our estimator. In particular, the bias induced by the measurement error decreases with square root of the sample size used to calculate each CPUMA's covariate values, the minimum of which were over three hundred. Meanwhile, the variance of our counterfactual estimate should decrease with the square root of the number of treated states (of which there are 21). From a theoretical perspective, the variance is of a larger order than the bias; moreover, adjusting for the bias will further increase the variance of the estimator. These concerns are consistent with our observed results: the change in our point estimates from the unadjusted data to the adjusted data is of a smaller magnitude than our variance estimate on our point estimate on the unadjusted data. Moreover, once we adjust for the measurement error, our confidence intervals increase more widely than the point estimates change.

\subsection{Limitations}

We caution that we required strong modeling assumptions throughout this analysis. In particular, we require SUTVA, no anticipatory treatment effects, no unmeasured confounding conditional on the true covariates, and several parametric assumptions about both the outcome and measurement error models. We were able to address some concerns about possible violations of these assumptions. For example, our results were qualitatively similar whether we excluded possible ``early expansion states,'' or used different weighting strategies (including relaxing the positivity restrictions and changing the tuning parameter $\rho$). We also examined two versions of our covariate adjustment and found similar results with either. However, we do not attempt to address concerns about SUTVA violations, particularly the impact of spillovers across regions. And while we believe that no unmeasured confounding is reasonable for this problem, we did not conduct a sensitivity analysis (see, e.g., \cite{bonvini2021sensitivity}) with respect to this assumption. Finally, our approach to correcting for measurement error relies on access to auxillary information that may not be available for many applications.

\subsection{Policy considerations}

We find that our point estimates for the ETC are somewhat smaller in absolute magnitude than existing estimates of the ETT. While we make no formal statistical claims about these differences, this finding nevertheless highlights the importance of caution when using estimates of the ETT to make inferences about the ETC. Because almost every outcome of interest is mediated through increasing the number of insured individuals, if the ETC is in fact different than the ETT, then projecting findings from an estimate of the ETT to the ETC may lead to inaccurate inference. For example, \cite{miller2019medicaid} study the effect of Medicaid expansion on mortality. Using their estimate of the ETT they project that had all states expanded Medicaid, 15,600 deaths would have been avoided during their study's time-period. If we believe that this number increases monotonically with the number of uninsured individuals, this estimate may be an overestimate if the ETC is less than the ETT, or an underestimate if the ETC is greater than the ETT. Directly estimating the ETC can help us better model policy relevant downstream effects mediated through decreasing the uninsurance rate. 

Medicaid expansion is also still an ongoing policy debate in the United States. Following the passage of the American Rescue Plan, state legislatures in Wyoming, Alabama, and North Carolina are reportedly considering expanding their programs. Our study estimates the effect of Medicaid expansion on adult uninsurance rates; however, this effect is only interesting because Medicaid enrollment is not automatic for eligible individuals. Different state policies may therefore make it easier or harder to enroll in Medicaid. We emphasize that if the goal of Medicaid expansion is to increase insurance access for low-income adults, state policy-makers also may wish to make it easier to enroll in Medicaid. 

\section{Conclusion}

This is the first study we are aware of that directly estimates the foregone coverage expansions of Medicaid expansion on states that did not expand Medicaid in 2014. Our estimation approach contributes to the methodological literature on synthetic controls by outlining a set of identifying assumptions to estimate the ETC rather than the ETT, and to the balancing weights literature by using an estimation procedure that account for hierarchical data structure and measurement error in the covariates. We estimate that had states that did not expand their Medicaid eligibility requirements in 2014 done so, they would have seen a -2.33 (-3.49, -1.16) percentage point change in their uninsurance rate. This point estimate is closer to zero than existing estimates of the ETT, which range between -3 and -6 percentage points (\cite{frean2017premium}).\footnote{As we noted above, prior studies differ with respect to the data used, the targeted population of interest, the modeling choices, and unit level of analysis.} From a policy-analysis perspective, we caution against using using existing estimates of the ETT to make inferences about the ETC. From a policy-making standpoint, we note that if the goal of Medicaid expansion is to increase access to insurance for low-income adults, state and federal policy-makers may wish to consider policies that make Medicaid enrollment easier if not automatic.

\section*{Acknowledgements}

The authors gratefully acknowledge invaluable advice and comments from Zachary Branson, Dave Choi, Edward Kennedy, Brian Kovak, Akshaya Jha, Lowell Taylor, and Jose Zubizaretta.

\begin{supplement}
Analysis programs and supporting materials are available online at github.com /mrubinst757/medicaid-expansion. Proofs and additional results are available in the Appendix.
\end{supplement}

\bibliographystyle{imsart-nameyear} % Style BST file
\bibliography{research.bib}       % Bibliography file (usually '*.bib')

\clearpage

\appendix
\input{Text_files/proof}

\input{Text_files/adjustment-details}

\input{Text_files/summary-tabs}

\input{Text_files/balance-tables}

\input{Text_files/results-tabs}

\section{Appendix}


\input{Text_files/variable-selection}

\end{document}
