We estimate that had states that did not expand Medicaid in 2014 instead expanded their programs, they would have seen a -2.33 (-3.49, -1.16) percentage point change in the adult uninsurance rate. Existing estimates place the ETT between -3 and -6 percentage points. These estimates vary depending on the targeted sub-population of interest, the data used, the level of modeling (individuals or regions), and the modeling approach (see, e.g., \cite{courtemanche2017early}, \cite{kaestner2017effects}, \cite{frean2017premium}). We find that our estimate of the ETC is closer to zero than these ETT estimates. This difference may be a function of these different modeling strategies, or it may suggest that the ETC is smaller in absolute magnitude than the ETC. Regardless, due to the potential for effect heterogeneity, we emphasize the importance of directly estimating the targeted counterfactual of interest (e.g. the ETT or ETC), and being explicit about the assumptions used to estimate these quantities. We now consider our methodological contributions, study limitations, and we conclude by considering the policy implications of these findings.

\subsection{Methodological considerations}

Our study makes several methodological contributions to the literature on synthetic controls and balancing weights. First, we clarify some of the assumptions required to extend the synthetic controls literature to estimate the treatment effect on the controls. The key challenge is that we need to predict treatment response rather than the outcome absent treatment. We argue that we cannot use pre-treatment outcomes to conduct variable selection or optimally determine relative covariate importance without strong assumptions about the relationship between the counterfactual outcome models. In brief, estimating the ETC is an arguably more difficult problem because it requires a priori understanding of which covariates likely predict treatment response. We emphasize there may exist covariates that are not strong confounders of the outcome absent treatment, but that may be important confounders of the outcome under treatment. Using pre-treatment outcomes to conduct variable selection or determine relative variable importance can result in biased treatment effect estimates. As an example, we consider the role of Republican governance in our application: \cite{kaestner2017effects} and \cite{courtemanche2017early} do not balance on these factors when generating their synthetic control weights in their estimates of the effect of Medicaid expansion on uninsurance rates among treated states. By contrast we show that failing to control for this factor in our models leads to substantially larger treatment effect estimates, indicating their confounding role in our counterfactual model.\footnote{We caution that in actuality these covariates may also be confounders of the outcome absent treatment; we do not directly investigate this.} While perhaps obvious, these points do not seem to have yet been appreciated in the applied literature. For example, \cite{born2020lockdowns} recently used synthetic controls to estimate Sweden's COVID cases and deaths had they instituted a lockdown. The authors balance on pre-treatment infections, urbanization rate, and population size. Yet they do not explicitly argue why these covariates are the most relevant determinants of the potential outcome model under treatment, or what assumptions they are relying on such that this procedure should produce a good counterfactual estimate.

Second, our estimation procedure introduces and illustrates the H-SBW objective, which can improve upon the SBW objective when using hierarchical data. Assuming the errors in the outcome model follow the covariance structure posited by \cite{kloek1981ols}, H-SBW produces a lower variance estimator by more evenly dispersing weights across states. The assumption underlying the particular structure of our objective is that our model errors have constant variance and constant within-state correlation $\rho$. However, our procedure requires assuming the covariance structure and $\rho$ in advance. We choose $\rho = 0.2$ for this application; however, it would be interesting to identify a data-driven approach to choose this tuning parameter (or perhaps for the covariance structure in general). 

Third, our estimation procedure accounts for measurement error in our covariates. We modify the constraint set to balance on a linear approximation to the true covariate values by adapting regression-calibration techniques (\cite{gleser1992importance}) to the balancing weights context. In Table~\ref{tab:balcomp} we show that the weights calculated on the unadjusted dataset fail to achieve the desired level of covariate balance on the adjusted dataset. Specifically, we find that the weighted pre-treatment outcomes may be lower than we wanted, which we speculate may bias our treatment effect downward. When we compared our estimates using the adjusted covariates to the unadjusted covariates, we find that our point estimates decrease (although often only slightly) in absolute magnitude. Essentially, when we generate weights on the unadjusted data to estimate the 2014 counterfactual outcome, they are likely fitting to noise. This causes the observed level of balance to appear better than it truly is. Meanwhile, the re-weighted region may suffer from regression to the mean in the post-treatment period, making our treatment effect estimates appear larger in absolute magnitude than the truth. Once we adjust for the measurement error, our point estimates decrease in absolute magnitude (see also \cite{daw2018matching}, who discuss this phenomenon in more detail in the context of difference-in-differences designs). Overall, our study provides a roadmap for future studies that may wish to correct for potential measurement error while using balancing weights. 

One direction for further work is to calibrate this procedure to determine an optimal bias-variance tradeoff with respect to the measurement error. It is possible that the procedure we implemented was sub-optimal with respect to the mean-square error of our estimator. In particular, the bias induced by the measurement error decreases with square root of the sample size used to calculate each CPUMAs covariate values, the minimum of which were over three hundred. Meanwhile, the variance of our counterfactual estimate should decrease with the square root of the number of treated states (of which there are 21). From a theoretical perspective, the variance is of a larger order than the bias; moreover, adjusting for the bias will further increase the variance of the estimator. These concerns are consistent with our observed results: we find that the change in our point estimates from the unadjusted data to the adjusted data are of smaller absolute magnitude than our variance estimate on our point estimate on the unadjusted data. Moreover, once we adjust for the measurement error, our confidence intervals increase more widely than the point estimates change.

\subsection{Limitations}

Our study is not without methodological limitations. We first caution that we required strong modeling assumptions throughout. In particular, we require SUTVA, no anticipatory treatment effects, no unmeasured confounding conditional on the true covariates, and several parametric assumptions about both the outcome and measurement error models. We were able to address some concerns about possible violations of these assumptions. For example, our results were qualitatively similar whether we excluded possible ``early expansion states,'' or used different weighting strategies (including relaxing the positivity restrictions and changing the tuning parameter $\rho$). We also examined two versions of our covariate adjustment and found similar results with either.  However, we do not attempt to address concerns about SUTVA violations, particularly the impact of spillovers across regions. And while we believe that no unmeasured confounding is reasonable for this problem, we did not conduct any sensitivity analyses with respect to this assumption.

A second limitation pertains to interpreting our results against the existing literature. As we noted above, prior studies differ with respect to the data used, the targeted population of interest, the modeling choices, and unit level of analysis. To attempt to make our study comparable with existing work, we follow the covariates, study period, and data used most closely by \cite{courtemanche2017early}, who calculate an ETT estimate of -3.1 percentage points. However, we note two key differences between our studies. First, \cite{courtemanche2017early} modeled individual-level data, while we model CPUMA-level aggregates. Second, we exclude several states from our expansion pool while \cite{courtemanche2017early} include all states and DC in their analysis. As a result we do not make any formal statistical claims about the differences between our estimates, or between our estimates and any other specific paper, as they could also be a function of any of these other differences in our study. Relatedly, we caution against committing the ``ecological fallacy:'' specifically, we cannot directly infer individual-level behavior from the ecological correlations in our study without much stronger assumptions (see, e.g., \cite{subramanian2009revisiting}).

\subsection{Policy considerations}

We find that our point estimates for the ETC are  somewhat smaller in absolute magnitude than existing estimates of the ETT. While we make no formal statistical claims about these differences, this finding nevertheless highlights the importance of caution when using estimates of the ETT to make inferences about the ETC. Because almost every outcome of interest is mediated through increasing the number of insured individuals, if the ETC is in fact different than the ETT, then projecting findings from an estimate of the ETT to the ETC may lead to inaccurate inference. For example, \cite{miller2019medicaid} study the effect of Medicaid expansion on mortality. Using their estimate of the ETT they project that had all states expanded Medicaid, 15,600 deaths would have been avoided during their study's time-period. If we believe that this number increases monotonically with the number of uninsured individuals, this estimate may be an overestimate if the ETC is less than the ETT, or an underestimate if the ETC is greater than the ETT. Directly estimating the ETC can therefore also help us better model policy relevant downstream effects mediated through decreasing the uninsurance rate. 

Medicaid expansion is still an ongoing policy debate in the United States. Following the passage of the American Rescue Plan, state legislatures in Wyoming, Alabama, and North Carolina are reportedly considering expanding their programs. Our study estimates the effect of Medicaid expansion on adult uninsurance rates; however, this effect is only interesting because Medicaid enrollment is not automatic for eligible individuals. Different state policies may therefore make it easier or harder to enroll in Medicaid. We again emphasize that if the goal of Medicaid expansion is to increase insurance access for low-income adults, state policy-makers also make wish to make Medicaid enrollment easier. 

\section{Conclusion}

This is the first study we are aware of that directly estimates the foregone coverage expansions of Medicaid expansion on states that did not expand Medicaid in 2014. Our estimation approach contributes to the methodological literature on synthetic controls by outlining a set of identifying assumptions to estimate the ETC rather than the ETT, and to the balancing weights literature by using an estimation procedure that account for hierarchical data structure and covariates measured with error. We estimate that had states that did not expand their Medicaid eligibility requirements in 2014 done so, they would have seen a -2.33 (-3.49, -1.16) percentage point change in their uninsurance rate. This point estimate is closer to zero than existing estimates of the ETT, which range between -3 and -6 percentage points (\cite{frean2017premium}). From a practical standpoint, we caution against using using existing estimates of the ETT to make inferences about the ETC. From a policy standpoint, if the goal of Medicaid expansion is to increase access to insurance for low-income adults, state and federal policy-makers may wish to consider policies that make Medicaid enrollment easier if not automatic in addition to eligibility changes.