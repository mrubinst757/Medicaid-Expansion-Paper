We estimate that had states that did not expand Medicaid in 2014 instead expanded their programs, they would have seen a -2.33 (-3.49, -1.16) percentage point change in the adult uninsurance rate. Existing estimates place the ETT between -3 and -6 percentage points. These estimates vary depending on the targeted sub-population of interest, the data used, the level of modeling (individuals or regions), and the modeling approach (see, e.g., \cite{courtemanche2017early}, \cite{kaestner2017effects}, \cite{frean2017premium}). We find that our estimate of the ETC is closer to zero than these ETT estimates. This difference may be a function of these different modeling strategies, or it may suggest that the ETC is smaller in absolute magnitude than the ETC. Our results do highlight the importance of controlling for Republican governance when generating our counterfactual estimate, which may drive our treatment effect estimate closer towards zero relative to the ETT; however, our investigation into treatment effect heterogeneity is ultimately statistically inconclusive. Regardless, due to the potential for effect heterogeneity, we emphasize the importance of directly estimating the targeted counterfactual of interest, and being explicit about the assumptions used to estimate these quantities. We now consider our methodological contributions, study limitations, and we conclude by considering the policy implications of these findings.

\subsection{Methodological considerations}

Our study makes several methodological contributions to the literature on synthetic controls and balancing weights. First, we clarify some of the assumptions required to extend the synthetic controls literature to estimate the treatment effect on the controls. The key challenge is that we need to predict treatment response rather than the outcome absent treatment. We argue that we cannot use pre-treatment outcomes to conduct variable selection or optimally determine relative covariate importance without strong assumptions about the relationship between the counterfactual outcome models. In brief, estimating the ETC is an arguably more difficult problem because it requires a priori understanding of which covariates likely predict treatment response. We emphasize there may exist covariates that are not confounders of the outcome absent treatment, but that may be important confounders of the outcome under treatment, and that using pre-treatment outcomes to conduct variable selection risks missing these covariates. We use Republican governance as an example of this in our application, which \cite{kaestner2017effects} and \cite{courtemanche2017early} do not balance in their synthetic control weights in their studies of the effects of Medicaid expansion on uninsurance rates. By contrast we show that failing to control for this factor in our models leads to substantially larger treatment effect estimates, indicating their confounding role in our counterfactual model.\footnote{We caution that in actuality these covariates may also be confounders of the outcome absent treatment, and our failure to conclusively show effect heterogeneity with respect to this covariate supports this.} While perhaps obvious, these points do not seem to have yet been appreciated in the applied literature. For example, \cite{born2020lockdowns} recently used synthetic controls to estimate Sweden's COVID cases and deaths had they instituted a lockdown. The authors balance on pre-treatment infections, urbanization rate, and population size. Yet they do not explicitly argue why these covariates are the most relevant determinants of the potential outcome model under treatment, or what assumptions they are relying on that their model should be a good counterfactual model.

Second, our estimation procedure illustrates the H-SBW objective, which can improve upon the SBW objective when using hierarchical data. Assuming the errors in the outcome model follow the covariance structure posited by \cite{kloek1981ols}, H-SBW produces a lower variance estimator by more evenly dispersing weights across states. The assumption underlying the particular structure of our objective is that our model errors have constant variance and constant within-state correlation $\rho$. However, our procedure requires assuming the covariance structure and $\rho$ in advance. We choose $\rho = 0.2$ for this application; however, it would be interesting to come up with a data-driven approach to choose this tuning parameter (or perhaps for the covariance structure in general). 

Third, our estimation procedure accounts for measurement error in our covariates. We modify the constraint set to balance on a linear approximation to the true covariate values using regression-calibration techniques (\cite{gleser1992importance}). In Table~\ref{tab:balcomp} we show that the weights calculated on the unadjusted dataset fail to achieve the desired level of covariate balance on the adjusted dataset. Specifically, we find that the weighted pre-treatment outcomes may be lower than we wanted, which we speculate may bias our treatment effect downward. When we compared our estimates using the adjusted covariates to the unadjusted covariates, we find that our point estimates decrease (although often only slightly) in absolute magnitude. Essentially, when generate weights on the unadjusted data to estimate the 2014 counterfactual outcome, they are fitting to noise, causing the weighted region to suffer from regression to the mean in the post-treatment period. This can make our treatment effect estimates appear larger in absolute magnitude than they should be. Once we adjust for the measurement error, our point estimates decrease in absolute magnitude (see also \cite{daw2018matching}, who discuss this phenomenon in more detail in the context of difference-in-differences designs). Overall, our study provides a roadmap for future studies that may wish to correct for potential measurement error while using balancing weights. 

One caveat is that we did not attempt to calibrate our procedure to navigate an optimal bias-variance tradeoff with respect to the measurement error. It is possible that this procedure was sub-optimal with respect to the mean-square error of our estimator. In particular, the bias induced by the measurement error decreases with square root of the sample size used to calculate each CPUMAs covariate values, the minimum of which were over three hundred. Meanwhile, the variance of our counterfactual estimate should decrease with the square root of the number of treated states (of which there are 21). From a theoretical perspective, the variance is of a larger order than the bias; moreover, adjusting for the bias will further increase the variance of the estimator. These concerns are consistent with our observed results. We find that our variance estimates are an order of magnitude larger than the changes in our point estimates when we adjust our data, and that our variance estimates typically increase more than the point estimates changes once we account for the measurement error. 

\subsection{Limitations}

Our study is not without methodological limitations. We first caution that we required strong modeling assumptions throughout. In particular, we require SUTVA, no anticipatory treatment effects, no unmeasured confounding conditional on the true covariates, and several parametric assumptions about both the outcome and measurement error models. We were able to address some concerns about possible violations of these assumptions. For example, our results were qualitatively similar whether we excluded possible ``early expansion states,'' or used different weighting strategies (including relaxing the positivity restrictions and changing the tuning parameter $\rho$). We also examined two versions of our covariate adjustment and found similar results with either. Our results do not change qualitatively. However, we do not attempt to address concerns about SUTVA violations, particularly the impact of spillovers across regions. And while we believe that no unmeasured confounding is reasonable for this problem, we did conduct any sensitivity analyses with respect to this assumption.

A second limitation pertains to interpreting our results against the existing literature. As we noted above, prior studies are idiosyncratic with respect to the data used, the targeted population of interest, the modeling choices, and level of analysis. To attempt to make our study comparable with existing work, we follow the covariates, study period, and data used most closely by \cite{courtemanche2017early}, who calculate an ETT estimate of -3.1 percentage points. However, we note two key differences between our studies. First, \cite{courtemanche2017early} modeled individual-level data, while we model CPUMA-level aggregates. Second, we exclude several states from our expansion pool while \cite{courtemanche2017early} include all states and DC in their analysis. As a result we do not make any formal statistical claims about the differences between our estimates, or between our estimates and any other specific paper, as they could also be a function of any of these other differences in our study. Relatedly, we caution against committing the ``ecological fallacy:'' specifically, we cannot directly infer individual-level behavior from the ecological correlations in our study without much stronger assumptions (see, e.g., \cite{subramanian2009revisiting}).

\subsection{Policy considerations}

We find that our point estimates for the ETC are still somewhat smaller in absolute magnitude than existing estimates of the ETC. While we make no formal statistical claims about these differences, this finding nevertheless highlights the importance of caution when using estimates of the ETT to make inferences about the ETC. Because almost every outcome of interest is largely mediated through increasing the number of insured individuals, if the ETC is in fact different than the ETT, then projecting findings from an estimate of the ETT to the ETC may lead to inaccurate inference. For example, \cite{miller2019medicaid} study the effect of Medicaid expansion on mortality. Using their estimate of the ETT they project that had all states expanded Medicaid, 15,600 deaths would have been avoided during their study's time-period. If we believe that this number increases monotonically with the number of uninsured individuals, this estimate may be an overestimate if the ETC is less than the ETT, or an underestimate if the ETC is greater than the ETT. Directly estimating the ETC can therefore also help us better model interesting downstream effects mediated through decreasing the uninsurance rate. 

\section{Conclusion}

This is the first study we are aware of that directly estimates the foregone coverage expansions of Medicaid expansion on states that did not expand Medicaid in 2014. Our estimation approach contributes to the methodological literature on synthetic controls by clarifying some of the assumptions required to use longitudinal data to estimate the ETC rather than the ETT, and to the balancing weights literature by using an estimation procedure that account for hierarchical data structure and covariates measured with error. We estimate that had states that did not expand their Medicaid eligibility requirements in 2014 done so, they would have seen a -2.33 (-3.49, -1.16) percentage point change in their uninsurance rate. This point estimate is closer to zero than existing estimates of the ETT, which range between -3 and -6 percentage points (\cite{frean2017premium}). From a practical standpoint, we caution against using using existing estimates of the ETT to make inferences about the ETC. From a policy standpoint, if the goal of Medicaid expansion is to increase access to insurance for low-income adults, state and federal policy-makers may wish to consider policies that make Medicaid enrollment easier if not automatic.

