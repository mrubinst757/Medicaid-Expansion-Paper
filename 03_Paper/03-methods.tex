In this section we present our causal estimand, identifying assumptions, estimation strategy, and inferential procedure.

\subsection{Estimand}

Our goal is to estimate the average effect 2014 Medicaid expansion would have had on the non-elderly adult uninsurance rate in states that did not expand Medicaid. Let $A$ indicate treatment assignment, $c$ index a CPUMA, $s$ index the state, and $t$ index the time period. Let $n_1$ be the number of treated CPUMAs, $n_0$ be the number of control CPUMAs, and $n$ be the total number of CPUMAs. Similarly, let and $m = m_1 + m_0$ states (with $m_1$ and $m_0$ defined analogously). Each state has $p_s$ CPUMAs. Since we are only interested in the counterfactual at time $T = 2014$, we simplify notation by removing this variable and the subscript and write our formal estimand as:

\begin{equation}
\psi = \psi^1 - \psi^0 &= n_0^{-1}\sum_{s, c: A_s = 0} Y_{sc}^{A_s = 1} - Y_{sc}^{A_s = 0} 
\end{equation}

The challenge is that we do not observe the counterfactual outcomes for non-expansion CPUMAs had they been in states that expanded their Medicaid programs. We therefore require causal assumptions to tie this counterfactual quantity to our observed data.\footnote{The 2014 Medicaid expansion occurred simultaneously with the implementation of several other major ACA provisions, including (but not limited to) the creation of the ACA-marketplace exchanges, the individual mandate, health insurance subsidies, and community-rating and guaranteed issue of insurance plans (\cite{courtemanche2017early}). Almost all states broadly implemented these reforms beginning January 2014. Conceptually we think of the other ACA components as a state-level treatment ($R$) separate from Medicaid expansion ($A$). Therefore, our total estimated effect may also include interactions between these policy changes; however, we do not attempt to separately identify these effects. Because the ACA implementation and Medicaid expansion may vary over time, we do not try to generalize these results beyond 2014.} 

\subsection{Identification}

The following causal assumptions are necessary (though insufficient) to identify our target parameter from our observed data: the stable unit treatment value assumption (SUTVA), no unmeasured confounding, and no anticipatory treatment effects. We explain these assumptions in detail and their consequences below. We additionally invoke several parametric assumptions to help us identify our causal parameter given the measurement error in our covariates. These assumptions in total are sufficient to identify our causal estimand.

SUTVA has two implications: first, that there is only one version of treatment; second, that $Y_{sc}^{\mathbf{a}} = Y_{sc}^{\mathbf{a}'}$ when $a_{sc} = a_{sc}'$, where $\mathbf{a}$ is the vector of all treatment assignments. We discussed potential violations of the first assumption previously when considering how to reduce Medicaid Expansion to a binary treatment classification. Our solution is to remove states with less restrictive Medicaid eligibility requirements prior to 2014 to approximately satisfy this condition. The second part of this assumption implies that the potential outcomes in each region do not depend on another region's treatment assignment. This is a standard assumption, but is often not realistic in practice. Violations are likely in our setting: for example, \cite{frean2017premium} find evidence that Medicaid expansion drove previously eligible but uninsured individuals to enroll in Medicaid in both expansion and non-expansion states. Signing the potential bias from this violation requires redefining the causal estimand: for example, we might consider the treatment effect on the untreated given that all states have expanded Medicaid, where the contrast is against where only the observed expansion states expanded Medicaid. If the spillover effects were equal in each region, and the magnitude of the spillovers increase with the total number of treated regions, then the true effect would be larger in absolute magnitude than the estimated estimand using the observed data. We could consider other estimands or assumptions to get different predictions about the sign of the bias; however, this is beyond the scope of this paper.

We next assume that there were no anticipatory treatment effects. Letting treatment occur at time $T$, we have that for $t < T$:

\begin{align*}
Y_{sct} = Y_{sct}^0
\end{align*}

This assumption is necessary because we will condition on pre-treatment outcomes. If these outcomes were affected by the treatment before it were implemented, these covariates would be endogenous. Anticipatory treatment effects may occur if plans to expand Medicaid induce uninsured but Medicaid-eligible individuals to enroll in Medicaid prior to expansion. We do not think these violations occurred in large enough numbers to substantially affect our results. Instead, we address a more concerning version of this violation: the fact that several states allowed certain counties to expand Medicaid prior to 2014. We test the sensitivity of our results to the exclusion of these states.

Third, we assume no unmeasured confounding; that is, that at time $T$ the potential outcomes for each CPUMA are independent of the state-level treatment assignment conditional on the population-level CPUMA and state-level covariates $X_{sc}$, a $q$ dimensional vector of covariates (which includes pre-treatment outcomes):

\begin{align*}
Y_{sc}^a \perp A_{sc} \mid X_{sc}
\end{align*}

While unverifiable, we believe it is reasonable here given our rich covariate set. To be explicit, we believe that the potential uninsurance rates for each CPUMA are independent of the treatment assignment conditional on the percentage of uninsured individuals in each year of the pre-treatment period, the percentage of unemployed individuals in each year of the pre-treatment period, the average population growth, the average ratio of households to non-elderly adult population, the state's political composition, the average proportion of households with one, two, or three or more children during the pre-treatment period, the average proportion of households who did not respond about their number children, and the average proportion of individuals during the pre-treatment period with given demographics noted above (age group, sex, white, Hispanic ethnicity, U.S. citizenship, foreign born, income-to-poverty group (including non-response), disability status, urban residence, and educational attainment group). 

A key problem we address in this paper is the violation of this assumption due to measurement error in our covariates. Let 
$\matr{X} = \begin{matrix}
\matr{X}_0\\
\matr{X}_1
\end{matrix}$ be the $n$ by $q$ matrix of true covariates (and separating the control from treated units using $\matr{X}_a$) and define $\matr{W}$ analogously. Because our covariates are estimated using the ACS data, rather than $(Y, \matr{X})$, we instead observe $(J, \matr{W})$, which consist of estimates of the true covariate values. Importantly, $Y_{sc}^a \perp A_{sc} \mid X_{sc} \centernot\implies J_{sc}^a \perp A_{sc} \mid W_{sc}$. The use of these proxies may therefore bias our estimates. We rely on several modeling assumptions to correct for this.

We first model our observed data as functions of the true values plus mean-zero Gaussian noise: $J_{sc} = Y_{sc} + \xi_{sc}$ and $W_{sc} = X_{sc} + v_{sc}$, where we assume $\xi_{sc}$ and $v_{sc}$ are independent though not identically distributed.\footnote{Our covariates are almost all ratio estimates, which will in general be biased. This bias, however, decreases quickly with the sample size (is $O(n^{-1})$). Given that our CPUMA sample sizes are all over 300, we treat these estimates as unbiased in our analysis.} We assume that these errors are uncorrelated with the true values, i.e. $\mathbb{E}\{\xi_{sc} \mid Y_{sc}\} = 0$ (and similarly for all elements of $X$). Second, we assume that $\xi_{sc}$ are uncorrelated with the errors in the covariate measurements. These assumptions and our model for the observed data are reasonable given that the measurement error in this context is sampling variability. Moreover, our outcomes are measured on a different cross-section than our covariates, so it is reasonable to assume that they are uncorrelated with the measurement errors in the covariates. 

We next assume that the true potential outcomes are linear in the true covariates $X_{sc}$. Specifically, we assume that the following model generates the potential non-elderly adult uninsurance rate under treatment $A = a$:

\begin{equation}\label{eqn:outcomemodel}
Y_{sc}^a = \alpha_a + X_{sc}^T\beta_a + \epsilon_{sc} + c_s
\end{equation}

We assume that the errors $\epsilon_{sc}$ and $c_s$ are mean-zero, independent from each other and across time (i.e., we rule out serial-correlation), and are uncorrelated with the true covariates and the treatment assignment $\mathbb{E}\{\epsilon_{sc} \mid X_{sc}, A_s\} = \mathbb{E}\{c_s \mid X_{sc}, A_s\} = 0$. We can then identify $\psi^a$ in terms of our model parameters (see Appendix A); specifically, we have that $\psi^a = \alpha_a + \bar{X}_0^T\beta_a$, where $\bar{X}_0$ is the vector of mean covariate values among the control units. Moreover, we can substitute $J_{sc}$ for $Y_{sc}$ in Equation~\ref{eqn:outcomemodel}, and add $\xi_{sc}$ to the error term without affecting identification. 

We still have the problem that we observe $W_{sc}$ instead of $X_{sc}$. Let $\eta_a = \mathbb{E}\{X_{sc} \mid W_{sc}, A_s = a\}$. By linearity, we know that

\begin{equation}
    J_{sc} = \eta_a(W_{sc})^T\beta + (X_{sc} - \eta_a(W_{sc}))^T\beta + \xi_{sc} + \epsilon_{sc} + c_s 
\end{equation}

If we knew $\eta_a$, we could estimate this model using the observed data $(J, \matr{W})$. The approach we follow here is known as ``regression calibration'' in the measurement-error literature. In particular, we assume a linear model for $\eta_a$:

\begin{align*}
\eta_a(W_{sc}) = \upsilon_a + \kappa_a^T(W_{sc} - \upsilon_a)
\end{align*}

where $\kappa_a = (\Sigma_{XX \mid A = a} + \Sigma_{vv \mid A = a})^{-1}\Sigma_{XX \mid A = a}$. This assumptions motivating this model is that $(X_{sc}, v_{sc}) \stackrel{iid}\sim MVN((\upsilon_a, 0), \Sigma_a)$ and $\Sigma_a$ is a $2q$ by $2q$ block-diagonal matrix consisting of $q$ by $q$ matrices $\Sigma_{XX \mid A = a}$ and $\Sigma_{vv \mid A = a}$ and $0$ in the off-diagonals. Given sufficient auxillary data to estimate $\kappa_a$, we can then estimate $\psi$. We discuss this further below and in Appendices A and B (see also \cite{gleser1992importance}).

\subsection{Estimation}

We outline our estimation strategy first emphasizing how estimating the ETC differs from estimating the ETT with respect to variable selection under the ``synthetic controls'' framework. Second, we explain our estimation procedure, whicht modifies the SBW criterion to address the hierarchical data structure (which we call H-SBW) that reduces the variance of our estimator under our assumption of constant variance and constant within-state correlation of model errors. Third, we connect our estimator to the regression calibration literature by generating weights that balance a linear prediction of the true covariates $\hat{\eta}_a(W_{sc})$ using the observed covariates $W_{sc}$. Fourth, we test the sensitivity of our estimator to a regression-augmented version, using ridge-regression weights following the suggestion of \cite{ben2018augmented}; this allows us to achieve better covariate balance by extrapolating beyond the support of the data. We conclude by proposing a model validation procedure that uses pre-treatment outcomes to compare the performance of our estimators on pre-treatment data.

\subsubsection{Variable selection}

We seek to generate a set of positive weights that balance the means of covariates for the treated units to the mean covariates for the control units. Assume that we observe the true covariate matrices for the treated data $\matr{X}_1 = (X_{1,1}, ..., X_{1, q})$, $\matr{X}_0$ (defined similarly), and the true outcomes $Y$. Let $\bar{X}_{0, r}$ be the mean covariate value for the $r$-th covariate in the non-expansion region. Ideally, there exists some $\gamma^\star \in \Gamma$ satisfying: 

\begin{equation}\label{eqn:constraint}
\Gamma = \{\gamma \in \mathbb{R}^{n_1}: &\lvert X_{1, r}^T\gamma - \bar{X}_{0, r} \lvert \le \delta_r \ \ (r = 1, ..., q), \ \gamma_{sc} > 0, \sum_{s, c: A_{sc} = 1}\gamma_{sc} = 1\}
\end{equation}

for $\delta = 0$. We could then estimate $\psi$ as

\begin{equation}\label{eqn:psi}
\hat{\psi} = \sum_{s: A_s = 1}^{m_1}\sum_{c = 1}^{p_s}\gamma_{sc}^\star Y_{sc} - n_0^{-1}\sum_{s: A_s = 0}^{m_0}\sum_{c = 1}^{p_s}Y_{sc}
\end{equation}

Again assuming that the potential outcomes are a linear function of the true covariates: $\mu_a(X_{sc}) = \alpha_a + X_{sc}^T\beta_a$, the bias of our estimate of $\psi^1$ (again assuming we observed $X_{sc}$), is less than or equal to $\lvert\beta_1\rvert^T\delta = 0$ (see, e.g., \cite{zubizarreta2015stable}). The challenge is that for any given dataset we have no guarantee that any such $\gamma^\star$ exists that exactly balances the covariates. We therefore often require some method of determining how to prioritize which parts of the covariate distribution we wish to balance to minimize this bias.

For synthetic controls, a common approach is to minimize the weighted L2-squared distance of the covariates using a diagonal weighting matrix $V$ that minimized the mean-square error of the weighted difference in pre-treatment outcomes. Letting $\matr{Z}_a$ be the matrix of pre-treatment outcomes for treatment group $A = a$, the synthetic controls algorithm solves the following objective:

\begin{equation}
\gamma^{sc}(V) = \arg\min_{\tilde{\gamma}(V)} = (\bar{X}_1 - X_0^T\tilde{\gamma})'V(\bar{X}_1 - X_0^T\tilde{\gamma}) \\
V = \arg\min (\bar{Z}_1 - Z_0^T\tilde{\gamma}(V))'(\bar{Z}_1 - Z_0^T\tilde{\gamma}(V))
\end{equation}

Notice that the covariate matrix $\matr{X}_a$ may contain $\matr{Z}_a$. Because the values $\{Y^0_{sct}\}_{t=1}^{T-1} = Z_{sc}$ are actually observed for $t < T$ for all units, it is natural to leverage this data to tune (or even select) models generally. While often $V$ is learned on the same data as the weights, \cite{abadie2015comparative} suggest dividing the pre-treatment data into a training and validation set to use cross-validation to learn $V$ \cite{abadie2015comparative}. To be precise, assume we have training data from periods $T = 1, ..., T - l - 1$ and a validation period from periods $T - l, ..., T - 1$. To make this discussion more general, assume that we are evaluating a set of $\mathcal{M}$ candidate models. Let $\bar{Y}^a_{a', t}$ be the mean potential outcome under treatment $A = a$ for treatment group $A = a'$ at time $t$, and let $\hat{\bar{Y}}^a(m)_{a', t}$ be an estimator of that potential outcome at time $t$ using model $m$ and data from treatment group $A = a'$. This tuning procedure implicitly assumes that:

\begin{equation}[]
m^\star = \min_{m \in \mathcal{M}}\sum_{t = T - l}^{T-1}\|\hat{Y}^0(m)_{0, t} - \bar{Y}^0_{1, t}\| = \min_{m \in \mathcal{M}}\mathbb{E}\{\|\hat{\bar{Y}}^0(m)_{0, T} - \bar{Y}^0(m)_{1, T}\|\}
\end{equation}

In other words, we use the empirical loss in the validation period as a proxy for the expected loss in the post-treatment time-period to determine the optimal $V$ (or covariates generally) to ultimately estimate to predict the post-treatment unobserved potential outcomes.\footnote{It is possible that we may multiple models that either perfectly predict the pre-treatment outcomes, or predict them equally well. In this case we would require some other criteria would be needed to choose the optimal model (see, e.g, \cite{becker2017cross}}. Unfortunately, in our setting we never observe $Y^1_{sct}$ (or a mean-unbiased proxy) prior to treatment for any unit. We therefore cannot easily use pre-treatment outcomes to conduct variable selection without stronger assumptions. In particular, we could assume the following:

\begin{assumption}[Counterfactual risk invariance] \label{assumption:second}
m^\star = \min_{m \in \mathcal{M}}\sum_{t = T - l}^{T-1}\|\hat{Y}^0(m)_{1, t} - \hat{Y}^0_{0, t}\| = \min_{m \in \mathcal{M}}\mathbb{E}\{\|\hat{Y}^1(m)_{1, T} - \bar{Y}^1_{0, T}\|\}
\end{assumption}

We call this assumption ``counterfactual risk invariance''. This is a very strong assumption for conducting any form of variable selection in this setting. As a simple example, assume that we can partition $\matr{X}$: $\matr{X} = (\matr{R},V)$ where $V$ is a one-dimensional covariate vector. Further assume that $Y^0_t \perp A \mid \matr{R}$ for all $t = 1, ..., T$ but that $Y^1_T \perp A \mid \matr{X}$. Again assume that $\mu_a$ are linear in the covariates with (time-invariant) coefficients $\beta_{a, r}$ ($r = 1, ..., q$). These assumptions imply that $\beta_{0, v} = 0$. We could then conduct some variable selection procedure using our pre-treatment data, learn that covariate $V$ is unimportant, and estimate a model that perfectly balances the remaining covariates and when run during the validation period perfectly predicts the pre-treatment outcomes for the treated units (i.e. $\hat{\bar{Y}}^0_{1, t} = \bar{Y}^0_{1, t}$ from $t = T - l, ..., T - 1$). If units in group $A = 1$ were untreated in time-period $T$, we would expect this model to give an unbiased estimate of the (observed factual) outcome for the untreated group in time-period $T$ ($\bar{Y}^0_{0, T}$) under our assumptions. However, when predicting $\bar{Y}^1_{0, T}$, this estimator will be biased, with bias equal to $(\gamma^TV_1 - \bar{V}_0) \beta_{1, v}$. If $V$ is a strong predictors of treatment, this could lead to substantial bias.

As a practical example, we highlight the confounding role of Republican governance for our counterfactual estimate. Republican governance is a strong predictor of a state's decision to expand Medicaid \cite{courtemanche2017early}. Moreover, existing evidence prior to Medicaid expansion showed that Medicaid take-up rates were lower in more conservative states \cite{sommers2012understanding}. Yet when generating their synthetic control weights to estimate the ETT, \cite{courtemanche2017early} and \cite{kaestner2017effects} do not control for these factors. \footnote{\cite{courtemanche2017early} does control for Republican governor in their regression model and they find that it is a statistically significant predictor of 2013 uninsurance rates. One reason they may not control for this in the synthetic control model is practical: it is much harder to balance this covariate using control data without extrapolating from the data.} However, it is clear that if take-up rates depend on governance, we may expect this to be a strong confounder of $Y^1$, even if arguably it is not a confounder of $Y^0$ (conditional on other covariates). 

We demonstrate this in our application by conducting a variable importance analysis. Specifically, we remove the balance constraints from the Republican governance indicators and examine how our estimates of $\hat{\psi}^1$ change. Letting $\hat{\psi}^1_v$ be the estimate when removing the Republican governance indicators (or more generally, the covariate matrix $\matr{V}$ where $\matr{X} = (\matr{R}, \matr{V})$). We subtract our original point estimate $\hat{\psi}^1_0$ from $\hat{\psi}^1_v$ to generate the difference $\hat{\Delta}^1$. This difference tells us about the direction of the bias our estimate of $\hat{\psi}^1$ would incur when removing the balance constraints $\delta_v$. Our hypothesis implies that we should expect $\hat{\Delta}_v^1 < 0$: that is, keeping all other covariates (roughly) fixed, we expect the predicted uninsurance rate will decrease when as the level of Republican governance decreases. In addition to the Republican governance indicators, we also examine four other covariate groups: pre-treatment uninsurance rates and pre-treatment unemployment rates, and three sets of different demographic indicators, which we detail in Appendix E. We caution that our results do not imply that Republican governance is not an important confounder of $Y^0_{1, T}$ since we do not analyze this directly. 

Overall we emphasize that predicting the outcome under treatment is different, and perhaps more challenging, than predicting the outcome absent treatment. The latter requires understanding which covariates matter most to predicting treatment response, which we cannot as naturally learn from pre-treatment outcomes. We instead rely on our prior knowledge and modeling choices assumptions to choose which covariates to balance and which covariates to prioritize balancing. We emphasize that modeling the ETC requires greater justification of the covariates used to predict treatment response than for the ETT, and that using the standard synthetic controls algorithm may not be optimal for this purpose.\footnote{Our analysis assumes no unmeasured confounding and a linear model for $\mu_a$. By contrast, synthetic controls are frequently motivated by a linear factor model for $\mu_0$. If we assume $\mu_1$ also follows a linear factor model, identification of the ETC could assume that the unobserved factor loadings are not affected by treatment. By contrast, the unobserved factors may differ for the treated group, although the bias of the estimator may increase due to imbalances in the factor loadings. Further analysis of this problem is beyond the scope of this paper.}

Given these challenges, we therefore use a variation of SBW to estimate the ETC.\footnote{Specifically, we use a modified implementation of Noah Griefer's ``optweight'' package in R, available on github.com/mrubinst757} SBW minimizes the variance of the weights subject to user-specified balance constraints. The primary advantages of H-SBW over synthetic controls in this setting are that it gives the user finer control over the desired levels of covariate balance, allowing the user to navigate a bias-variance tradeoff with respect to balance and the variability of the weights. Specifically, SBW weights solves the following program:

\begin{equation}[SBW]
\gamma &= \arg\min_{\tilde{\gamma} \in \Gamma} \quad \sum_{s: A_s = 1}^{m_1}(\sum_{c = 1}^{p_s} \tilde{\gamma}_{sc}^2  
\end{equation}

where $\Gamma$ is defined in Equation~\ref{eqn:constraint}. We can then estimate $\psi$ using Equation~\ref{eqn:psi}, substituting $J_{sc}$ for $Y_{sc}$ and plugging in the weights $\gamma$. By contrast, the synthetic controls algorithm will minimize the weighted L2 distance between the treated and control units in the criterion; this algorithm in general may lead to lower imbalances, but the balances tradeoffs are difficult to control, the resulting weights may be more extreme, and the algorithm, as formulated in \cite{abadie2010synthetic} and presented above, may not have a unique solution (but see \cite{ben2018augmented}).

For our primary estimates we lean heavily on assumptions to justify our choice of $\delta$. We use a priori domain knowledge about which covariates are most likely to be important predictors of treatment response when setting $\delta$, but also choose $\delta$ to avoid generating overly extreme weights. For our application, we constrain $\delta$ to be 0.05 percentage points for pre-treatment outcomes, 0.15 percentage points for pre-treatment unemployment rates, and 25 percentage points for the Republican governance indicators. We believe these covariates are most likely to predict treatment response. While we believe that Republican governance is an important covariate to balance, we are unable to reduce the constraints further given the support of the data. For the remaining covariates, we let $\delta$ be 0.5 percentage points for average population growth and household to adult ratio, 1 percentage point for female, Hispanic ethnicity, white race, age category, disability, and number of children category; 2 percentage points for urban, citizenship, education category, income-to-poverty category, student, and foreign-born, again choosing these constraints with respect to both feasibility and extreme weight concerns. 

\subsubsection{H-SBW objective}

The motivation of the SBW criterion is to produce the minimum variance weights for a fixed $\delta$. This produces the minimum variance estimator within the constraint set if, for example, the errors in the outcome model are independent and identically distributed \cite{zubizarreta2015stable}. In our setting we allow for possible state-level dependencies, potentially reducing the efficiency of the SBW estimator. To address this possibility, we add the tuning parameter $\rho \in [0, 1)$ in the objective below. Assuming a constant variance across units for each error component, $\rho$ represents a constant (and known) within-state correlation of the errors. 

\begin{equation}\label{eqn:objective}
\gamma &= \arg\min_{\tilde{\gamma} \in \Gamma} \quad \sum_{s: A_s = 1}^{m_1}(\sum_{c = 1}^{p_s} \tilde{\gamma}_{sc}^2 + \sum_{c \ne d}\rho \tilde{\gamma}_{sc}\tilde{\gamma}_{sd})\\
\end{equation}

For $\delta \to \infty$, this objective yields the solution:

\begin{equation}\label{eqn:sbwsol}
\gamma_{sc} = \gamma_s = \frac{1}{\sum_{s=1}^{m_1}\frac{p_s}{(p_s - 1)\rho + 1}}\frac{1}{(p_s - 1)\rho + 1}
\end{equation}

Setting $\rho \approx 1$, the solution becomes

\begin{equation}\label{eqn:sbwsol}
\gamma_{sc} \approx \gamma_s \approx \frac{1}{m_1p_s}
\end{equation}

In other words, this objective will attempt to downweight states with large numbers of CPUMAs and upweight states with small numbers of CPUMAs within the solution space. Notice that setting $\rho = 1$ returns the previous solution and $\rho = 0$ returns the SBW solution. In short, as we increase $\rho$, the objective will attempt to more uniformly disperse the weights across states. 

As a brief illustration, we simulate $N = 800$ observations in $m = 40$ regions each with $p_s = 20$ units, and draw $X_{sc} \sim N(\mu_s, 1)$, for $\mu \in \{0, 1\}$ (drawn from a Bernoulli with equal probability); $A_s \sim Bern(expit(\bar{X}_s))$. We then generate weights to balance the control to the treated group mean. We then run H-SBW variants setting $\rho = 0$ (which is equivalent to SBW), $\rho = 0.5$, and $\rho = 0.99$ all while keeping $\delta$ fixed at zero. Figure 1 shows the weights summed to the group level for all control regions. The color of each set of bars is the overall variance of the weights. We can see that the weights in general are uniform across units for SBW, but poorly dispersed across regions, while the weights are more uniformly dispersed as we increase $\rho$. 

\begin{figure}
\begin{center}
    \includegraphics[scale=0.5]{01_Plots/proofofconcept.png}
    \caption{Comparison of SBW and H-SBW: within group sum of weights}
    \label{oatepref}
\end{center}
\end{figure}

The particular covariance structure we assume is identical to the one proposed by \cite{kloek1981ols}. In Appendix A, we show that this objective produces the minimum variance estimator under the constraint set for this correlation structure. We note that theoretically we could incorporate any other assumed covariance structure into this objective, though the number of tuning parameters might change. Broadly speaking, we can think of H-SBW being to SBW what generalized least squares (GLS) is to ordinary least squares (OLS): both SBW and OLS can produce unbiased estimates of model parameters; however, H-SBW and GLS can improve the efficiency of our estimates under different assumed correlation structures of the outcome errors.

\subsubsection{Measurement error}

A second departure in our estimation procedure comes in our balance constraints: rather than balancing on the observed covariate values $W_{sc}$, we instead balance on the imputed covariate estimates $\hat{\eta}_1(W_{sc})$ (we refer to these as the ``adjusted covariates''). This procedure attempts to correct for the estimation error in these CPUMA-level covariates that may bias our estimate of $\psi^1$. In Appendix A, we consider the super-population target $\psi^{1, sp} = \mathbb{E}\{Y^1 \mid A = 0\}$ and show that under the classical errors-in-variables model, the bias for the SBW estimator that balances on the observed covariates $W$ and sets $\delta = 0$ is equivalent to the bias of a linear combination of coefficient estimates from the OLS-based regression estimator. Specifically, the bias for either estimator is:

\begin{equation}
\mathbb{E}\{\hat{\psi}^{1} - \psi^{1, sp}\} = (\upsilon_0 - \upsilon_1)^T(\kappa - I_d)\beta_1
\end{equation}

The intuition for this result is as follows: exact balancing weights implicitly estimate $\beta_1$ on a subset of the data where we have sufficient covariate overlap. We can therefore think of SBW as returning a solution to some weighted-least squares problem. Assuming that the outcome model holds across all of the data, WLS and OLS are estimating the same $\beta_1$; therefore, the bias that effects the least squares solution will have the same effect on the WLS, and therefore SBW, solution. In Appendix A, Proposition 2, we show that if we had access to $\eta_1$, we can obtain an unbiased estimate of $\psi^{1, sp}$ by reweighting $\eta_1(W_{sc})$.\footnote{For the finite-sample parameter we're targeting, this estimator will have finite-sample bias conditional on $W$ and viewing $X$ as fixed.} Of course, in practice we do not know $\eta_1$ but must instead estimate it using auxillary data. In Appendix A, Proposition 3, we show that we can obtain a consistent of $\psi^{1, sp}$ when balancing on an estimate of $\eta_1$ using auxillary data. 

The key in our application is therefore to estimate $\eta_a$: at a high-level, we use the ACS micro-data replicate survey weights to estimate the covariance matrix of CPUMA sampling-variability $\Sigma_{vv, sc}$. Using our observed data to estimate $\Sigma_{WW \mid A = a}$ and $\bar{W}$, we combine these estimates to generate an estimate of $\eta_a$. This is an old technique that comes from the regression-calibration literature (see, e.g., \cite{gleser1992importance}). We also consider an adjustment procedure that further accounts for the differential measurement error due to the highly variable sample sizes used to calculate each covariate. Specifically, letting $s_{sc}$ be the $q$ dimensional vector of (known) sample sizes, we consider the model $\sqrt{s}_{sc} \cdot v_{sc} \stackrel{iid}\sim MNV(0, \Sigma_{vv \mid A = a})$. By pooling the individual $\hat{\Sigma}_{vv, sc}$ using $s_{sc}$, we can then generate a modeled estimate $\hat{\Sigma}^m_{vv, sc}$ that uses the full efficiency of the data (which we model separately by treatment status). We then use these estimates generate a CPUMA-specific $\hat{\kappa}_{sc}$. This procedure allows our adjustment to differentially adjust covariate values depending on the sample-sizes involved in the adjustment procedure. Further details about these procedures are available in Appendix B.

This is the first application we are aware of to use regression calibration in the context of balancing weights to address the problem of measurement error. We emphasize two critical assumptions for using this procedure in our context: (1) the outcome model is linear in the true covariates; and (2) the measurement error in the outcome is uncorrelated with the measurement error in the covariates. The first assumption is strong, though often used in practice. The second assumption is reasonable, because our outcomes are estimated from a different cross-section than our covariates. 

\subsubsection{Bias-correction for imbalances}

Because we are unable to reduce the balance constraints to our preferred level without generating very extreme weights, following the recent literature on synthetic controls, we test the sensitivity of our results to these imbalances by using ridge-regression augmented weights \cite{ben2018augmented}. Letting $\matr{\hat{X}}_1$ be the matrix of adjusted covariates, and $\gamma^{hsbw}$ be our H-SBW weights, we consider the regression-augmented weights:

\begin{equation}
\gamma^{aug} = \gamma^{hsbw} + (\matr{\hat{X}}_1^T\gamma^{hsbw} - \bar{\matr{W}}_0)^T(\matr{\hat{X}}_1^T\Omega^{-1}\matr{\hat{X}}_1 + \lambda I_q)^{-1}\matr{\hat{X}}_1^T\Omega^{-1}
\end{equation}

where $\Omega$ is a block diagonal matrix with diagonal entries equal to one and the within-group off diagonals equal to $\rho$. We choose $\lambda$ so that the remaining imbalances all fall within 0.5 percentage points. The cost of this procedure is that we must extrapolate off the support of the data, and therefore rely more heavily on our outcome modeling assumptions. We refer to \cite{ben2018augmented} for more details about this procedure. In our results we consider estimators using SBW ($\rho = 0$), H-SBW ($\rho = 0.2$), and ridge-augmented versions of SBW and H-SBW that we call BC-SBW and BC-HSBW. 

\subsection{Model validation}

While we argue that we cannot use pre-treatment outcomes to conduct variable selection or to learn about the relative importance of covariates, we argue that we can use validation period data to compare the performance of different models for fixed covariates and selected levels of imbalance $\delta$. More generally, the best model of $\mu_{0, t}$ $(t = 1,..., T-1)$ is not necessarily the best model, or even a good model, of $\mu_{1, T}$ and vice versa. We invoke additional assumptions to claim the latter: that a good model of $\mu_{1, T}$ is a good model of $\mu_{0, t}$.

First, we again assume the models for $\mu_{a, t}$ are linear in the covariates $\matr{X}$ for all time periods $t$. This is a stronger assumption than required to identify the ETC, which is only that $Y^1_T \perp A \mid X$. Second, we assume that $\matr{X}$ at worst contains some irrelevant covariates $V$ for the model $\mu_{0, t}$. Third, assume that there is a feasible $\delta$ such that all covariate imbalances are small. Under these assumptions, it is easy to see that the best model of $\psi^1$ for time $T$ should also be a good model for $n_t^{-1}\sum_{s,c: A_{sc} = 1}Y_{sct}^0$ for $t = T-l,...,T$ when trained on data from $t = 1, ..., T-l-1$. Even so, we again note that the best model of $\mu_{0, t}$ is not necessarily the best model of $\mu_{1, t}$.

We therefore rerun our procedures on pre-treatment data to compare the performance of our models for a fixed level of imbalances $\delta$. In particular, we train our model on 2009-2011 data to predict 2012 outcomes, and 2010-2012 data to predict 2013 outcomes, and present the errors. We limit to one-year prediction error since our estimand is only one-year forward. We then examine the performance of the H-SBW versus SBW estimators, which only vary with respect to the tuning parameter $\rho$, the bias-corrected versions, and the covariate adjustment procedure used. 

If our outcome models are correct, we expect that the only differences will be due to the covariate adjustment procedure: the adjustment that allows us to best reduce the true (unobserved) covariate imbalances should perform best. This procedure can therefore inform us which covariate adjustment is optimal. We can also examine the tradeoffs from extrapolation versus imbalances: if the outcome model is at best an approximation, we may find that extrapolation does not reduce the bias of the procedure. Finally, we expect that H-SBW versus SBW should have similar performance. However, in Appendix A, we do show that for fixed (unobserved) $X$, the estimator has finite sample bias that reduces with the square of the weights, suggesting the SBW may have less bias than H-SBW; however, we believe will that H-SBW should have less variability. It is unclear how this tradeoff will play out, especially given that we have only two years of pre-treatment data to conduct this test on.

\subsection{Inference}

We consider $W$ to be fixed (and $X$ as fixed unknown parameters), and we consider inference over repeated samples from some super-population of CPUMAs with a state-level dependency structure.\footnote{Alternatively, viewing the potential outcomes as fixed and treatment assignment as random, we could consider inference over the randomization distribution of treatment at the state-level.} While placebo tests are frequently used in the synthetic controls literature for inference, we view these as qualitative statistical tests (see, e.g., \cite{arkhangelsky2019synthetic}) and instead use the leave-one-state-out jackknife to estimate the variance of $\hat{\psi}^1$ (\cite{cameron2015practitioner}). Specifically, we exclude each state and re-calculate the weights holding our targeted mean fixed at $\bar{W}_0$.\footnote{When our preferred initial choice of $\delta$ does not converge, we gradually reduce the constraints until it does.} We compute this estimator in two ways: first, we condition on our covariate adjustment $\hat{\eta}_1$. This is our preferred estimator; however, it does not account for the randomness in $\hat{eta}_1$. We therefore also conduct a second procedure where we re-estimate $\hat{\eta}_1$ for each state omitted in the jackknife procedure and provide these results in the Appendix.

To estimate $Var(\hat{\psi}^0 \mid X, W)$ we use an auxillary regression model and use the CR-2 standard error adjustment (using the ``clubSandwich'' package in R) to estimate the variance of the linear combination $\bar{W}_0^T\hat{\beta}_0$. We can estimate this quantity using the original (unadjusted) data given that $\mathbb{E}\{\bar{W}_0^T\hat{\beta}_0\} = \psi^0$ (since the regression line runs through the point $(\bar{W}_0, \bar{J}_0)$, which are unbiased estimates of $(\bar{X}_0, \bar{Y}_0)$). Our total estimate $\hat{Var}(\hat{\psi})$ is simply the sum of these two variance estimates. We use the standard normal quantiles to generate confidence intervals. 
